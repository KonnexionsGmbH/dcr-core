{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DCR-CORE - Document Content Recognition","text":""},{"location":"#1-introduction","title":"1. Introduction","text":"<p>Based on the paper \"Unfolding the Structure of a Document using Deep Learning\" (Rahman and Finin, 2019), this software project attempts to use various software techniques to automatically recognise the structure in any <code>pdf</code> documents and thus make them more searchable.</p> <p>The processing logic is as follows:</p> <ul> <li>New documents are made available in the file directory <code>inbox</code>. If required, other language-related file directories can also be used (see section Document Language).</li> <li>Documents in a file format accepted by <code>DCR-CORE</code> are registered and moved to the file directory <code>\u00ecnbox_accepted</code>. All other documents are registered and moved to the file directory <code>\u00ecnbox_rejected</code>.</li> <li>Documents not in <code>pdf</code> format are converted to <code>pdf</code> format using Pandoc and TeX Live. </li> <li>Documents based on scanning which, therefore, do not contain text elements, are scanned and converted to <code>pdf</code> format using the Tesseract OCR software. This process applies to all image format files e.g. <code>jpeg</code>, <code>tiff</code> etc., as well as scanned images in <code>pdf</code> format.  </li> <li>From all <code>pdf</code> documents, the text and associated metadata is extracted into a document-specific <code>xml</code> file using PDFlib TET.</li> <li>The document-specific <code>xml</code> files are then parsed and the <code>DCR-CORE</code>-relevant contents are written to the <code>JSON</code> files. </li> <li>From the <code>JSON</code> file(s) spaCy extracts qualified tokens and stores them either in a <code>JSON</code> file or in the database table <code>token</code>.</li> </ul>"},{"location":"#11-rahman-finin-paper","title":"1.1 Rahman &amp; Finin Paper","text":""},{"location":"#12-supported-file-types","title":"1.2 Supported File Types","text":"<p><code>DCR-CORE</code> can handle the following file types based on the file extension:</p> File extension File type Initial processing <code>bmp</code> bitmap image file tesseract <code>csv</code> comma-separated values pandoc <code>docx</code> Office Open XML pandoc <code>epub</code> e-book file format pandoc <code>gif</code> Graphics Interchange Format tesseract <code>html</code> HyperText Markup Language pandoc <code>jp2</code> JPEG 2000 tesseract <code>jpeg</code> Joint Photographic Experts Group tesseract <code>odt</code> Open Document Format for Office Applications pandoc <code>pdf</code> Portable Document Format pdflib / pdf2image <code>png</code> Portable Network Graphics tesseract <code>pnm</code> portable any-map format tesseract <code>rst</code> reStructuredText (RST pandoc <code>rtf</code> Rich Text Format pandoc <code>tif</code> Tag Image File Format tesseract <code>tiff</code> Tag Image File Format tesseract <code>webp</code> Image file format with lossless and lossy compression tesseract"},{"location":"#2-detailed-processing-actions","title":"2. Detailed Processing Actions","text":"<p>The documents to be processed are divided into individual steps, so-called actions.  Each action has the task of changing the state of a document by transforming an input file format into a different output file format. The database tables <code>run</code>, <code>document</code>, and <code>action</code> document the current state of a document, as well as the actions performed so far. If an error occurs during the processing of the document, this is recorded in the database tables <code>document</code> and <code>action</code>. During the next run with the same action, the faulty documents are also processed again.</p>"},{"location":"#21-preprocessor","title":"2.1 Preprocessor","text":""},{"location":"#211-preprocessor-architecture","title":"2.1.1 Preprocessor Architecture","text":""},{"location":"#212-process-the-inbox-directory-action-p_i","title":"2.1.2 Process the inbox directory (action: <code>p_i</code>)","text":"<p>In the first action, the file directory <code>inbox</code> is checked for new document files.  An entry is created in the <code>document</code> database table for each new document, showing the current processing status of the document. </p> <p>The association of document and language is managed via subdirectories of the file folder <code>inbox</code>.  In the database table <code>language</code>, the column <code>directory_name_inbox</code> specifies per language in which subdirectory the documents in this language are to be supplied.  Detailed information on this can be found in the chapter Running DCR-CORE in the section Document Language.</p> <p>The new document files are processed based on their file extension as follows:</p>"},{"location":"#2121-file-extension-pdf","title":"2.1.2.1 File extension <code>pdf</code>","text":"<p>The module <code>fitz</code> from package PyMuPDF is used to check whether the <code>pdf</code> document is a scanned image or not.  A <code>pdf</code> document consisting of a scanned image is marked for conversion from <code>pdf</code> format to an image format and moved to the file directory <code>\u00ecnbox_accepted</code>. Other <code>pdf</code> documents are marked for further processing with the <code>pdf</code> parser and then also moved to the file directory <code>\u00ecnbox_accepted</code>. If, however, when checking the <code>pdf</code> document with <code>fitz</code>, it turns out that the document with the file extension <code>pdf</code> is not really a <code>pdf</code> document, then the document is moved to the file directory <code>inbox_rejected</code>.</p>"},{"location":"#2122-file-extensions-of-documents-for-processing-with-pandoc-and-tex-live","title":"2.1.2.2 File extensions of documents for processing with Pandoc and TeX Live","text":"<p>Document files with the following file extensions are moved to the file directory <code>\u00ecnbox_accepted</code> and  marked for converting to <code>pdf</code> format using Pandoc and TeX Live:</p> <ul> <li><code>csv</code></li> <li><code>docx</code></li> <li><code>epub</code></li> <li><code>html</code></li> <li><code>odt</code></li> <li><code>rst</code></li> <li><code>rtf</code></li> </ul> <p>An exception are files with the file name <code>README.md</code>, which are ignored and not processed.</p>"},{"location":"#2123-file-extensions-of-documents-for-processing-with-tesseract-ocr","title":"2.1.2.3 File extensions of documents for processing with Tesseract OCR","text":"<p>Document files with the following file extensions are moved to the file directory <code>\u00ecnbox_accepted</code> and marked for converting to <code>pdf</code> format using Tesseract OCR:</p> <ul> <li><code>bmp</code></li> <li><code>gif</code></li> <li><code>jp2</code></li> <li><code>jpeg</code></li> <li><code>png</code></li> <li><code>pnm</code></li> <li><code>tif</code></li> <li><code>tiff</code></li> <li><code>webp</code></li> </ul>"},{"location":"#2124-other-file-extensions-of-documents","title":"2.1.2.4 Other file extensions of documents","text":"<p>Document files that do not fall into one of the previous categories are marked as faulty and moved to the file directory <code>\u00ecnbox_rejected</code>.</p>"},{"location":"#213-convert-pdf-documents-to-image-files-action-p_2_i","title":"2.1.3 Convert <code>pdf</code> documents to image files (action: <code>p_2_i</code>)","text":"<p>This processing action only has to be carried out if there are new <code>pdf</code> documents in the document input that only consist of scanned images. <code>pdf</code> documents consisting of scanned images must first be processed with OCR software in order to extract text they contain.  Since Tesseract OCR does not support the <code>pdf</code> file format, such a <code>pdf</code> document must first be converted into one or more image files.  This is done with the software pdf2image, which in turn is based on the Poppler software.</p> <p>The processing of the original document (parent document) is then completed and the further processing is carried out with the newly created image file(s) (child document(s)).</p> <p>Since an image file created here always contains only one page of a <code>pdf</code> document, a multi-page <code>pdf</code> document is distributed over several image files.  After processing with Tesseract OCR, these separated files are then combined into one <code>pdf</code> document.</p>"},{"location":"#214-convert-appropriate-image-files-to-pdf-files-action-ocr","title":"2.1.4 Convert appropriate image files to <code>pdf</code> files (action: <code>ocr</code>)","text":"<p>This processing action only has to be performed if there are new documents in the document entry that correspond to one of the document types listed in section 2.1.2.3. In this processing action, the documents of this document types are converted to the <code>pdf</code> format using Tesseract OCR.</p> <p>After processing with Tesseract OCR, the files split in the previous processing action are combined into a single <code>pdf</code> document.</p>"},{"location":"#215-convert-appropriate-non-pdf-documents-to-pdf-files-action-n_2_p","title":"2.1.5 Convert appropriate non-<code>pdf</code> documents to <code>pdf</code> files (action: <code>n_2_p</code>)","text":"<p>This processing action only has to be performed if there are new documents in the document entry that correspond to one of the document types listed in section 2.1.2.2. In this processing action, the documents of this document types are converted to <code>pdf</code> format using Pandoc and TeX Live.</p>"},{"location":"#22-nlp","title":"2.2 NLP","text":""},{"location":"#221-nlp-architecture","title":"2.2.1 NLP Architecture","text":""},{"location":"#222-extract-text-from-pdf-documents-action-tet","title":"2.2.2 Extract text from <code>pdf</code> documents (action: <code>tet</code>)","text":"<p>In this processing action, the text of the <code>pdf</code> documents from sections 2.1.2.1, 2.1.4 and 2.1.5 are extracted and written to <code>xml</code> files in <code>tetml</code> format for each document. The PDFlib TET library is used for this purpose.</p> <p>Depending on the configuration parameters <code>tetml_page</code> and <code>tetml_word</code>, up to three different <code>xml</code> files with different granularity can be created per document:</p> <ul> <li><code>tetml_line</code>: granularity document <code>line</code> (generated by default),</li> <li><code>tetml_page</code>: granularity document <code>page</code>,</li> <li><code>tetml_word</code>: granularity document <code>word</code>.</li> </ul> <p>The <code>page</code> variant and the <code>word</code> variant are both optional.</p> <p>Example extract from granularity <code>line</code>:</p> <pre><code>&lt;Pages&gt;\n&lt;Page number=\"1\" width=\"594.96\" height=\"840.96\"&gt;\n&lt;Options&gt;granularity=line&lt;/Options&gt;\n&lt;Content granularity=\"line\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"&gt;\n&lt;Para&gt;\n &lt;Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"&gt;\n  &lt;Line llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"&gt;\n   &lt;Text&gt;19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm&lt;/Text&gt;\n  &lt;/Line&gt;\n &lt;/Box&gt;\n&lt;/Para&gt;\n</code></pre> <p>Example extract from granularity <code>page</code>:</p> <pre><code>&lt;Pages&gt;\n&lt;Page number=\"1\" width=\"594.96\" height=\"840.96\"&gt;\n&lt;Options&gt;granularity=page&lt;/Options&gt;\n&lt;Content granularity=\"page\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"&gt;\n&lt;Para&gt;\n &lt;Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"&gt;\n  &lt;Text&gt;19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm&lt;/Text&gt;\n &lt;/Box&gt;\n&lt;/Para&gt;\n</code></pre> <p>Example extract from granularity <code>word</code>:</p> <pre><code>&lt;Pages&gt;\n&lt;Page number=\"1\" width=\"594.96\" height=\"840.96\"&gt;\n&lt;Options&gt;granularity=word tetml={elements={line}}&lt;/Options&gt;\n&lt;Content granularity=\"word\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"&gt;\n&lt;Para&gt;\n &lt;Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"&gt;\n  &lt;Line llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"&gt;\n   &lt;Word&gt;\n    &lt;Text&gt;19&lt;/Text&gt;\n    &lt;Box llx=\"26.45\" lly=\"818.96\" urx=\"34.45\" ury=\"826.96\"/&gt;\n   &lt;/Word&gt;\n   &lt;Word&gt;\n    &lt;Text&gt;/&lt;/Text&gt;\n    &lt;Box llx=\"34.45\" lly=\"818.96\" urx=\"36.67\" ury=\"826.96\"/&gt;\n   &lt;/Word&gt;\n   &lt;Word&gt;\n    &lt;Text&gt;04&lt;/Text&gt;\n    &lt;Box llx=\"36.67\" lly=\"818.96\" urx=\"44.67\" ury=\"826.96\"/&gt;\n   &lt;/Word&gt;\n</code></pre>"},{"location":"#223-store-the-parser-result-in-a-json-file-action-s_p_j","title":"2.2.3 Store the parser result in a <code>JSON</code> file (action: <code>s_p_j</code>)","text":"<p>From the xml files of the granularity document <code>line</code> (<code>&lt;file_name&gt;_&lt;doc_id&gt;.line.xml</code>) or document <code>word</code> (<code>&lt;file_name&gt;_&lt;doc_id&gt;.word.xml</code>) created in the previous action, the text contained is now extracted with the existing metadata using xml parsing and stored in a <code>JSON</code> format in the database tables <code>content_tetml_line</code> and <code>content_tetml_word</code>.</p> <p>The document <code>line</code> granularity attempts to type the lines. Details on this process can be found in section 4.</p> <p>Example extract from granularity <code>line</code>:</p> <pre><code>{\n    \"documentId\": 1,\n    \"documentFileName\": \"Example.pdf\",\n    \"noLinesFooter\": 1,\n    \"noLinesHeader\": 1,\n    \"noLinesInDocument\": 2220,\n    \"noLinesToc\": 85,\n    \"noPagesInDocument\": 57,\n    \"noParagraphsInDocument\": 829,\n    \"noTablesInDocument\": 5,\n    \"pages\": [\n        {\n            \"pageNo\": 1,\n            \"noLinesInPage\": 15,\n            \"noParagraphsInPage\": 7,\n            \"lines\": [\n                {\n                    \"coordLLX\": 26.45,\n                    \"coordURX\": 485.41,\n                    \"lineIndexPage\": 0,\n                    \"lineIndexParagraph\": 0,\n                    \"lineNo\": 1,\n                    \"lineType\": \"h\",\n                    \"paragraphNo\": 1,\n                    \"text\": \"19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm\"\n                },\n</code></pre> <p>Example extract from the optional file <code>line_list_bullet</code>:</p> <pre><code>{\n</code></pre> <p>Example extract from the optional file <code>line_list_number</code>:</p> <pre><code>{\n</code></pre> <p>Example extract from the optional file <code>line_table</code>:</p> <pre><code>{\n    \"documentId\": 1,\n    \"documentFileName\": \"Example.pdf\",\n    \"noTablesInDocument\": 5,\n    \"tables\": [\n        {\n            \"firstRowLLX\": 52.0,\n            \"firstRowURX\": 426.45,\n            \"noColumns\": 30,\n            \"noRows\": 10,\n            \"pageNoFrom\": 9,\n            \"pageNoTill\": 9,\n            \"tableNo\": 1,\n            \"rows\": [\n                {\n                    \"firstColumnLLX\": 52.0,\n                    \"lastColumnURX\": 426.45,\n                    \"noColumns\": 3,\n                    \"rowNo\": 1,\n                    \"columns\": [\n                        {\n                            \"columnNo\": 1,\n                            \"coordLLX\": 52.0,\n                            \"coordURX\": 63.77,\n                            \"lineIndexPage\": 18,\n                            \"lineIndexParagraph\": 0,\n                            \"lineNo\": 1,\n                            \"paragraphNo\": 4,\n                            \"text\": \"No.\"\n                        },\n</code></pre> <p>Example extract from the optional file <code>line_toc</code>:</p> <pre><code>{\n    \"documentId\": 1,\n    \"documentFileName\": \"Example.pdf\",\n    \"toc\": [\n        {\n            \"headingLevel\": 1,\n            \"headingText\": \"1. Lease Term: After the existing Tenant has vacated Landlord will allow Tenant to access the Demised\",\n            \"pageNo\": 4,\n            \"headingCtxLine1\": \"not delay or interfere with the completion of the Allowance Improvements by the Landlord in any material respect; and (b) prior to\",\n            \"headingCtxLine2\": \"entering the Demised Premises the Tenant shall provide insurance coverage as required by this Lease. Landlord shall offer the\",\n            \"headingCtxLine3\": \"existing tenant an early termination of its lease on December 31, 2011, instead of the normal expiration date of January 31, 2012.\",\n            \"regexp\": \"\\\\d+\\\\.$\"\n        },\n</code></pre> <p>Example extract from granularity <code>page</code>:</p> <pre><code>{\n    \"documentId\": 1,\n    \"documentFileName\": \"Example.pdf\",\n    \"noPagesInDocument\": 57,\n    \"noParagraphsInDocument\": 829,\n    \"pages\": [\n        {\n            \"pageNo\": 1,\n            \"noParagraphsInPage\": 7,\n            \"paragraphs\": [\n                {\n                    \"paragraphNo\": 1,\n                    \"text\": \"19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm\"\n                },\n</code></pre> <p>Example extract from granularity <code>word</code>:</p> <pre><code>{\n    \"documentId\": 1,\n    \"documentFileName\": \"Example.pdf\",\n    \"noLinesInDocument\": 2217,\n    \"noPagesInDocument\": 57,\n    \"noParagraphsInDocument\": 828,\n    \"noWordsInDocument\": 38674,\n    \"pages\": [\n        {\n            \"pageNo\": 1,\n            \"noLinesInPage\": 15,\n            \"noParagraphsInPage\": 7,\n            \"noWordsInPage\": 112,\n            \"paragraphs\": [\n                {\n                    \"paragraphNo\": 1,\n                    \"noLinesInParagraph\": 1,\n                    \"noWordsInParagraph\": 28,\n                    \"lines\": [\n                        {\n                            \"lineNo\": 1,\n                            \"noWordsInLine\": 28,\n                            \"words\": [\n                                {\n                                    \"wordNo\": 1,\n                                    \"text\": \"19\"\n                                },\n</code></pre>"},{"location":"#224-create-qualified-document-tokens-action-tkn","title":"2.2.4 Create qualified document tokens (action: <code>tkn</code>)","text":"<p>For tokenization, spaCy is used. </p> <p>The document text is made available to spaCy page by page. Either the granularity document <code>line</code> or document <code>page</code> can be used for this. With the granularity document <code>line</code>, the recognised headers and footers are left out of the token creation.</p> <p>spaCy provides a number of attributes for the token.  Details can be found here in the spaCy documentation. The configuration parameters of the type <code>spacy_tkn_attr_...</code> control which of these attributes are stored to the database table <code>content_token</code>. By default, the following attributes are stored:</p> <ul> <li><code>spacy_tkn_attr_ent_iob_</code></li> <li><code>spacy_tkn_attr_ent_type_</code></li> <li><code>spacy_tkn_attr_i</code></li> <li><code>spacy_tkn_attr_is_currency</code></li> <li><code>spacy_tkn_attr_is_digit</code></li> <li><code>spacy_tkn_attr_is_oov</code></li> <li><code>spacy_tkn_attr_is_punct</code></li> <li><code>spacy_tkn_attr_is_sent_end</code></li> <li><code>spacy_tkn_attr_is_sent_start</code></li> <li><code>spacy_tkn_attr_is_stop</code></li> <li><code>spacy_tkn_attr_is_title</code></li> <li><code>spacy_tkn_attr_lemma_</code></li> <li><code>spacy_tkn_attr_like_email</code></li> <li><code>spacy_tkn_attr_like_num</code></li> <li><code>spacy_tkn_attr_like_url</code></li> <li><code>spacy_tkn_attr_norm_</code></li> <li><code>spacy_tkn_attr_pos_</code></li> <li><code>spacy_tkn_attr_tag_</code></li> <li><code>spacy_tkn_attr_text</code></li> <li><code>spacy_tkn_attr_whitespace_</code></li> </ul> <p>In the event of an error, the original document is marked as erroneous and an explanatory entry is also written in the <code>document</code> table. </p> <p>Example extract from granularity <code>line</code>:</p> <pre><code>{\n    \"documentId\": 1,\n    \"documentFileName\": \"Example.pdf\",\n    \"noLinesFooter\": 1,\n    \"noLinesHeader\": 1,\n    \"noLinesInDocument\": 2031,\n    \"noLinesToc\": 85,\n    \"noPagesInDocument\": 57,\n    \"noParagraphsInDocument\": 630,\n    \"noSentencesInDocument\": 949,\n    \"noTablesInDocument\": 5,\n    \"noTokensInDocument\": 16495,\n    \"pages\": [\n        {\n            \"pageNo\": 1,\n            \"noLinesInPage\": 13,\n            \"noParagraphsInPage\": 5,\n            \"noSentencesInPage\": 5,\n            \"noTokensInPage\": 39,\n            \"paragraphs\": [\n                {\n                    \"paragraphNo\": 2,\n                    \"noLinesInParagraph\": 2,\n                    \"noSentencesInParagraph\": 1,\n                    \"noTokensInParagraph\": 7,\n                    \"sentences\": [\n                        {\n                            \"sentenceNo\": 1,\n                            \"coordLLX\": 34.0,\n                            \"coordURX\": 244.18,\n                            \"lineType\": \"b\",\n                            \"noTokensInSentence\": 7,\n                            \"text\": \"EX-10.19 3 t1700141_ex10-19.htm EXHIBIT 10.19 Exhibit 10.19\",\n                            \"tokens\": [\n                                {\n                                    \"tknI\": 0,\n                                    \"tknIsOov\": true,\n                                    \"tknLemma_\": \"ex-10.19\",\n                                    \"tknNorm_\": \"ex-10.19\",\n                                    \"tknPos_\": \"NUM\",\n                                    \"tknTag_\": \"CD\",\n                                    \"tknText\": \"EX-10.19\",\n                                    \"tknWhitespace_\": \" \"\n                                },\n</code></pre>"},{"location":"#3-auxiliary-file-names","title":"3. Auxiliary File Names","text":"<p>The processing actions are based on different flat files, each of which is generated from the original document on an action-related basis. Apart from the <code>JSON</code> files optionally created during the 'tokenizer' action, these can be automatically deleted after error-free processing.</p>"},{"location":"#31-naming-system","title":"3.1 Naming System","text":"<p>Action <code>p_i</code> - process the inbox directory</p> <pre><code>in : &lt;ost&gt;.&lt;oft&gt;              \nout: &lt;ost&gt;_&lt;di&gt;.&lt;oft&gt;\n</code></pre> <p>Action <code>p_2_i</code> - convert pdf documents to image files</p> <pre><code>in : &lt;ost&gt;_&lt;di&gt;.pdf                \nout: &lt;ost&gt;_&lt;di&gt;.&lt;jpeg|png&gt;\n</code></pre> <p>Action <code>ocr</code> - convert image files to pdf documents</p> <pre><code>in : &lt;ost&gt;_&lt;di&gt;.&lt;oft&gt;              \nor : &lt;ost&gt;_&lt;di&gt;.&lt;jpeg|png&gt;        \nout: &lt;ost&gt;_&lt;di&gt;_&lt;pn&gt;.pdf \n     &lt;ost&gt;_&lt;di&gt;_0.pdf\n</code></pre> <p>Action <code>n_2_p</code> - convert non-pdf documents to pdf documents</p> <pre><code>in : &lt;ost&gt;_&lt;di&gt;.&lt;oft&gt;              \nout: &lt;ost&gt;_&lt;di&gt;.pdf\n</code></pre> <p>Action <code>tet</code> - extract text and metadata from pdf documents</p> <pre><code>in : &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0].pdf       \nout: &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.xml        \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_page.xml \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_word.xml\n</code></pre> <p>Action <code>s_p_j</code> - store the parser result in a <code>JSON</code> file</p> <pre><code>in : &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.xml  \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_page.xml        \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_word.xml \nout: &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.json \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.list_bullet.json \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.list_number.json \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.table.json \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.toc.json \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_page.json \n     &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_word.json\n</code></pre> <p>Action <code>tkn</code> - create qualified document tokens</p> <pre><code>in : &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line.json \nout: &lt;ost&gt;_&lt;di&gt;[_&lt;pn&gt;|_0]_line_token.json\n</code></pre> Abbr. Meaning <code>oft</code> original file type <code>osn</code> original stem name <code>di</code> document identifier <code>pn</code> page number"},{"location":"#32-examples","title":"3.2 Examples","text":""},{"location":"#321-possible-intermediate-files-from-a-docx-document","title":"3.2.1 Possible intermediate files from a <code>docx</code> document:","text":"<pre><code>case_2_docx_route_inbox_pandoc_pdflib_2.docx\n\ncase_2_docx_route_inbox_pandoc_pdflib_2.pdf\n\ncase_2_docx_route_inbox_pandoc_pdflib_2.line.xml\ncase_2_docx_route_inbox_pandoc_pdflib_2.page.xml\ncase_2_docx_route_inbox_pandoc_pdflib_2.word.xml\n\ncase_2_docx_route_inbox_pandoc_pdflib_2.line.json\ncase_2_docx_route_inbox_pandoc_pdflib_2.line_list_bullet.json\ncase_2_docx_route_inbox_pandoc_pdflib_2.line_list_number.json\ncase_2_docx_route_inbox_pandoc_pdflib_2.line_table.json\ncase_2_docx_route_inbox_pandoc_pdflib_2.line_toc.json\ncase_2_docx_route_inbox_pandoc_pdflib_2.page.json\ncase_2_docx_route_inbox_pandoc_pdflib_2.word.json\n\ncase_2_docx_route_inbox_pandoc_pdflib_2.line.token.json\n</code></pre>"},{"location":"#322-possible-intermediate-files-from-a-jpg-document","title":"3.2.2 Possible intermediate files from a <code>jpg</code> document:","text":"<pre><code>case_6_jpg_route_inbox_tesseract_pdflib_6.jpg\n\ncase_6_jpg_route_inbox_tesseract_pdflib_6.pdf\n\ncase_6_jpg_route_inbox_tesseract_pdflib_6.line.xml\ncase_6_jpg_route_inbox_tesseract_pdflib_6.page.xml\ncase_6_jpg_route_inbox_tesseract_pdflib_6.word.xml\n\ncase_6_jpg_route_inbox_tesseract_pdflib_6.line.json\ncase_6_jpg_route_inbox_tesseract_pdflib_6.line_list_bullet.json\ncase_6_jpg_route_inbox_tesseract_pdflib_6.line_list_number.json\ncase_6_jpg_route_inbox_tesseract_pdflib_6.line_table.json\ncase_6_jpg_route_inbox_tesseract_pdflib_6.line_toc.json\ncase_6_jpg_route_inbox_tesseract_pdflib_6.page.json\ncase_6_jpg_route_inbox_tesseract_pdflib_6.word.json\n\ncase_6_jpg_route_inbox_tesseract_pdflib_6.line.token.json\n</code></pre>"},{"location":"#323-possible-intermediate-files-from-a-proper-pdf-document","title":"3.2.3 Possible intermediate files from a proper <code>pdf</code> document:","text":"<pre><code>case_3_pdf_text_route_inbox_pdflib_3.pdf\n\ncase_3_pdf_text_route_inbox_pdflib_3.line.xml\ncase_3_pdf_text_route_inbox_pdflib_3.page.xml\ncase_3_pdf_text_route_inbox_pdflib_3.word.xml\n\ncase_3_pdf_text_route_inbox_pdflib_3.line.json\ncase_3_pdf_text_route_inbox_pdflib_3.line_list_bullet.json\ncase_3_pdf_text_route_inbox_pdflib_3.line_list_number.json\ncase_3_pdf_text_route_inbox_pdflib_3.line_table.json\ncase_3_pdf_text_route_inbox_pdflib_3.line_toc.json\ncase_3_pdf_text_route_inbox_pdflib_3.page.json\ncase_3_pdf_text_route_inbox_pdflib_3.word.json\n\ncase_3_pdf_text_route_inbox_pdflib_3.line.token.json\n</code></pre>"},{"location":"#324-possible-intermediate-files-from-a-single-page-scanned-image-pdf-document","title":"3.2.4 Possible intermediate files from a single page scanned image <code>pdf</code> document:","text":"<pre><code>case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4.pdf\n\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.jpeg\n\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.pdf\n\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.xml\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.page.xml\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.word.xml\n\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.json\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_list_bullet.json\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_list_number.json\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_table.json\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_toc.json\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.page.json\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.word.json\n\ncase_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.token.json\n</code></pre>"},{"location":"#325-possible-intermediate-files-from-a-multi-page-scanned-image-pdf-document","title":"3.2.5 Possible intermediate files from a multi page scanned image <code>pdf</code> document:","text":"<pre><code>case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5.pdf\n\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_1.jpeg\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_2.jpeg\n\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_1.pdf\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_2.pdf\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.pdf\n\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.xml\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.page.xml\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.word.xml\n\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.json\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_list_bullet.json\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_list_number.json\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_table.json\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_toc.json\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.page.json\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.word.json\n\ncase_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.token.json\n</code></pre>"},{"location":"application_api_documentation/","title":"DCR-CORE - Application - API Documentation","text":""},{"location":"application_api_documentation/#documentation-for-cls_process","title":"Documentation for <code>cls_process</code>","text":"<p>Process utility class.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>class Process:\n\"\"\"Process utility class.\"\"\"\n\n    # ------------------------------------------------------------------\n    # Class variables.\n    # ------------------------------------------------------------------\n    ERROR_01_901: ClassVar[str] = \"01.901 Issue (p_i): Document rejected because of unknown file extension='{extension}'.\"\n    ERROR_01_903: ClassVar[str] = (\n        \"01.903 Issue (p_i): Error with fitz.open() processing of file '{file_name}' \" + \"- RuntimeError - error: '{error_msg}'\"\n    )\n\n    ERROR_21_901: ClassVar[str] = (\n        \"21.901 Issue (p_2_i): Processing file '{full_name}' with pdf2image failed - PDFPageCountError - \"\n        + \"error type: '{error_type}' - error: '{error_msg}'\"\n    )\n    ERROR_31_902: ClassVar[str] = (\n        \"31.902 Issue (n_2_p): The file '{full_name}' cannot be converted to an \" + \"'PDF' document - FileNotFoundError\"\n    )\n    ERROR_31_903: ClassVar[str] = (\n        \"31.903 Issue (n_2_p): The file '{full_name}' cannot be converted to an \" + \"'PDF' document - RuntimeError - error: '{error_msg}'\"\n    )\n    ERROR_31_911: ClassVar[str] = \"31.911 Issue (n_2_p): The pdf document {full_name} for PDFlib TET is an empty file\"\n    ERROR_41_901: ClassVar[str] = (\n        \"41.901 Issue (ocr): Converting the file '{full_name}' with Tesseract OCR failed - \" + \"RuntimeError - error: '{error_msg}'\"\n    )\n    ERROR_41_911: ClassVar[str] = \"41.911 Issue (ocr): Tesseract OCR has created an empty pdf file from the file {full_name}\"\n    ERROR_51_901: ClassVar[str] = (\n        \"51.901 Issue (tet): Opening document '{full_name}' - \" + \"error no: '{error_no}' - api: '{api_name}' - error: '{error_msg}'\"\n    )\n    ERROR_61_901: ClassVar[str] = \"61.901 Issue (s_p_j): Parsing the file '{full_name}' failed - FileNotFoundError\"\n    ERROR_71_901: ClassVar[str] = \"71.901 Issue (tkn): Tokenizing the file '{full_name}' failed - FileNotFoundError\"\n\n    PANDOC_PDF_ENGINE_LULATEX: ClassVar[str] = \"lulatex\"\n    PANDOC_PDF_ENGINE_XELATEX: ClassVar[str] = \"xelatex\"\n\n    # ------------------------------------------------------------------\n    # Initialise the instance.\n    # ------------------------------------------------------------------\n    def __init__(self) -&gt; None:\n\"\"\"Initialise the instance.\"\"\"\n        try:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n        except AttributeError:\n            dcr_core.core_glob.initialise_logger()\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        self._document_id: int = 0\n\n        self._full_name_in = \"\"\n        self._full_name_in_directory = \"\"\n        self._full_name_in_extension = \"\"\n        self._full_name_in_extension_int = \"\"\n        self._full_name_in_next_step = \"\"\n        self._full_name_in_pandoc = \"\"\n        self._full_name_in_parser_line = \"\"\n        self._full_name_in_parser_page = \"\"\n        self._full_name_in_parser_word = \"\"\n        self._full_name_in_pdf2image = \"\"\n        self._full_name_in_pdflib = \"\"\n        self._full_name_in_stem_name = \"\"\n        self._full_name_in_tesseract = \"\"\n        self._full_name_in_tokenizer_line = \"\"\n        self._full_name_in_tokenizer_page = \"\"\n        self._full_name_in_tokenizer_word = \"\"\n        self._full_name_orig = \"\"\n\n        self._is_delete_auxiliary_files = False\n        self._is_pandoc = False\n        self._is_pdf2image = False\n        self._is_tesseract = False\n        self._is_verbose = False\n\n        self._language_pandoc: str = \"\"\n        self._language_spacy: str = \"\"\n        self._language_tesseract: str = \"\"\n\n        self._no_lines_footer: int = 0\n        self._no_lines_header: int = 0\n        self._no_lines_toc: int = 0\n        self._no_pdf_pages: int = 0\n\n        self._exist = True\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Check the document by the file extension and determine further\n    # processing.\n    # ------------------------------------------------------------------\n    def _document_check_extension(self):\n\"\"\"Document processing control.\n\n        Check the document by the file extension and determine further\n        processing.\n\n        Raises:\n            RuntimeError: ERROR_01_903\n        \"\"\"\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        if self._full_name_in_extension_int == dcr_core.core_glob.FILE_TYPE_PDF:\n            try:\n                if bool(\"\".join([page.get_text() for page in fitz.open(self._full_name_in)])):\n                    self._full_name_in_pdflib = self._full_name_in\n                else:\n                    self._is_pdf2image = True\n                    self._is_tesseract = True\n                    self._full_name_in_pdf2image = self._full_name_in\n            except RuntimeError as exc:\n                raise RuntimeError(\n                    Process.ERROR_01_903.replace(\"{file_name}\", self._full_name_in).replace(\"{error_msg}\", str(exc)),\n                ) from exc\n        elif self._full_name_in_extension_int in dcr_core.core_glob.FILE_TYPE_PANDOC:\n            self._is_pandoc = True\n            self._full_name_in_pandoc = self._full_name_in\n        elif self._full_name_in_extension_int in dcr_core.core_glob.FILE_TYPE_TESSERACT:\n            self._is_tesseract = True\n            self._full_name_in_tesseract = self._full_name_in\n        else:\n            raise RuntimeError(Process.ERROR_01_901.replace(\"{extension}\", self._full_name_in_extension_int))\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # -----------------------------------------------------------------------------\n    # Delete the given auxiliary file.\n    # -----------------------------------------------------------------------------\n    def _document_delete_auxiliary_file(self, full_name: str) -&gt; None:\n\"\"\"Delete the given auxiliary file.\n\n        Args:\n            full_name (str): File name.\n        \"\"\"\n        if not self._is_delete_auxiliary_files:\n            return\n\n        # Don't remove the base document !!!\n        if full_name == self._full_name_in:\n            return\n\n        if os.path.isfile(full_name):\n            os.remove(full_name)\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"Auxiliary file '{full_name}' deleted\")\n\n    # ------------------------------------------------------------------\n    # Initialize the document recognition process.\n    # ------------------------------------------------------------------\n    def _document_init(self) -&gt; None:\n\"\"\"Initialize the document recognition process.\"\"\"\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        self._document_id: int = 0\n\n        self._full_name_in: str = \"\"\n        self._full_name_in_directory: str = \"\"\n        self._full_name_in_extension: str = \"\"\n        self._full_name_in_extension_int: str = \"\"\n        self._full_name_in_next_step: str = \"\"\n        self._full_name_in_pandoc: str = \"\"\n        self._full_name_in_parser_line: str = \"\"\n        self._full_name_in_parser_page: str = \"\"\n        self._full_name_in_parser_word: str = \"\"\n        self._full_name_in_pdf2image: str = \"\"\n        self._full_name_in_pdflib: str = \"\"\n        self._full_name_in_stem_name: str = \"\"\n        self._full_name_in_tesseract: str = \"\"\n        self._full_name_in_tokenizer_line: str = \"\"\n        self._full_name_in_tokenizer_page: str = \"\"\n        self._full_name_in_tokenizer_word: str = \"\"\n        self._full_name_orig: str = \"\"\n\n        self._is_pandoc: bool = False\n        self._is_pdf2image: bool = False\n        self._is_tesseract: bool = False\n\n        self._language_pandoc: str = \"\"\n        self._language_spacy: str = \"\"\n        self._language_tesseract: str = \"\"\n\n        self._no_lines_footer: int = 0\n        self._no_lines_header: int = 0\n        self._no_lines_toc: int = 0\n        self._no_pdf_pages: int = 0\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Convert the document to PDF format using Pandoc.\n    # ------------------------------------------------------------------\n    def _document_pandoc(self):\n\"\"\"Convert the document to PDF format using Pandoc.\n\n        Raises:\n            RuntimeError: Any Pandoc issue.\n        \"\"\"\n        if self._is_pandoc:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing Pandoc        {self._full_name_in_pandoc}\")\n\n            self._full_name_in_pdflib = dcr_core.core_utils.get_full_name_from_components(\n                self._full_name_in_directory, self._full_name_in_stem_name, dcr_core.core_glob.FILE_TYPE_PDF\n            )\n\n            return_code, error_msg = Process.pandoc(\n                self._full_name_in_pandoc,\n                self._full_name_in_pdflib,\n                self._language_pandoc,\n            )\n            if return_code != \"ok\":\n                raise RuntimeError(error_msg)\n\n            self._document_delete_auxiliary_file(self._full_name_in_pandoc)\n\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing Pandoc        {self._full_name_in_pdflib}\")\n\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Extract the text for all granularities from the PDF document.\n    # ------------------------------------------------------------------\n    def _document_parser(self):\n\"\"\"Extract the text for all granularities from the PDF document.\"\"\"\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        self._full_name_in_tokenizer_line = dcr_core.core_utils.get_full_name_from_components(\n            self._full_name_in_directory,\n            self._full_name_in_stem_name + \".\" + dcr_core.cls_nlp_core.NLPCore.LINE_XML_VARIATION + dcr_core.core_glob.FILE_TYPE_JSON,\n        )\n\n        self._full_name_in_tokenizer_page = dcr_core.core_utils.get_full_name_from_components(\n            self._full_name_in_directory,\n            self._full_name_in_stem_name + \".\" + dcr_core.cls_nlp_core.NLPCore.PAGE_XML_VARIATION + dcr_core.core_glob.FILE_TYPE_JSON,\n        )\n\n        self._full_name_in_tokenizer_word = dcr_core.core_utils.get_full_name_from_components(\n            self._full_name_in_directory,\n            self._full_name_in_stem_name + \".\" + dcr_core.cls_nlp_core.NLPCore.WORD_XML_VARIATION + dcr_core.core_glob.FILE_TYPE_JSON,\n        )\n\n        for (\n            full_name_in_parser,\n            full_name_in_tokenizer,\n            tetml_type,\n            is_parsing_line,\n            is_parsing_page,\n            is_parsing_word,\n        ) in (\n            (\n                self._full_name_in_parser_line,\n                self._full_name_in_tokenizer_line,\n                dcr_core.cls_nlp_core.NLPCore.TETML_TYPE_LINE,\n                True,\n                False,\n                False,\n            ),\n            (\n                self._full_name_in_parser_page,\n                self._full_name_in_tokenizer_page,\n                dcr_core.cls_nlp_core.NLPCore.TETML_TYPE_PAGE,\n                False,\n                True,\n                False,\n            ),\n            (\n                self._full_name_in_parser_word,\n                self._full_name_in_tokenizer_word,\n                dcr_core.cls_nlp_core.NLPCore.TETML_TYPE_WORD,\n                False,\n                False,\n                True,\n            ),\n        ):\n            if (\n                is_parsing_page\n                and not dcr_core.core_glob.setup.is_tetml_page\n                or is_parsing_word\n                and not dcr_core.core_glob.setup.is_tetml_word\n            ):\n                continue\n\n            self._document_parser_tetml_type(\n                full_name_in_parser,\n                full_name_in_tokenizer,\n                tetml_type,\n                is_parsing_line,\n                is_parsing_page,\n                is_parsing_word,\n            )\n\n            if is_parsing_line:\n                self._no_lines_footer = dcr_core.core_glob.line_type_header_footer.no_lines_footer\n                self._no_lines_header = dcr_core.core_glob.line_type_header_footer.no_lines_header\n                self._no_lines_toc = dcr_core.core_glob.line_type_toc.no_lines_toc\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Extract the text for a specific granularity from the PDF document.\n    # ------------------------------------------------------------------\n    def _document_parser_tetml_type(\n        self,\n        full_name_in_parser,\n        full_name_in_tokenizer,\n        tetml_type,\n        is_parsing_line,\n        is_parsing_page,\n        is_parsing_word,\n    ):\n\"\"\"XML Parser processing.\n\n        Extract the text for a specific granularity from the PDF\n        document.\n\n        Raises:\n            RuntimeError: Any parser issue.\n        \"\"\"\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing {tetml_type}          {full_name_in_parser}\")\n\n        dcr_core.core_glob.setup.is_parsing_line = is_parsing_line\n        dcr_core.core_glob.setup.is_parsing_page = is_parsing_page\n        dcr_core.core_glob.setup.is_parsing_word = is_parsing_word\n\n        return_code, error_msg = Process.parser(\n            full_name_in_parser,\n            full_name_in_tokenizer,\n            self._no_pdf_pages,\n            self._document_id,\n            self._full_name_orig,\n        )\n        if return_code != \"ok\":\n            raise RuntimeError(error_msg)\n\n        self._document_delete_auxiliary_file(full_name_in_parser)\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing {tetml_type}          {full_name_in_tokenizer}\")\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Convert the PDF document to an image file using pdf2image.\n    # ------------------------------------------------------------------\n    def _document_pdf2image(self):\n\"\"\"Convert the PDF document to an image file using pdf2image.\n\n        Raises:\n            RuntimeError: Any pdf2image issue.\n        \"\"\"\n        if self._is_pdf2image:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing pdf2image     {self._full_name_in_pdf2image}\")\n\n            self._full_name_in_tesseract = dcr_core.core_utils.get_full_name_from_components(\n                self._full_name_in_directory,\n                self._full_name_in_stem_name\n                + \"_[0-9]*.\"\n                + (\n                    dcr_core.core_glob.FILE_TYPE_PNG\n                    if dcr_core.core_glob.setup.pdf2image_type == dcr_core.cls_setup.Setup.PDF2IMAGE_TYPE_PNG\n                    else dcr_core.core_glob.FILE_TYPE_JPEG\n                ),\n            )\n\n            return_code, error_msg, _ = Process.pdf2image(\n                self._full_name_in_pdf2image,\n            )\n            if return_code != \"ok\":\n                raise RuntimeError(error_msg)\n\n            self._document_delete_auxiliary_file(self._full_name_in_pdf2image)\n\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing pdf2image     {self._full_name_in_tesseract}\")\n\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Extract the text and metadata from a PDF document to an XML file.\n    # ------------------------------------------------------------------\n    def _document_pdflib(self):\n\"\"\"Extract the text and metadata from a PDF document to an XML file.\n\n        Raises:\n            RuntimeError: Any PDFlib TET issue.\n        \"\"\"\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing PDFlib TET    {self._full_name_in_pdflib}\")\n\n        # noinspection PyUnresolvedReferences\n        self._no_pdf_pages = len(PyPDF2.PdfReader(self._full_name_in_pdflib).pages)\n        if self._no_pdf_pages == 0:\n            raise RuntimeError(f\"The number of pages of the PDF document {self._full_name_in_pdflib} cannot be determined\")\n\n        self._full_name_in_parser_line = dcr_core.core_utils.get_full_name_from_components(\n            self._full_name_in_directory,\n            self._full_name_in_stem_name + \".\" + dcr_core.cls_nlp_core.NLPCore.LINE_XML_VARIATION + dcr_core.core_glob.FILE_TYPE_XML,\n        )\n\n        return_code, error_msg = Process.pdflib(\n            full_name_in=self._full_name_in_pdflib,\n            full_name_out=self._full_name_in_parser_line,\n            document_opt_list=dcr_core.cls_nlp_core.NLPCore.LINE_TET_DOCUMENT_OPT_LIST,\n            page_opt_list=dcr_core.cls_nlp_core.NLPCore.LINE_TET_PAGE_OPT_LIST,\n        )\n        if return_code != \"ok\":\n            raise RuntimeError(error_msg)\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing PDFlib TET    {self._full_name_in_parser_line}\")\n\n        if dcr_core.core_glob.setup.is_tetml_page:\n            self._full_name_in_parser_page = dcr_core.core_utils.get_full_name_from_components(\n                self._full_name_in_directory,\n                self._full_name_in_stem_name + \".\" + dcr_core.cls_nlp_core.NLPCore.PAGE_XML_VARIATION + dcr_core.core_glob.FILE_TYPE_XML,\n            )\n            return_code, error_msg = Process.pdflib(\n                full_name_in=self._full_name_in_pdflib,\n                full_name_out=self._full_name_in_parser_page,\n                document_opt_list=dcr_core.cls_nlp_core.NLPCore.PAGE_TET_DOCUMENT_OPT_LIST,\n                page_opt_list=dcr_core.cls_nlp_core.NLPCore.PAGE_TET_PAGE_OPT_LIST,\n            )\n            if return_code != \"ok\":\n                raise RuntimeError(error_msg)\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing PDFlib TET    {self._full_name_in_parser_page}\")\n\n        if dcr_core.core_glob.setup.is_tetml_word:\n            self._full_name_in_parser_word = dcr_core.core_utils.get_full_name_from_components(\n                self._full_name_in_directory,\n                self._full_name_in_stem_name + \".\" + dcr_core.cls_nlp_core.NLPCore.WORD_XML_VARIATION + dcr_core.core_glob.FILE_TYPE_XML,\n            )\n            return_code, error_msg = Process.pdflib(\n                full_name_in=self._full_name_in_pdflib,\n                full_name_out=self._full_name_in_parser_word,\n                document_opt_list=dcr_core.cls_nlp_core.NLPCore.WORD_TET_DOCUMENT_OPT_LIST,\n                page_opt_list=dcr_core.cls_nlp_core.NLPCore.WORD_TET_PAGE_OPT_LIST,\n            )\n            if return_code != \"ok\":\n                raise RuntimeError(error_msg)\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing PDFlib TET    {self._full_name_in_parser_word}\")\n\n        self._document_delete_auxiliary_file(self._full_name_in_pdflib)\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Convert one or more image files to a PDF file using Tesseract OCR.\n    # ------------------------------------------------------------------\n    def _document_tesseract(self):\n\"\"\"Process the document with Tesseract OCR.\n\n        Convert one or more image files to a PDF file using Tesseract\n        OCR.\n\n        Raises:\n            RuntimeError: Any Tesseract OCR issue.\n        \"\"\"\n        if self._is_tesseract:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing Tesseract OCR {self._full_name_in_tesseract}\")\n\n            if self._is_pdf2image:\n                self._full_name_in_stem_name += \"_0\"\n\n            self._full_name_in_pdflib = dcr_core.core_utils.get_full_name_from_components(\n                self._full_name_in_directory, self._full_name_in_stem_name, dcr_core.core_glob.FILE_TYPE_PDF\n            )\n\n            return_code, error_msg, children = Process.tesseract(\n                self._full_name_in_tesseract,\n                self._full_name_in_pdflib,\n                self._language_tesseract,\n            )\n            if return_code != \"ok\":\n                raise RuntimeError(error_msg)\n\n            # noinspection PyUnresolvedReferences\n            self._no_pdf_pages = len(PyPDF2.PdfReader(self._full_name_in_pdflib).pages)\n            if self._no_pdf_pages == 0:\n                raise RuntimeError(f\"The number of pages of the PDF document {self._full_name_in_pdflib} cannot be determined\")\n\n            for child in children:\n                self._document_delete_auxiliary_file(child)\n\n            dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing Tesseract OCR {self._full_name_in_pdflib}\")\n\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Convert the PDF document to an image file using pdf2image.\n    # ------------------------------------------------------------------\n    def _document_tokenizer(self) -&gt; None:\n\"\"\"Tokenize the document with spaCy.\n\n        Raises:\n            RuntimeError: Any spaCy issue.\n        \"\"\"\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing spaCy         {self._full_name_in_tokenizer_line}\")\n\n        try:\n            dcr_core.core_glob.tokenizer_spacy.exists()\n        except AttributeError:\n            dcr_core.core_glob.tokenizer_spacy = dcr_core.cls_tokenizer_spacy.TokenizerSpacy()\n\n        self._full_name_in_next_step = dcr_core.core_utils.get_full_name_from_components(\n            self._full_name_in_directory,\n            self._full_name_in_stem_name + \".line_token.\" + dcr_core.core_glob.FILE_TYPE_JSON,\n        )\n\n        return_code, error_msg = Process.tokenizer(\n            full_name_in=self._full_name_in_tokenizer_line,\n            full_name_out=self._full_name_in_next_step,\n            pipeline_name=self._language_spacy,\n            document_id=self._document_id,\n            full_name_orig=self._full_name_orig,\n            no_lines_footer=self._no_lines_footer,\n            no_lines_header=self._no_lines_header,\n            no_lines_toc=self._no_lines_toc,\n        )\n        if return_code != \"ok\":\n            raise RuntimeError(error_msg)\n\n        self._document_delete_auxiliary_file(self._full_name_in_tokenizer_line)\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing spaCy         {self._full_name_in_next_step}\")\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Document content recognition for a specific file.\n    # ------------------------------------------------------------------\n    def document(\n        self,\n        full_name_in: str,\n        document_id: int = None,\n        full_name_orig: str = None,\n        is_delete_auxiliary_files: bool = None,\n        is_verbose: bool = None,\n        language_pandoc: str = None,\n        language_spacy: str = None,\n        language_tesseract: str = None,\n        output_directory: str = None,\n    ) -&gt; None:\n\"\"\"Document content recognition for a specific file.\n\n        This method extracts the document content structure from a\n        given document and stores it in JSON format. For this purpose,\n        all non-pdf documents and all scanned pdf documents are first\n        converted into a searchable pdf format. Depending on the file\n        format, the tools Pandoc, pdf2image or Tesseract OCR are used\n        for this purpose. PDFlib TET then extracts the text and metadata\n        from the searchable pdf file and makes them available in XML format.\n        spaCY generates qualified tokens from the document text, and these\n        token data are then made available together with the metadata in a\n        JSON format.\n\n        Args:\n            full_name_in (str):\n                Full file name of the document file.\n            document_id (int, optional):\n                Document identification.\n                Defaults to -1 i.e. no document identification.\n            full_name_orig (str, optional):\n                Original full file name.\n                Defaults to the full file name of the document file.\n            is_delete_auxiliary_files (bool, optional):\n                Delete the auxiliary files after a successful processing step.\n                Defaults to parameter `delete_auxiliary_files` in `setup.cfg`.\n            is_verbose (bool, optional):\n                Display progress messages for processing.\n                Defaults to parameter `verbose` in `setup.cfg`.\n            language_pandoc (str, optional):\n                Pandoc language code.\n                Defaults to English.\n            language_spacy (str, optional):\n                spaCy language code.\n                Defaults to English transformer pipeline (roberta-base)..\n            language_tesseract (str, optional):\n                Tesseract OCR language code.\n                Defaults to English.\n            output_directory (str, optional):\n                Directory for the flat files to be created.\n                Defaults to the directory of the document file.\n\n        Raises:\n            RuntimeError: Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR.\n        \"\"\"\n        # Initialise the logging functionality.\n        dcr_core.core_glob.initialise_logger()\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_glob.logger.debug(\"param full_name_in=%s\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"param document_id =%i\", document_id)\n\n        self._document_init()\n\n        self._document_id = document_id if document_id else -1\n        self._full_name_in = full_name_in\n        self._full_name_orig = full_name_orig if full_name_orig else full_name_in\n        self._language_pandoc = language_pandoc if language_pandoc else dcr_core.cls_nlp_core.NLPCore.LANGUAGE_PANDOC_DEFAULT\n        self._language_spacy = language_spacy if language_spacy else dcr_core.cls_nlp_core.NLPCore.LANGUAGE_SPACY_DEFAULT\n        self._language_tesseract = language_tesseract if language_tesseract else dcr_core.cls_nlp_core.NLPCore.LANGUAGE_TESSERACT_DEFAULT\n\n        dcr_core.core_glob.logger.debug(\"param full_name_orig    =%s\", self._full_name_orig)\n        dcr_core.core_glob.logger.debug(\"param language_pandoc   =%s\", self._language_pandoc)\n        dcr_core.core_glob.logger.debug(\"param language_spacy    =%s\", self._language_spacy)\n        dcr_core.core_glob.logger.debug(\"param language_tesseract=%s\", self._language_tesseract)\n\n        # Load the configuration parameters.\n        dcr_core.core_glob.setup = dcr_core.cls_setup.Setup()\n\n        self._is_delete_auxiliary_files = (\n            is_delete_auxiliary_files if is_delete_auxiliary_files is not None else dcr_core.core_glob.setup.is_delete_auxiliary_files\n        )\n        self._is_verbose = is_verbose if is_verbose is not None else dcr_core.core_glob.setup.is_verbose\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing document file {self._full_name_orig}\")\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"Language key Pandoc            {self._language_pandoc}\")\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"Language key spaCy             {self._language_spacy}\")\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"Language key Tesseract OCR     {self._language_tesseract}\")\n\n        (\n            full_name_in_directory,\n            self._full_name_in_stem_name,\n            self._full_name_in_extension,\n        ) = dcr_core.core_utils.get_components_from_full_name(self._full_name_in)\n\n        self._full_name_in_directory = output_directory if output_directory is not None else full_name_in_directory\n\n        self._full_name_in_extension_int = (\n            self._full_name_in_extension.lower() if self._full_name_in_extension else self._full_name_in_extension\n        )\n\n        self._document_check_extension()\n\n        self._document_pandoc()\n\n        self._document_pdf2image()\n\n        self._document_tesseract()\n\n        self._document_pdflib()\n\n        self._document_parser()\n\n        self._document_tokenizer()\n\n        dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing document file {self._full_name_orig}\")\n\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    # ------------------------------------------------------------------\n    # Converting a Non-PDF file to a PDF file.\n    # ------------------------------------------------------------------\n    @classmethod\n    def pandoc(\n        cls,\n        full_name_in: str,\n        full_name_out: str,\n        language_pandoc: str,\n    ) -&gt; tuple[str, str]:\n\"\"\"Convert a Non-PDF file to a PDF file.\n\n        The following file formats are converted into\n        PDF format here with the help of Pandoc:\n\n        - csv - comma-separated values\n        - docx - Office Open XML\n        - epub - e-book file format\n        - html - HyperText Markup Language\n        - odt - Open Document Format for Office Applications\n        - rst - reStructuredText (RST\n        - rtf - Rich Text Format\n\n        Args:\n            full_name_in (str):\n                    The directory name and file name of the input file.\n            full_name_out (str):\n                    The directory name and file name of the output file.\n            language_pandoc (str):\n                    The Pandoc name of the document language.\n\n        Returns:\n            tuple[str, str]:\n                    (\"ok\", \"\") if the processing has been completed successfully,\n                               otherwise a corresponding error code and error message.\n        \"\"\"\n        try:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n        except AttributeError:\n            dcr_core.core_glob.initialise_logger()\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_glob.logger.debug(\"param full_name_in   =%s\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"param full_name_out  =%s\", full_name_out)\n        dcr_core.core_glob.logger.debug(\"param language_pandoc=%s\", language_pandoc)\n\n        # Convert the document\n        extra_args = [\n            f\"--pdf-engine={Process.PANDOC_PDF_ENGINE_XELATEX}\",\n            \"-V\",\n            f\"lang:{language_pandoc}\",\n        ]\n\n        try:\n            pypandoc.convert_file(\n                full_name_in,\n                dcr_core.core_glob.FILE_TYPE_PDF,\n                extra_args=extra_args,\n                outputfile=full_name_out,\n            )\n\n            if len(PyPDF2.PdfReader(full_name_out).pages) == 0:\n                error_msg = Process.ERROR_31_911.replace(\"{full_name}\", full_name_out)\n                dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n                dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n                return error_msg[:6], error_msg\n\n        except FileNotFoundError:\n            error_msg = Process.ERROR_31_902.replace(\"{full_name}\", full_name_in)\n            dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg\n        except RuntimeError as err:\n            error_msg = Process.ERROR_31_903.replace(\"{full_name}\", full_name_in).replace(\"{error_msg}\", str(err))\n            dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg\n\n        dcr_core.core_glob.logger.debug(\"return               =%s\", dcr_core.core_glob.RETURN_OK)\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n        return dcr_core.core_glob.RETURN_OK\n\n    # ------------------------------------------------------------------\n    # Extracting the text from the PDF document.\n    # ------------------------------------------------------------------\n    @classmethod\n    def parser(\n        cls,\n        full_name_in: str,\n        full_name_out: str,\n        no_pdf_pages: int,\n        document_id: int = -1,\n        full_name_orig: str = dcr_core.core_glob.INFORMATION_NOT_YET_AVAILABLE,\n    ) -&gt; tuple[str, str]:\n\"\"\"Extract the text from the PDF document.\n\n        From the line-oriented XML output file of PDFlib TET,\n        the text and relevant metadata are extracted with the\n        help of an XML parser and stored in a JSON file.\n\n        Args:\n            full_name_in (str):\n                    The directory name and file name of the input file.\n            full_name_out (str):\n                    The directory name and file name of the output file.\n            no_pdf_pages (int):\n                    Total number of PDF pages.\n            document_id (int, optional):\n                    The identification number of the document.\n                    Defaults to -1.\n            full_name_orig (str, optional):\n                    The file name of the originating document.\n                    Defaults to dcr_core.core_glob.INFORMATION_NOT_YET_AVAILABLE.\n\n        Returns:\n            tuple[str, str]:\n                    (\"ok\", \"\") if the processing has been completed successfully,\n                               otherwise a corresponding error code and error message.\n        \"\"\"\n        try:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n        except AttributeError:\n            dcr_core.core_glob.initialise_logger()\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_glob.logger.debug(\"param document_id   =%i\", document_id)\n        dcr_core.core_glob.logger.debug(\"param full_name_orig=%s\", full_name_orig)\n        dcr_core.core_glob.logger.debug(\"param full_name_in  =%s\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"param full_name_out =%s\", full_name_out)\n        dcr_core.core_glob.logger.debug(\"param no_pdf_pages  =%i\", no_pdf_pages)\n\n        try:\n            # Create the Element tree object\n            tree = defusedxml.ElementTree.parse(full_name_in)\n\n            # Get the root Element\n            root = tree.getroot()\n\n            dcr_core.core_glob.text_parser = dcr_core.cls_text_parser.TextParser()\n\n            for child in root:\n                child_tag = child.tag[dcr_core.cls_nlp_core.NLPCore.PARSE_ELEM_FROM :]\n                match child_tag:\n                    case dcr_core.cls_nlp_core.NLPCore.PARSE_ELEM_DOCUMENT:\n                        dcr_core.core_glob.text_parser.parse_tag_document(\n                            directory_name=os.path.dirname(full_name_in),\n                            document_id=document_id,\n                            environment_variant=dcr_core.core_glob.setup.environment_variant,\n                            file_name_curr=os.path.basename(full_name_in),\n                            file_name_next=full_name_out,\n                            file_name_orig=full_name_orig,\n                            no_pdf_pages=no_pdf_pages,\n                            parent=child,\n                            parent_tag=child_tag,\n                        )\n                    case dcr_core.cls_nlp_core.NLPCore.PARSE_ELEM_CREATION:\n                        pass\n        except FileNotFoundError:\n            error_msg = Process.ERROR_61_901.replace(\"{full_name}\", full_name_in)\n            dcr_core.core_glob.logger.debug(\"return              =%s\", (error_msg[:6], error_msg))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg\n\n        dcr_core.core_glob.logger.debug(\"return              =%s\", dcr_core.core_glob.RETURN_OK)\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n        return dcr_core.core_glob.RETURN_OK\n\n    # ------------------------------------------------------------------\n    # Converting a scanned PDF file to a set of image files.\n    # ------------------------------------------------------------------\n    @classmethod\n    def pdf2image(\n        cls,\n        full_name_in: str,\n    ) -&gt; tuple[str, str, list[tuple[str, str]]]:\n\"\"\"Convert a scanned PDF file to a set of image files.\n\n        To extract the text from a scanned PDF document, it must\n        first be converted into one or more image files, depending\n        on the number of pages. Then these image files are converted\n        into a normal PDF document with the help of an OCR programme.\n        The input file for this method must be a scanned PDF document,\n        which is then converted into image files with the help of PDF2Image.\n\n        Args:\n            full_name_in (str):\n                    The directory name and file name of the input file.\n\n        Returns:\n            tuple[str, str, list[tuple[str,str]]]:\n                    (\"ok\", \"\", [...]) if the processing has been completed successfully,\n                                      otherwise a corresponding error code and error message.\n        \"\"\"\n        try:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n        except AttributeError:\n            dcr_core.core_glob.initialise_logger()\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_glob.logger.debug(\"param full_name_in=%s\", full_name_in)\n\n        try:\n            images = pdf2image.convert_from_path(full_name_in)\n\n            children: list[tuple[str, str]] = []\n            no_children = 0\n\n            directory_name = os.path.dirname(full_name_in)\n            stem_name = os.path.splitext(os.path.basename(full_name_in))[0]\n\n            try:\n                os.remove(\n                    dcr_core.core_utils.get_full_name_from_components(\n                        directory_name,\n                        stem_name\n                        + \"_*.\"\n                        + (\n                            dcr_core.core_glob.FILE_TYPE_PNG\n                            if dcr_core.core_glob.setup.pdf2image_type == dcr_core.cls_setup.Setup.PDF2IMAGE_TYPE_PNG\n                            else dcr_core.core_glob.FILE_TYPE_JPEG\n                        ),\n                    )\n                )\n            except OSError:\n                pass\n\n            # Store the image pages\n            for img in images:\n                no_children += 1\n\n                file_name_next = (\n                    stem_name\n                    + \"_\"\n                    + str(no_children)\n                    + \".\"\n                    + (\n                        dcr_core.core_glob.FILE_TYPE_PNG\n                        if dcr_core.core_glob.setup.pdf2image_type == dcr_core.cls_setup.Setup.PDF2IMAGE_TYPE_PNG\n                        else dcr_core.core_glob.FILE_TYPE_JPEG\n                    )\n                )\n\n                full_name_next = dcr_core.core_utils.get_full_name_from_components(\n                    directory_name,\n                    file_name_next,\n                )\n\n                img.save(\n                    full_name_next,\n                    dcr_core.core_glob.setup.pdf2image_type,\n                )\n\n                children.append((file_name_next, full_name_next))\n        except PDFPageCountError as err:\n            error_msg = (\n                Process.ERROR_21_901.replace(\"{full_name}\", full_name_in)\n                .replace(\"{error_type}\", str(type(err)))\n                .replace(\"{error_msg}\", str(err))\n            )\n            dcr_core.core_glob.logger.debug(\"return            =%s\", (error_msg[:6], error_msg, []))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg, []\n\n        dcr_core.core_glob.logger.debug(\n            \"return            =%s\", (dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children)\n        )\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n        return dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children\n\n    # ------------------------------------------------------------------\n    # Processing a PDF file with PDFlib TET.\n    # ------------------------------------------------------------------\n    @classmethod\n    def pdflib(\n        cls,\n        full_name_in: str,\n        full_name_out: str,\n        document_opt_list: str,\n        page_opt_list: str,\n    ) -&gt; tuple[str, str]:\n\"\"\"Process a PDF file with PDFlib TET.\n\n        The data from a PDF file is made available in XML files\n        with the help of PDFlib TET. The granularity of the XML\n        files can be word, line or paragraph depending on the\n        document and page options selected.\n\n        Args:\n            full_name_in (str):\n                    Directory name and file name of the input file.\n            full_name_out (str):\n                    Directory name and file name of the output file.\n            document_opt_list (str):\n                    Document level options:\n                        word: engines={noannotation noimage text notextcolor novector}\n                        line: engines={noannotation noimage text notextcolor novector}\n                        page: engines={noannotation noimage text notextcolor novector} lineseparator=U+0020\n            page_opt_list (str):\n                    Page level options:\n                        word: granularity=word tetml={elements={line}}\n                        line: granularity=line\n                        page: granularity=page\n\n        Returns:\n            tuple[str, str]:\n                    (\"ok\", \"\") if the processing has been completed successfully,\n                               otherwise a corresponding error code and error message.\n        \"\"\"\n        try:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n        except AttributeError:\n            dcr_core.core_glob.initialise_logger()\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_glob.logger.debug(\"param full_name_in     =%s\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"param full_name_out    =%s\", full_name_out)\n        dcr_core.core_glob.logger.debug(\"param document_opt_list=%s\", document_opt_list)\n        dcr_core.core_glob.logger.debug(\"param page_opt_list    =%s\", page_opt_list)\n\n        tet = dcr_core.PDFlib.TET.TET()\n\n        doc_opt_list = f\"tetml={{filename={{{full_name_out}}}}} {document_opt_list}\"\n\n        if (file_curr := tet.open_document(full_name_in, doc_opt_list)) == -1:\n            error_msg = (\n                Process.ERROR_51_901.replace(\"{full_name}\", full_name_in)\n                .replace(\"{error_no}\", str(tet.get_errnum()))\n                .replace(\"{api_name}\", tet.get_apiname() + \"()\")\n                .replace(\"{error_msg}\", tet.get_errmsg())\n            )\n            dcr_core.core_glob.logger.debug(\"return                 =%s\", (error_msg[:6], error_msg))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg\n\n        # get number of pages in the document */\n        no_pages = tet.pcos_get_number(file_curr, \"length:pages\")\n\n        # loop over pages in the document */\n        for page_no in range(1, int(no_pages) + 1):\n            tet.process_page(file_curr, page_no, page_opt_list)\n\n        # This could be combined with the last page-related call\n        tet.process_page(file_curr, 0, \"tetml={trailer}\")\n\n        tet.close_document(file_curr)\n\n        tet.delete()\n\n        dcr_core.core_glob.logger.debug(\"return                 =%s\", dcr_core.core_glob.LOGGER_END)\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n        return dcr_core.core_glob.RETURN_OK\n\n    # ------------------------------------------------------------------\n    # Converting image files to PDF files via OCR.\n    # ------------------------------------------------------------------\n    @classmethod\n    def tesseract(\n        cls,\n        full_name_in: str,\n        full_name_out: str,\n        language_tesseract: str,\n    ) -&gt; tuple[str, str, list[str]]:\n\"\"\"Convert image files to PDF files via OCR.\n\n        The documents of the following document types are converted\n        to the PDF format using Tesseract OCR:\n\n        - bmp - bitmap image file\n        - gif - Graphics Interchange Format\n        - jp2 - JPEG 2000\n        - jpeg - Joint Photographic Experts Group\n        - png - Portable Network Graphics\n        - pnm - portable any-map format\n        - tif - Tag Image File Format\n        - tiff - Tag Image File Format\n        - webp - Image file format with lossless and lossy compression\n\n        After processing with Tesseract OCR, the files split previously\n        into multiple image files are combined into a single PDF document.\n\n        Args:\n            full_name_in (str):\n                    The directory name and file name of the input file.\n            full_name_out (str):\n                    The directory name and file name of the output file.\n            language_tesseract (str):\n                    The Tesseract name of the document language.\n\n        Returns:\n            tuple[str, str, list[str]]:\n                    (\"ok\", \"\", [...]) if the processing has been completed successfully,\n                                      otherwise a corresponding error code and error message.\n        \"\"\"\n        try:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n        except AttributeError:\n            dcr_core.core_glob.initialise_logger()\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_glob.logger.debug(\"param full_name_in      =%s\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"param full_name_out     =%s\", full_name_out)\n        dcr_core.core_glob.logger.debug(\"param language_tesseract=%s\", language_tesseract)\n\n        children: list[str] = []\n\n        pdf_writer = PyPDF2.PdfWriter()\n\n        for full_name in sorted(glob.glob(full_name_in)):\n            try:\n                pdf = pytesseract.image_to_pdf_or_hocr(\n                    extension=\"pdf\",\n                    image=full_name,\n                    lang=language_tesseract,\n                    timeout=dcr_core.core_glob.setup.tesseract_timeout,\n                )\n\n                with open(full_name_out, \"w+b\") as file_handle:\n                    # PDF type is bytes by default\n                    file_handle.write(pdf)\n\n                if len(PyPDF2.PdfReader(full_name_out).pages) == 0:\n                    error_msg = Process.ERROR_41_911.replace(\"{full_name_out}\", full_name_out)\n                    dcr_core.core_glob.logger.debug(\"return                  =%s\", (error_msg[:6], error_msg, []))\n                    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n                    return error_msg[:6], error_msg, []\n\n                pdf_reader = PyPDF2.PdfReader(full_name_out)\n\n                for page in pdf_reader.pages:\n                    # Add each page to the writer object\n                    pdf_writer.add_page(page)\n\n                children.append(full_name)\n\n            except RuntimeError as err:\n                error_msg = Process.ERROR_41_901.replace(\"{full_name}\", full_name_in).replace(\"{error_msg}\", str(err))\n                dcr_core.core_glob.logger.debug(\"return                  =%s\", (error_msg[:6], error_msg, []))\n                dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n                return error_msg[:6], error_msg, []\n\n        # Write out the merged PDF\n        with open(full_name_out, \"wb\") as file_handle:\n            pdf_writer.write(file_handle)\n\n        dcr_core.core_glob.logger.debug(\n            \"return                  =%s\", (dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children)\n        )\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n        return dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children\n\n    # ------------------------------------------------------------------\n    # Tokenizing the text from the PDF document.\n    # ------------------------------------------------------------------\n    @classmethod\n    def tokenizer(\n        cls,\n        full_name_in: str,\n        full_name_out: str,\n        pipeline_name: str,\n        document_id: int = -1,\n        full_name_orig: str = \"\",\n        no_lines_footer: int = -1,\n        no_lines_header: int = -1,\n        no_lines_toc: int = -1,\n    ) -&gt; tuple[str, str]:\n\"\"\"Tokenizing the text from the PDF document.\n\n        The line-oriented text is broken down into qualified\n        tokens with the means of SpaCy.\n\n        Args:\n            full_name_in (str):\n                    The directory name and file name of the input file.\n            full_name_out (str):\n                    The directory name and file name of the output file.\n            pipeline_name (str):\n                    The loaded SpaCy pipeline.\n            document_id (int, optional):\n                    The identification number of the document.\n                    Defaults to -1.\n            full_name_orig (str, optional):\n                    The file name of the originating document. Defaults to \"\".\n            no_lines_footer (int, optional):\n                    Total number of footer lines.\n                    Defaults to -1.\n            no_lines_header (int, optional):\n                    Total number of header lines.\n                    Defaults to -1.\n            no_lines_toc (int, optional):\n                    Total number of TOC lines.\n                    Defaults to -1.\n\n        Returns:\n            tuple[str, str]:\n                    (\"ok\", \"\") if the processing has been completed successfully,\n                               otherwise a corresponding error code and error message.\n        \"\"\"\n        try:\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n        except AttributeError:\n            dcr_core.core_glob.initialise_logger()\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n        dcr_core.core_glob.logger.debug(\"param document_id    =%i\", document_id)\n        dcr_core.core_glob.logger.debug(\"param full_name_in   =%s\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"param full_name_orig =%s\", full_name_orig)\n        dcr_core.core_glob.logger.debug(\"param full_name_out  =%s\", full_name_out)\n        dcr_core.core_glob.logger.debug(\"param no_lines_footer=%i\", no_lines_footer)\n        dcr_core.core_glob.logger.debug(\"param no_lines_header=%i\", no_lines_header)\n        dcr_core.core_glob.logger.debug(\"param no_lines_toc   =%i\", no_lines_toc)\n        dcr_core.core_glob.logger.debug(\"param pipeline_name  =%s\", pipeline_name)\n\n        try:\n            dcr_core.core_glob.text_parser = dcr_core.cls_text_parser.TextParser.from_files(\n                file_encoding=dcr_core.core_glob.FILE_ENCODING_DEFAULT, full_name_line=full_name_in\n            )\n\n            dcr_core.core_glob.tokenizer_spacy.process_document(\n                document_id=document_id,\n                file_name_next=full_name_out,\n                file_name_orig=full_name_orig,\n                no_lines_footer=no_lines_footer,\n                no_lines_header=no_lines_header,\n                no_lines_toc=no_lines_toc,\n                pipeline_name=pipeline_name,\n            )\n\n        except FileNotFoundError:\n            error_msg = Process.ERROR_71_901.replace(\"{full_name}\", full_name_in)\n            dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg\n\n        dcr_core.core_glob.logger.debug(\"return               =%s\", dcr_core.core_glob.RETURN_OK)\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n        return dcr_core.core_glob.RETURN_OK\n</code></pre>"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.document","title":"<code>document(full_name_in, document_id=None, full_name_orig=None, is_delete_auxiliary_files=None, is_verbose=None, language_pandoc=None, language_spacy=None, language_tesseract=None, output_directory=None)</code>","text":"<p>Document content recognition for a specific file.</p> <p>This method extracts the document content structure from a given document and stores it in JSON format. For this purpose, all non-pdf documents and all scanned pdf documents are first converted into a searchable pdf format. Depending on the file format, the tools Pandoc, pdf2image or Tesseract OCR are used for this purpose. PDFlib TET then extracts the text and metadata from the searchable pdf file and makes them available in XML format. spaCY generates qualified tokens from the document text, and these token data are then made available together with the metadata in a JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>full_name_in</code> <code>str</code> <p>Full file name of the document file.</p> required <code>document_id</code> <code>int</code> <p>Document identification. Defaults to -1 i.e. no document identification.</p> <code>None</code> <code>full_name_orig</code> <code>str</code> <p>Original full file name. Defaults to the full file name of the document file.</p> <code>None</code> <code>is_delete_auxiliary_files</code> <code>bool</code> <p>Delete the auxiliary files after a successful processing step. Defaults to parameter <code>delete_auxiliary_files</code> in <code>setup.cfg</code>.</p> <code>None</code> <code>is_verbose</code> <code>bool</code> <p>Display progress messages for processing. Defaults to parameter <code>verbose</code> in <code>setup.cfg</code>.</p> <code>None</code> <code>language_pandoc</code> <code>str</code> <p>Pandoc language code. Defaults to English.</p> <code>None</code> <code>language_spacy</code> <code>str</code> <p>spaCy language code. Defaults to English transformer pipeline (roberta-base)..</p> <code>None</code> <code>language_tesseract</code> <code>str</code> <p>Tesseract OCR language code. Defaults to English.</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Directory for the flat files to be created. Defaults to the directory of the document file.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>def document(\n    self,\n    full_name_in: str,\n    document_id: int = None,\n    full_name_orig: str = None,\n    is_delete_auxiliary_files: bool = None,\n    is_verbose: bool = None,\n    language_pandoc: str = None,\n    language_spacy: str = None,\n    language_tesseract: str = None,\n    output_directory: str = None,\n) -&gt; None:\n\"\"\"Document content recognition for a specific file.\n\n    This method extracts the document content structure from a\n    given document and stores it in JSON format. For this purpose,\n    all non-pdf documents and all scanned pdf documents are first\n    converted into a searchable pdf format. Depending on the file\n    format, the tools Pandoc, pdf2image or Tesseract OCR are used\n    for this purpose. PDFlib TET then extracts the text and metadata\n    from the searchable pdf file and makes them available in XML format.\n    spaCY generates qualified tokens from the document text, and these\n    token data are then made available together with the metadata in a\n    JSON format.\n\n    Args:\n        full_name_in (str):\n            Full file name of the document file.\n        document_id (int, optional):\n            Document identification.\n            Defaults to -1 i.e. no document identification.\n        full_name_orig (str, optional):\n            Original full file name.\n            Defaults to the full file name of the document file.\n        is_delete_auxiliary_files (bool, optional):\n            Delete the auxiliary files after a successful processing step.\n            Defaults to parameter `delete_auxiliary_files` in `setup.cfg`.\n        is_verbose (bool, optional):\n            Display progress messages for processing.\n            Defaults to parameter `verbose` in `setup.cfg`.\n        language_pandoc (str, optional):\n            Pandoc language code.\n            Defaults to English.\n        language_spacy (str, optional):\n            spaCy language code.\n            Defaults to English transformer pipeline (roberta-base)..\n        language_tesseract (str, optional):\n            Tesseract OCR language code.\n            Defaults to English.\n        output_directory (str, optional):\n            Directory for the flat files to be created.\n            Defaults to the directory of the document file.\n\n    Raises:\n        RuntimeError: Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR.\n    \"\"\"\n    # Initialise the logging functionality.\n    dcr_core.core_glob.initialise_logger()\n\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n    dcr_core.core_glob.logger.debug(\"param full_name_in=%s\", full_name_in)\n    dcr_core.core_glob.logger.debug(\"param document_id =%i\", document_id)\n\n    self._document_init()\n\n    self._document_id = document_id if document_id else -1\n    self._full_name_in = full_name_in\n    self._full_name_orig = full_name_orig if full_name_orig else full_name_in\n    self._language_pandoc = language_pandoc if language_pandoc else dcr_core.cls_nlp_core.NLPCore.LANGUAGE_PANDOC_DEFAULT\n    self._language_spacy = language_spacy if language_spacy else dcr_core.cls_nlp_core.NLPCore.LANGUAGE_SPACY_DEFAULT\n    self._language_tesseract = language_tesseract if language_tesseract else dcr_core.cls_nlp_core.NLPCore.LANGUAGE_TESSERACT_DEFAULT\n\n    dcr_core.core_glob.logger.debug(\"param full_name_orig    =%s\", self._full_name_orig)\n    dcr_core.core_glob.logger.debug(\"param language_pandoc   =%s\", self._language_pandoc)\n    dcr_core.core_glob.logger.debug(\"param language_spacy    =%s\", self._language_spacy)\n    dcr_core.core_glob.logger.debug(\"param language_tesseract=%s\", self._language_tesseract)\n\n    # Load the configuration parameters.\n    dcr_core.core_glob.setup = dcr_core.cls_setup.Setup()\n\n    self._is_delete_auxiliary_files = (\n        is_delete_auxiliary_files if is_delete_auxiliary_files is not None else dcr_core.core_glob.setup.is_delete_auxiliary_files\n    )\n    self._is_verbose = is_verbose if is_verbose is not None else dcr_core.core_glob.setup.is_verbose\n\n    dcr_core.core_utils.progress_msg(self._is_verbose, f\"Start processing document file {self._full_name_orig}\")\n    dcr_core.core_utils.progress_msg(self._is_verbose, f\"Language key Pandoc            {self._language_pandoc}\")\n    dcr_core.core_utils.progress_msg(self._is_verbose, f\"Language key spaCy             {self._language_spacy}\")\n    dcr_core.core_utils.progress_msg(self._is_verbose, f\"Language key Tesseract OCR     {self._language_tesseract}\")\n\n    (\n        full_name_in_directory,\n        self._full_name_in_stem_name,\n        self._full_name_in_extension,\n    ) = dcr_core.core_utils.get_components_from_full_name(self._full_name_in)\n\n    self._full_name_in_directory = output_directory if output_directory is not None else full_name_in_directory\n\n    self._full_name_in_extension_int = (\n        self._full_name_in_extension.lower() if self._full_name_in_extension else self._full_name_in_extension\n    )\n\n    self._document_check_extension()\n\n    self._document_pandoc()\n\n    self._document_pdf2image()\n\n    self._document_tesseract()\n\n    self._document_pdflib()\n\n    self._document_parser()\n\n    self._document_tokenizer()\n\n    dcr_core.core_utils.progress_msg(self._is_verbose, f\"End   processing document file {self._full_name_orig}\")\n\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n</code></pre>"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.pandoc","title":"<code>pandoc(full_name_in, full_name_out, language_pandoc)</code>  <code>classmethod</code>","text":"<p>Convert a Non-PDF file to a PDF file.</p> <p>The following file formats are converted into PDF format here with the help of Pandoc:</p> <ul> <li>csv - comma-separated values</li> <li>docx - Office Open XML</li> <li>epub - e-book file format</li> <li>html - HyperText Markup Language</li> <li>odt - Open Document Format for Office Applications</li> <li>rst - reStructuredText (RST</li> <li>rtf - Rich Text Format</li> </ul> <p>Parameters:</p> Name Type Description Default <code>full_name_in</code> <code>str</code> <pre><code>The directory name and file name of the input file.\n</code></pre> required <code>full_name_out</code> <code>str</code> <pre><code>The directory name and file name of the output file.\n</code></pre> required <code>language_pandoc</code> <code>str</code> <pre><code>The Pandoc name of the document language.\n</code></pre> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]:     (\"ok\", \"\") if the processing has been completed successfully,                otherwise a corresponding error code and error message.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>@classmethod\ndef pandoc(\n    cls,\n    full_name_in: str,\n    full_name_out: str,\n    language_pandoc: str,\n) -&gt; tuple[str, str]:\n\"\"\"Convert a Non-PDF file to a PDF file.\n\n    The following file formats are converted into\n    PDF format here with the help of Pandoc:\n\n    - csv - comma-separated values\n    - docx - Office Open XML\n    - epub - e-book file format\n    - html - HyperText Markup Language\n    - odt - Open Document Format for Office Applications\n    - rst - reStructuredText (RST\n    - rtf - Rich Text Format\n\n    Args:\n        full_name_in (str):\n                The directory name and file name of the input file.\n        full_name_out (str):\n                The directory name and file name of the output file.\n        language_pandoc (str):\n                The Pandoc name of the document language.\n\n    Returns:\n        tuple[str, str]:\n                (\"ok\", \"\") if the processing has been completed successfully,\n                           otherwise a corresponding error code and error message.\n    \"\"\"\n    try:\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n    except AttributeError:\n        dcr_core.core_glob.initialise_logger()\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n    dcr_core.core_glob.logger.debug(\"param full_name_in   =%s\", full_name_in)\n    dcr_core.core_glob.logger.debug(\"param full_name_out  =%s\", full_name_out)\n    dcr_core.core_glob.logger.debug(\"param language_pandoc=%s\", language_pandoc)\n\n    # Convert the document\n    extra_args = [\n        f\"--pdf-engine={Process.PANDOC_PDF_ENGINE_XELATEX}\",\n        \"-V\",\n        f\"lang:{language_pandoc}\",\n    ]\n\n    try:\n        pypandoc.convert_file(\n            full_name_in,\n            dcr_core.core_glob.FILE_TYPE_PDF,\n            extra_args=extra_args,\n            outputfile=full_name_out,\n        )\n\n        if len(PyPDF2.PdfReader(full_name_out).pages) == 0:\n            error_msg = Process.ERROR_31_911.replace(\"{full_name}\", full_name_out)\n            dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg\n\n    except FileNotFoundError:\n        error_msg = Process.ERROR_31_902.replace(\"{full_name}\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n        return error_msg[:6], error_msg\n    except RuntimeError as err:\n        error_msg = Process.ERROR_31_903.replace(\"{full_name}\", full_name_in).replace(\"{error_msg}\", str(err))\n        dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n        return error_msg[:6], error_msg\n\n    dcr_core.core_glob.logger.debug(\"return               =%s\", dcr_core.core_glob.RETURN_OK)\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    return dcr_core.core_glob.RETURN_OK\n</code></pre>"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.parser","title":"<code>parser(full_name_in, full_name_out, no_pdf_pages, document_id=-1, full_name_orig=dcr_core.core_glob.INFORMATION_NOT_YET_AVAILABLE)</code>  <code>classmethod</code>","text":"<p>Extract the text from the PDF document.</p> <p>From the line-oriented XML output file of PDFlib TET, the text and relevant metadata are extracted with the help of an XML parser and stored in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>full_name_in</code> <code>str</code> <pre><code>The directory name and file name of the input file.\n</code></pre> required <code>full_name_out</code> <code>str</code> <pre><code>The directory name and file name of the output file.\n</code></pre> required <code>no_pdf_pages</code> <code>int</code> <pre><code>Total number of PDF pages.\n</code></pre> required <code>document_id</code> <code>int</code> <pre><code>The identification number of the document.\nDefaults to -1.\n</code></pre> <code>-1</code> <code>full_name_orig</code> <code>str</code> <pre><code>The file name of the originating document.\nDefaults to dcr_core.core_glob.INFORMATION_NOT_YET_AVAILABLE.\n</code></pre> <code>dcr_core.core_glob.INFORMATION_NOT_YET_AVAILABLE</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]:     (\"ok\", \"\") if the processing has been completed successfully,                otherwise a corresponding error code and error message.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>@classmethod\ndef parser(\n    cls,\n    full_name_in: str,\n    full_name_out: str,\n    no_pdf_pages: int,\n    document_id: int = -1,\n    full_name_orig: str = dcr_core.core_glob.INFORMATION_NOT_YET_AVAILABLE,\n) -&gt; tuple[str, str]:\n\"\"\"Extract the text from the PDF document.\n\n    From the line-oriented XML output file of PDFlib TET,\n    the text and relevant metadata are extracted with the\n    help of an XML parser and stored in a JSON file.\n\n    Args:\n        full_name_in (str):\n                The directory name and file name of the input file.\n        full_name_out (str):\n                The directory name and file name of the output file.\n        no_pdf_pages (int):\n                Total number of PDF pages.\n        document_id (int, optional):\n                The identification number of the document.\n                Defaults to -1.\n        full_name_orig (str, optional):\n                The file name of the originating document.\n                Defaults to dcr_core.core_glob.INFORMATION_NOT_YET_AVAILABLE.\n\n    Returns:\n        tuple[str, str]:\n                (\"ok\", \"\") if the processing has been completed successfully,\n                           otherwise a corresponding error code and error message.\n    \"\"\"\n    try:\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n    except AttributeError:\n        dcr_core.core_glob.initialise_logger()\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n    dcr_core.core_glob.logger.debug(\"param document_id   =%i\", document_id)\n    dcr_core.core_glob.logger.debug(\"param full_name_orig=%s\", full_name_orig)\n    dcr_core.core_glob.logger.debug(\"param full_name_in  =%s\", full_name_in)\n    dcr_core.core_glob.logger.debug(\"param full_name_out =%s\", full_name_out)\n    dcr_core.core_glob.logger.debug(\"param no_pdf_pages  =%i\", no_pdf_pages)\n\n    try:\n        # Create the Element tree object\n        tree = defusedxml.ElementTree.parse(full_name_in)\n\n        # Get the root Element\n        root = tree.getroot()\n\n        dcr_core.core_glob.text_parser = dcr_core.cls_text_parser.TextParser()\n\n        for child in root:\n            child_tag = child.tag[dcr_core.cls_nlp_core.NLPCore.PARSE_ELEM_FROM :]\n            match child_tag:\n                case dcr_core.cls_nlp_core.NLPCore.PARSE_ELEM_DOCUMENT:\n                    dcr_core.core_glob.text_parser.parse_tag_document(\n                        directory_name=os.path.dirname(full_name_in),\n                        document_id=document_id,\n                        environment_variant=dcr_core.core_glob.setup.environment_variant,\n                        file_name_curr=os.path.basename(full_name_in),\n                        file_name_next=full_name_out,\n                        file_name_orig=full_name_orig,\n                        no_pdf_pages=no_pdf_pages,\n                        parent=child,\n                        parent_tag=child_tag,\n                    )\n                case dcr_core.cls_nlp_core.NLPCore.PARSE_ELEM_CREATION:\n                    pass\n    except FileNotFoundError:\n        error_msg = Process.ERROR_61_901.replace(\"{full_name}\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"return              =%s\", (error_msg[:6], error_msg))\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n        return error_msg[:6], error_msg\n\n    dcr_core.core_glob.logger.debug(\"return              =%s\", dcr_core.core_glob.RETURN_OK)\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    return dcr_core.core_glob.RETURN_OK\n</code></pre>"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.pdf2image","title":"<code>pdf2image(full_name_in)</code>  <code>classmethod</code>","text":"<p>Convert a scanned PDF file to a set of image files.</p> <p>To extract the text from a scanned PDF document, it must first be converted into one or more image files, depending on the number of pages. Then these image files are converted into a normal PDF document with the help of an OCR programme. The input file for this method must be a scanned PDF document, which is then converted into image files with the help of PDF2Image.</p> <p>Parameters:</p> Name Type Description Default <code>full_name_in</code> <code>str</code> <pre><code>The directory name and file name of the input file.\n</code></pre> required <p>Returns:</p> Type Description <code>tuple[str, str, list[tuple[str, str]]]</code> <p>tuple[str, str, list[tuple[str,str]]]:     (\"ok\", \"\", [...]) if the processing has been completed successfully,                       otherwise a corresponding error code and error message.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>@classmethod\ndef pdf2image(\n    cls,\n    full_name_in: str,\n) -&gt; tuple[str, str, list[tuple[str, str]]]:\n\"\"\"Convert a scanned PDF file to a set of image files.\n\n    To extract the text from a scanned PDF document, it must\n    first be converted into one or more image files, depending\n    on the number of pages. Then these image files are converted\n    into a normal PDF document with the help of an OCR programme.\n    The input file for this method must be a scanned PDF document,\n    which is then converted into image files with the help of PDF2Image.\n\n    Args:\n        full_name_in (str):\n                The directory name and file name of the input file.\n\n    Returns:\n        tuple[str, str, list[tuple[str,str]]]:\n                (\"ok\", \"\", [...]) if the processing has been completed successfully,\n                                  otherwise a corresponding error code and error message.\n    \"\"\"\n    try:\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n    except AttributeError:\n        dcr_core.core_glob.initialise_logger()\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n    dcr_core.core_glob.logger.debug(\"param full_name_in=%s\", full_name_in)\n\n    try:\n        images = pdf2image.convert_from_path(full_name_in)\n\n        children: list[tuple[str, str]] = []\n        no_children = 0\n\n        directory_name = os.path.dirname(full_name_in)\n        stem_name = os.path.splitext(os.path.basename(full_name_in))[0]\n\n        try:\n            os.remove(\n                dcr_core.core_utils.get_full_name_from_components(\n                    directory_name,\n                    stem_name\n                    + \"_*.\"\n                    + (\n                        dcr_core.core_glob.FILE_TYPE_PNG\n                        if dcr_core.core_glob.setup.pdf2image_type == dcr_core.cls_setup.Setup.PDF2IMAGE_TYPE_PNG\n                        else dcr_core.core_glob.FILE_TYPE_JPEG\n                    ),\n                )\n            )\n        except OSError:\n            pass\n\n        # Store the image pages\n        for img in images:\n            no_children += 1\n\n            file_name_next = (\n                stem_name\n                + \"_\"\n                + str(no_children)\n                + \".\"\n                + (\n                    dcr_core.core_glob.FILE_TYPE_PNG\n                    if dcr_core.core_glob.setup.pdf2image_type == dcr_core.cls_setup.Setup.PDF2IMAGE_TYPE_PNG\n                    else dcr_core.core_glob.FILE_TYPE_JPEG\n                )\n            )\n\n            full_name_next = dcr_core.core_utils.get_full_name_from_components(\n                directory_name,\n                file_name_next,\n            )\n\n            img.save(\n                full_name_next,\n                dcr_core.core_glob.setup.pdf2image_type,\n            )\n\n            children.append((file_name_next, full_name_next))\n    except PDFPageCountError as err:\n        error_msg = (\n            Process.ERROR_21_901.replace(\"{full_name}\", full_name_in)\n            .replace(\"{error_type}\", str(type(err)))\n            .replace(\"{error_msg}\", str(err))\n        )\n        dcr_core.core_glob.logger.debug(\"return            =%s\", (error_msg[:6], error_msg, []))\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n        return error_msg[:6], error_msg, []\n\n    dcr_core.core_glob.logger.debug(\n        \"return            =%s\", (dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children)\n    )\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    return dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children\n</code></pre>"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.pdflib","title":"<code>pdflib(full_name_in, full_name_out, document_opt_list, page_opt_list)</code>  <code>classmethod</code>","text":"<p>Process a PDF file with PDFlib TET.</p> <p>The data from a PDF file is made available in XML files with the help of PDFlib TET. The granularity of the XML files can be word, line or paragraph depending on the document and page options selected.</p> <p>Parameters:</p> Name Type Description Default <code>full_name_in</code> <code>str</code> <pre><code>Directory name and file name of the input file.\n</code></pre> required <code>full_name_out</code> <code>str</code> <pre><code>Directory name and file name of the output file.\n</code></pre> required <code>document_opt_list</code> <code>str</code> <pre><code>Document level options:\n    word: engines={noannotation noimage text notextcolor novector}\n    line: engines={noannotation noimage text notextcolor novector}\n    page: engines={noannotation noimage text notextcolor novector} lineseparator=U+0020\n</code></pre> required <code>page_opt_list</code> <code>str</code> <pre><code>Page level options:\n    word: granularity=word tetml={elements={line}}\n    line: granularity=line\n    page: granularity=page\n</code></pre> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]:     (\"ok\", \"\") if the processing has been completed successfully,                otherwise a corresponding error code and error message.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>@classmethod\ndef pdflib(\n    cls,\n    full_name_in: str,\n    full_name_out: str,\n    document_opt_list: str,\n    page_opt_list: str,\n) -&gt; tuple[str, str]:\n\"\"\"Process a PDF file with PDFlib TET.\n\n    The data from a PDF file is made available in XML files\n    with the help of PDFlib TET. The granularity of the XML\n    files can be word, line or paragraph depending on the\n    document and page options selected.\n\n    Args:\n        full_name_in (str):\n                Directory name and file name of the input file.\n        full_name_out (str):\n                Directory name and file name of the output file.\n        document_opt_list (str):\n                Document level options:\n                    word: engines={noannotation noimage text notextcolor novector}\n                    line: engines={noannotation noimage text notextcolor novector}\n                    page: engines={noannotation noimage text notextcolor novector} lineseparator=U+0020\n        page_opt_list (str):\n                Page level options:\n                    word: granularity=word tetml={elements={line}}\n                    line: granularity=line\n                    page: granularity=page\n\n    Returns:\n        tuple[str, str]:\n                (\"ok\", \"\") if the processing has been completed successfully,\n                           otherwise a corresponding error code and error message.\n    \"\"\"\n    try:\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n    except AttributeError:\n        dcr_core.core_glob.initialise_logger()\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n    dcr_core.core_glob.logger.debug(\"param full_name_in     =%s\", full_name_in)\n    dcr_core.core_glob.logger.debug(\"param full_name_out    =%s\", full_name_out)\n    dcr_core.core_glob.logger.debug(\"param document_opt_list=%s\", document_opt_list)\n    dcr_core.core_glob.logger.debug(\"param page_opt_list    =%s\", page_opt_list)\n\n    tet = dcr_core.PDFlib.TET.TET()\n\n    doc_opt_list = f\"tetml={{filename={{{full_name_out}}}}} {document_opt_list}\"\n\n    if (file_curr := tet.open_document(full_name_in, doc_opt_list)) == -1:\n        error_msg = (\n            Process.ERROR_51_901.replace(\"{full_name}\", full_name_in)\n            .replace(\"{error_no}\", str(tet.get_errnum()))\n            .replace(\"{api_name}\", tet.get_apiname() + \"()\")\n            .replace(\"{error_msg}\", tet.get_errmsg())\n        )\n        dcr_core.core_glob.logger.debug(\"return                 =%s\", (error_msg[:6], error_msg))\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n        return error_msg[:6], error_msg\n\n    # get number of pages in the document */\n    no_pages = tet.pcos_get_number(file_curr, \"length:pages\")\n\n    # loop over pages in the document */\n    for page_no in range(1, int(no_pages) + 1):\n        tet.process_page(file_curr, page_no, page_opt_list)\n\n    # This could be combined with the last page-related call\n    tet.process_page(file_curr, 0, \"tetml={trailer}\")\n\n    tet.close_document(file_curr)\n\n    tet.delete()\n\n    dcr_core.core_glob.logger.debug(\"return                 =%s\", dcr_core.core_glob.LOGGER_END)\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    return dcr_core.core_glob.RETURN_OK\n</code></pre>"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.tesseract","title":"<code>tesseract(full_name_in, full_name_out, language_tesseract)</code>  <code>classmethod</code>","text":"<p>Convert image files to PDF files via OCR.</p> <p>The documents of the following document types are converted to the PDF format using Tesseract OCR:</p> <ul> <li>bmp - bitmap image file</li> <li>gif - Graphics Interchange Format</li> <li>jp2 - JPEG 2000</li> <li>jpeg - Joint Photographic Experts Group</li> <li>png - Portable Network Graphics</li> <li>pnm - portable any-map format</li> <li>tif - Tag Image File Format</li> <li>tiff - Tag Image File Format</li> <li>webp - Image file format with lossless and lossy compression</li> </ul> <p>After processing with Tesseract OCR, the files split previously into multiple image files are combined into a single PDF document.</p> <p>Parameters:</p> Name Type Description Default <code>full_name_in</code> <code>str</code> <pre><code>The directory name and file name of the input file.\n</code></pre> required <code>full_name_out</code> <code>str</code> <pre><code>The directory name and file name of the output file.\n</code></pre> required <code>language_tesseract</code> <code>str</code> <pre><code>The Tesseract name of the document language.\n</code></pre> required <p>Returns:</p> Type Description <code>tuple[str, str, list[str]]</code> <p>tuple[str, str, list[str]]:     (\"ok\", \"\", [...]) if the processing has been completed successfully,                       otherwise a corresponding error code and error message.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>@classmethod\ndef tesseract(\n    cls,\n    full_name_in: str,\n    full_name_out: str,\n    language_tesseract: str,\n) -&gt; tuple[str, str, list[str]]:\n\"\"\"Convert image files to PDF files via OCR.\n\n    The documents of the following document types are converted\n    to the PDF format using Tesseract OCR:\n\n    - bmp - bitmap image file\n    - gif - Graphics Interchange Format\n    - jp2 - JPEG 2000\n    - jpeg - Joint Photographic Experts Group\n    - png - Portable Network Graphics\n    - pnm - portable any-map format\n    - tif - Tag Image File Format\n    - tiff - Tag Image File Format\n    - webp - Image file format with lossless and lossy compression\n\n    After processing with Tesseract OCR, the files split previously\n    into multiple image files are combined into a single PDF document.\n\n    Args:\n        full_name_in (str):\n                The directory name and file name of the input file.\n        full_name_out (str):\n                The directory name and file name of the output file.\n        language_tesseract (str):\n                The Tesseract name of the document language.\n\n    Returns:\n        tuple[str, str, list[str]]:\n                (\"ok\", \"\", [...]) if the processing has been completed successfully,\n                                  otherwise a corresponding error code and error message.\n    \"\"\"\n    try:\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n    except AttributeError:\n        dcr_core.core_glob.initialise_logger()\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n    dcr_core.core_glob.logger.debug(\"param full_name_in      =%s\", full_name_in)\n    dcr_core.core_glob.logger.debug(\"param full_name_out     =%s\", full_name_out)\n    dcr_core.core_glob.logger.debug(\"param language_tesseract=%s\", language_tesseract)\n\n    children: list[str] = []\n\n    pdf_writer = PyPDF2.PdfWriter()\n\n    for full_name in sorted(glob.glob(full_name_in)):\n        try:\n            pdf = pytesseract.image_to_pdf_or_hocr(\n                extension=\"pdf\",\n                image=full_name,\n                lang=language_tesseract,\n                timeout=dcr_core.core_glob.setup.tesseract_timeout,\n            )\n\n            with open(full_name_out, \"w+b\") as file_handle:\n                # PDF type is bytes by default\n                file_handle.write(pdf)\n\n            if len(PyPDF2.PdfReader(full_name_out).pages) == 0:\n                error_msg = Process.ERROR_41_911.replace(\"{full_name_out}\", full_name_out)\n                dcr_core.core_glob.logger.debug(\"return                  =%s\", (error_msg[:6], error_msg, []))\n                dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n                return error_msg[:6], error_msg, []\n\n            pdf_reader = PyPDF2.PdfReader(full_name_out)\n\n            for page in pdf_reader.pages:\n                # Add each page to the writer object\n                pdf_writer.add_page(page)\n\n            children.append(full_name)\n\n        except RuntimeError as err:\n            error_msg = Process.ERROR_41_901.replace(\"{full_name}\", full_name_in).replace(\"{error_msg}\", str(err))\n            dcr_core.core_glob.logger.debug(\"return                  =%s\", (error_msg[:6], error_msg, []))\n            dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n            return error_msg[:6], error_msg, []\n\n    # Write out the merged PDF\n    with open(full_name_out, \"wb\") as file_handle:\n        pdf_writer.write(file_handle)\n\n    dcr_core.core_glob.logger.debug(\n        \"return                  =%s\", (dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children)\n    )\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    return dcr_core.core_glob.RETURN_OK[0], dcr_core.core_glob.RETURN_OK[1], children\n</code></pre>"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.tokenizer","title":"<code>tokenizer(full_name_in, full_name_out, pipeline_name, document_id=-1, full_name_orig='', no_lines_footer=-1, no_lines_header=-1, no_lines_toc=-1)</code>  <code>classmethod</code>","text":"<p>Tokenizing the text from the PDF document.</p> <p>The line-oriented text is broken down into qualified tokens with the means of SpaCy.</p> <p>Parameters:</p> Name Type Description Default <code>full_name_in</code> <code>str</code> <pre><code>The directory name and file name of the input file.\n</code></pre> required <code>full_name_out</code> <code>str</code> <pre><code>The directory name and file name of the output file.\n</code></pre> required <code>pipeline_name</code> <code>str</code> <pre><code>The loaded SpaCy pipeline.\n</code></pre> required <code>document_id</code> <code>int</code> <pre><code>The identification number of the document.\nDefaults to -1.\n</code></pre> <code>-1</code> <code>full_name_orig</code> <code>str</code> <pre><code>The file name of the originating document. Defaults to \"\".\n</code></pre> <code>''</code> <code>no_lines_footer</code> <code>int</code> <pre><code>Total number of footer lines.\nDefaults to -1.\n</code></pre> <code>-1</code> <code>no_lines_header</code> <code>int</code> <pre><code>Total number of header lines.\nDefaults to -1.\n</code></pre> <code>-1</code> <code>no_lines_toc</code> <code>int</code> <pre><code>Total number of TOC lines.\nDefaults to -1.\n</code></pre> <code>-1</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]:     (\"ok\", \"\") if the processing has been completed successfully,                otherwise a corresponding error code and error message.</p> Source code in <code>src/dcr_core/cls_process.py</code> <pre><code>@classmethod\ndef tokenizer(\n    cls,\n    full_name_in: str,\n    full_name_out: str,\n    pipeline_name: str,\n    document_id: int = -1,\n    full_name_orig: str = \"\",\n    no_lines_footer: int = -1,\n    no_lines_header: int = -1,\n    no_lines_toc: int = -1,\n) -&gt; tuple[str, str]:\n\"\"\"Tokenizing the text from the PDF document.\n\n    The line-oriented text is broken down into qualified\n    tokens with the means of SpaCy.\n\n    Args:\n        full_name_in (str):\n                The directory name and file name of the input file.\n        full_name_out (str):\n                The directory name and file name of the output file.\n        pipeline_name (str):\n                The loaded SpaCy pipeline.\n        document_id (int, optional):\n                The identification number of the document.\n                Defaults to -1.\n        full_name_orig (str, optional):\n                The file name of the originating document. Defaults to \"\".\n        no_lines_footer (int, optional):\n                Total number of footer lines.\n                Defaults to -1.\n        no_lines_header (int, optional):\n                Total number of header lines.\n                Defaults to -1.\n        no_lines_toc (int, optional):\n                Total number of TOC lines.\n                Defaults to -1.\n\n    Returns:\n        tuple[str, str]:\n                (\"ok\", \"\") if the processing has been completed successfully,\n                           otherwise a corresponding error code and error message.\n    \"\"\"\n    try:\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n    except AttributeError:\n        dcr_core.core_glob.initialise_logger()\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_START)\n\n    dcr_core.core_glob.logger.debug(\"param document_id    =%i\", document_id)\n    dcr_core.core_glob.logger.debug(\"param full_name_in   =%s\", full_name_in)\n    dcr_core.core_glob.logger.debug(\"param full_name_orig =%s\", full_name_orig)\n    dcr_core.core_glob.logger.debug(\"param full_name_out  =%s\", full_name_out)\n    dcr_core.core_glob.logger.debug(\"param no_lines_footer=%i\", no_lines_footer)\n    dcr_core.core_glob.logger.debug(\"param no_lines_header=%i\", no_lines_header)\n    dcr_core.core_glob.logger.debug(\"param no_lines_toc   =%i\", no_lines_toc)\n    dcr_core.core_glob.logger.debug(\"param pipeline_name  =%s\", pipeline_name)\n\n    try:\n        dcr_core.core_glob.text_parser = dcr_core.cls_text_parser.TextParser.from_files(\n            file_encoding=dcr_core.core_glob.FILE_ENCODING_DEFAULT, full_name_line=full_name_in\n        )\n\n        dcr_core.core_glob.tokenizer_spacy.process_document(\n            document_id=document_id,\n            file_name_next=full_name_out,\n            file_name_orig=full_name_orig,\n            no_lines_footer=no_lines_footer,\n            no_lines_header=no_lines_header,\n            no_lines_toc=no_lines_toc,\n            pipeline_name=pipeline_name,\n        )\n\n    except FileNotFoundError:\n        error_msg = Process.ERROR_71_901.replace(\"{full_name}\", full_name_in)\n        dcr_core.core_glob.logger.debug(\"return               =%s\", (error_msg[:6], error_msg))\n        dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n        return error_msg[:6], error_msg\n\n    dcr_core.core_glob.logger.debug(\"return               =%s\", dcr_core.core_glob.RETURN_OK)\n    dcr_core.core_glob.logger.debug(dcr_core.core_glob.LOGGER_END)\n\n    return dcr_core.core_glob.RETURN_OK\n</code></pre>"},{"location":"application_configuration/","title":"DCR-CORE - Application - Configuration","text":""},{"location":"application_configuration/#1-logging_cfgyaml","title":"1. <code>logging_cfg.yaml</code>","text":"<p>This file controls the logging behaviour of the application. </p> <p>Default content:</p> <pre><code>version: 1\n\nformatters:\n  simple:\n    format: \"%(asctime)s [%(module)s.py  ] %(levelname)-5s %(funcName)s:%(lineno)d %(message)s\"\n  extended:\n    format: \"%(asctime)s [%(module)s.py  ] %(levelname)-5s %(funcName)s:%(lineno)d \\n%(message)s\"\n\nhandlers:\n  console:\n    class: logging.StreamHandler\n    level: INFO\n    formatter: simple\n\n  file_handler:\n    class: logging.FileHandler\n    level: INFO\n    filename: logging_dcr_core.log\n    formatter: extended\n\nloggers:\n  dcr_core:\n    handlers: [ console ]\nroot:\n  handlers: [ file_handler ]\n</code></pre>"},{"location":"application_configuration/#2-setupcfg","title":"2. <code>setup.cfg</code>","text":"<p>This file controls the behaviour of the <code>DCR-CORE</code> application. </p> <p>The customisable entries are:</p> <pre><code>[dcr_core]\ncreate_extra_file_heading = true\ncreate_extra_file_list_bullet = true\ncreate_extra_file_list_number = true\ncreate_extra_file_table = true\ndelete_auxiliary_files = true\ndirectory_inbox = data/inbox_prod\njson_indent = 4\njson_sort_keys = false\nlt_export_rule_file_heading = data/lt_export_rule_heading.json\nlt_export_rule_file_list_bullet = data/lt_export_rule_list_bullet.json\nlt_export_rule_file_list_number = data/lt_export_rule_list_number.json\nlt_footer_max_distance = 3\nlt_footer_max_lines = 3\nlt_header_max_distance = 3\nlt_header_max_lines = 3\nlt_heading_file_incl_no_ctx = 1\nlt_heading_file_incl_regexp = false\nlt_heading_max_level = 3\nlt_heading_min_pages = 2\nlt_heading_rule_file = none\nlt_heading_tolerance_llx = 10\nlt_list_bullet_min_entries = 2\nlt_list_bullet_rule_file = none\nlt_list_bullet_tolerance_llx = 10\nlt_list_number_file_incl_regexp = false\nlt_list_number_min_entries = 2\nlt_list_number_rule_file = none\nlt_list_number_tolerance_llx = 10\nlt_table_file_incl_empty_columns = true\nlt_toc_last_page = 5\nlt_toc_min_entries = 5\npdf2image_type = jpeg\ntesseract_timeout = 30\ntetml_page = false\ntetml_word = false\ntokenize_2_database = true\ntokenize_2_jsonfile = true\nverbose = true\nverbose_lt_header_footer = false\nverbose_lt_heading = false\nverbose_lt_list_bullet = false\nverbose_lt_list_number = false\nverbose_lt_table = false\nverbose_lt_toc = false\nverbose_parser = none\n</code></pre> Parameter Description create_extra_file_heading Create a separate <code>JSON</code> file with the table of contents. create_extra_file_list_bullet Create a separate <code>JSON</code> file with the bulleted lists. create_extra_file_list_number Create a separate <code>JSON</code> file with the numbered lists. create_extra_file_table Create a separate <code>JSON</code> file with the tables. delete_auxiliary_files Delete the auxiliary files after a successful processing step. directory_inbox Directory for the new documents received. json_indent Improves the readability of the <code>JSON</code> file. json_sort_keys If it is set to <code>true</code>, the keys are set in ascending order else, they appear as in the Python object. lt_export_rule_file_heading File name for the export of the heading rules. lt_export_rule_file_list_bullet File name for the export of the bulleted list rules. lt_export_rule_file_list_number File name for the export of the numbered list rules. lt_footer_max_distance Maximum Levenshtein distance for a footer line. lt_footer_max_lines Maximum number of footers. lt_header_max_distance Maximum Levenshtein distance for a header line. lt_header_max_lines Maximum number of headers. lt_heading_file_incl_no_ctx The number of lines following the heading to be included as context into the <code>JSON</code> file. lt_heading_file_incl_regexp If it is set to <code>true</code>, the regular expression for the heading is included in the <code>JSON</code> file. lt_heading_max_level Maximum level of the heading structure. lt_heading_min_pages Minimum number of pages to determine the headings. lt_heading_rule_file File with rules to determine the headings. lt_heading_tolerance_llx Tolerance of vertical indentation in percent. lt_list_bullet_min_entries Minimum number of entries to determine a bulleted list. lt_list_bullet_rule_file File with rules to determine the bulleted lists. lt_list_bullet_tolerance_llx Tolerance of vertical indentation in percent. lt_list_number_file_incl_regexp If it is set to <code>true</code>, the regular expression for the numbered list is included in the <code>JSON</code> file. lt_list_number_min_entries Minimum number of entries to determine a numbered list. lt_list_number_rule_file File with rules to determine the numbered lists. lt_list_number_tolerance_llx Tolerance of vertical indentation in percent. lt_table_file_incl_empty_columns If it is set to <code>true</code>, the the empty cells are included in the separate <code>JSON</code> file with the tables. lt_toc_last_page Maximum number of pages for the search of the TOC (from the beginning). lt_toc_min_entries Minimum number of TOC entries. pdfimage_type Format of the image files for the scanned <code>pdf</code> document: <code>jpeg</code> or <code>pdf</code>. tesseract_timeout Terminate the tesseract job after a period of time (seconds). tetml_page PDFlib TET granularity 'page'. tetml_word PDFlib TET granularity 'word'. tokenize_2_database Store the tokens in the database table <code>token</code>. tokenize_2_jsonfile Store the tokens in a <code>JSON</code> flat file. verbose Display progress messages for processing. verbose_lt_headers_footers Display progress messages for headers &amp; footers line type determination. verbose_lt_heading Display progress messages for heading line type determination. verbose_lt_list_bullet Display progress messages for line type determination of a bulleted list. verbose_lt_list_number Display progress messages for line type determination of a numbered list. verbose_lt_table Display progress messages for table line type determination. verbose_lt_toc Display progress messages for table of content line type determination. verbose_parser Display progress messages for parsing <code>xml</code> (TETML) : <code>all</code>, <code>none</code> or <code>text</code>. <p>The configuration parameters can be set differently for the individual environments (<code>dev</code>, <code>prod</code> and <code>test</code>).</p> <p>Examples:</p> <pre><code>[dcr_core.env.dev]\ndelete_auxiliary_files = false\ndirectory_inbox = data/inbox_dev\nlt_footer_max_lines = 3\nlt_header_max_lines = 3\nlt_heading_file_incl_no_ctx = 3\nlt_heading_file_incl_regexp = true\nlt_heading_tolerance_llx = 5\nlt_list_bullet_tolerance_llx = 5\nlt_list_number_file_incl_regexp = true\nlt_list_number_tolerance_llx = 5\nlt_table_file_incl_empty_columns = false\ntetml_page = true\ntetml_word = true\n...\n</code></pre>"},{"location":"application_configuration/#4-setupcfg-spacy-token-attributes","title":"4. <code>setup.cfg</code> - spaCy Token Attributes","text":"<p>The tokens derived from the documents can be qualified via various attributes.  The available options are described below.</p> <pre><code>[dcr_core.spacy]\nspacy_ignore_bracket = false\nspacy_ignore_left_punct = false\nspacy_ignore_line_type_footer = false\nspacy_ignore_line_type_header = false\nspacy_ignore_line_type_heading = false\nspacy_ignore_line_type_list_bullet = false\nspacy_ignore_line_type_list_number = false\nspacy_ignore_line_type_table = false\nspacy_ignore_line_type_toc = false\nspacy_ignore_punct = false\nspacy_ignore_quote = false\nspacy_ignore_right_punct = false\nspacy_ignore_space = false\nspacy_ignore_stop = false\nspacy_tkn_attr_cluster = true\nspacy_tkn_attr_dep_ = true\nspacy_tkn_attr_doc = true\nspacy_tkn_attr_ent_iob_ = true\nspacy_tkn_attr_ent_kb_id_ = true\nspacy_tkn_attr_ent_type_ = true\nspacy_tkn_attr_head = true\nspacy_tkn_attr_i = true\nspacy_tkn_attr_idx = true\nspacy_tkn_attr_is_alpha = true\nspacy_tkn_attr_is_ascii = true\nspacy_tkn_attr_is_bracket = true\nspacy_tkn_attr_is_currency = true\nspacy_tkn_attr_is_digit = true\nspacy_tkn_attr_is_left_punct = true\nspacy_tkn_attr_is_lower = true\nspacy_tkn_attr_is_oov = true\nspacy_tkn_attr_is_punct = true\nspacy_tkn_attr_is_quote = true\nspacy_tkn_attr_is_right_punct = true\nspacy_tkn_attr_is_sent_end = true\nspacy_tkn_attr_is_sent_start = true\nspacy_tkn_attr_is_space = true\nspacy_tkn_attr_is_stop = true\nspacy_tkn_attr_is_title = true\nspacy_tkn_attr_is_upper = true\nspacy_tkn_attr_lang_ = true\nspacy_tkn_attr_left_edge = true\nspacy_tkn_attr_lemma_ = true\nspacy_tkn_attr_lex = true\nspacy_tkn_attr_lex_id = true\nspacy_tkn_attr_like_email = true\nspacy_tkn_attr_like_num = true\nspacy_tkn_attr_like_url = true\nspacy_tkn_attr_lower_ = true\nspacy_tkn_attr_morph = true\nspacy_tkn_attr_norm_ = true\nspacy_tkn_attr_orth_ = true\nspacy_tkn_attr_pos_ = true\nspacy_tkn_attr_prefix_ = true\nspacy_tkn_attr_prob = true\nspacy_tkn_attr_rank = true\nspacy_tkn_attr_right_edge = true\nspacy_tkn_attr_sent = true\nspacy_tkn_attr_sentiment = true\nspacy_tkn_attr_shape_ = true\nspacy_tkn_attr_suffix_ = true\nspacy_tkn_attr_tag_ = true\nspacy_tkn_attr_tensor = true\nspacy_tkn_attr_text = true\nspacy_tkn_attr_text_with_ws = true\nspacy_tkn_attr_vocab = true\nspacy_tkn_attr_whitespace_ = true\n</code></pre> Parameter Description spacy_ignore_bracket Ignore the tokens which are brackets ? spacy_ignore_left_punct Ignore the tokens which are left punctuation marks, e.g. \"(\" ? spacy_ignore_line_type_footer Ignore the tokens from line type footer ? spacy_ignore_line_type_header Ignore the tokens from line type header ? spacy_ignore_line_type_heading Ignore the tokens from line type heading ? spacy_ignore_line_type_list_bullet Ignore the tokens from line type bulleted list ? spacy_ignore_line_type_list_number Ignore the tokens from line type numbered list ? spacy_ignore_line_type_table Ignore the tokens from line type table ? spacy_ignore_line_type_toc Ignore the tokens from line type TOC ? spacy_ignore_punct Ignore the tokens which are punctuations ? spacy_ignore_quote Ignore the tokens which are quotation marks ? spacy_ignore_right_punct Ignore the tokens which are right punctuation marks, e.g. \")\" ? spacy_ignore_space Ignore the tokens which consist of whitespace characters ? spacy_ignore_stop Ignore the tokens which are part of a \u201cstop list\u201d ? spacy_tkn_attr_cluster Brown cluster ID. spacy_tkn_attr_dep_ Syntactic dependency relation. spacy_tkn_attr_doc The parent document. spacy_tkn_attr_ent_iob_ IOB code of named entity tag. spacy_tkn_attr_ent_kb_id_ Knowledge base ID that refers to the named entity this token is a part of, if any. spacy_tkn_attr_ent_type_ Named entity type. spacy_tkn_attr_head The syntactic parent, or \u201cgovernor\u201d, of this token. spacy_tkn_attr_i The index of the token within the parent document. spacy_tkn_attr_idx The character offset of the token within the parent document. spacy_tkn_attr_is_alpha Does the token consist of alphabetic characters? spacy_tkn_attr_is_ascii Does the token consist of ASCII characters? Equivalent to all (ord(c) &lt; 128 for c in token.text). spacy_tkn_attr_is_bracket Is the token a bracket? spacy_tkn_attr_is_currency Is the token a currency symbol? spacy_tkn_attr_is_digit Does the token consist of digits? spacy_tkn_attr_is_left_punct Is the token a left punctuation mark, e.g. \"(\" ? spacy_tkn_attr_is_lower Is the token in lowercase? Equivalent to token.text.islower(). spacy_tkn_attr_is_oov Is the token out-of-vocabulary? spacy_tkn_attr_is_punct Is the token punctuation? spacy_tkn_attr_is_quote Is the token a quotation mark? spacy_tkn_attr_is_right_punct Is the token a right punctuation mark, e.g. \")\" ? spacy_tkn_attr_is_sent_end Does the token end a sentence? spacy_tkn_attr_is_sent_start Does the token start a sentence? spacy_tkn_attr_is_space Does the token consist of whitespace characters? Equivalent to token.text.isspace(). spacy_tkn_attr_is_stop Is the token part of a \u201cstop list\u201d? spacy_tkn_attr_is_title Is the token in titlecase? spacy_tkn_attr_is_upper Is the token in uppercase? Equivalent to token.text.isupper(). spacy_tkn_attr_lang_ Language of the parent document\u2019s vocabulary. spacy_tkn_attr_left_edge The leftmost token of this token\u2019s syntactic descendants. spacy_tkn_attr_lemma_ Base form of the token, with no inflectional suffixes. spacy_tkn_attr_lex The underlying lexeme. spacy_tkn_attr_lex_id Sequential ID of the token\u2019s lexical type, used to index into tables, e.g. for word vectors. spacy_tkn_attr_like_email Does the token resemble an email address? spacy_tkn_attr_like_num Does the token represent a number? spacy_tkn_attr_like_url Does the token resemble a URL? spacy_tkn_attr_lower_ Lowercase form of the token text. Equivalent to Token.text.lower(). spacy_tkn_attr_morph Morphological analysis. spacy_tkn_attr_norm_ The token\u2019s norm, i.e. a normalized form of the token text. spacy_tkn_attr_orth_ Verbatim text content (identical to Token.text). Exists mostly for consistency with the other attributes. spacy_tkn_attr_pos_ Coarse-grained part-of-speech from the Universal POS tag set. spacy_tkn_attr_prefix_ A length-N substring from the start of the token. Defaults to N=1. spacy_tkn_attr_prob Smoothed log probability estimate of token\u2019s word type (context-independent entry in the vocabulary). spacy_tkn_attr_rank Sequential ID of the token\u2019s lexical type, used to index into tables, e.g. for word vectors. spacy_tkn_attr_right_edge The rightmost token of this token\u2019s syntactic descendants. spacy_tkn_attr_sent The sentence span that this token is a part of. spacy_tkn_attr_sentiment A scalar value indicating the positivity or negativity of the token. spacy_tkn_attr_shape_ Transform of the token\u2019s string to show orthographic features. spacy_tkn_attr_suffix_ Length-N substring from the end of the token. Defaults to N=3. spacy_tkn_attr_tag_ Fine-grained part-of-speech. spacy_tkn_attr_tensor The token\u2019s slice of the parent Doc\u2019s tensor. spacy_tkn_attr_text Verbatim text content. spacy_tkn_attr_text_with_ws Text content, with trailing space character if present. spacy_tkn_attr_vocab The vocab object of the parent Doc. spacy_tkn_attr_whitespace_ Trailing space character if present. <p>More information about the spaCy token attributes can be found here. <code>DCR-CORE</code> currently supports only a subset of the possible attributes, but this can easily be extended if required.</p> <p>Detailed information about the universal POS tags can be found here.</p>"},{"location":"application_document_language/","title":"DCR-CORE - Application - Document Language","text":""},{"location":"application_document_language/#1-overview","title":"1. Overview","text":"<p><code>DCR-CORE</code> supports the processing of documents in different languages.  The supported languages must be accepted by Pandoc respectively Babel, spaCy and Tesseract OCR. </p>"},{"location":"application_document_language/#2-default-document-language","title":"2. Default Document Language","text":"<p>The default document language is English . </p> Application Content Pandoc en spaCy en_core_web_trf Tesseract OCR eng"},{"location":"application_installation/","title":"DCR-CORE - Application - Installation","text":""},{"location":"application_installation/#1-use-as-a-library","title":"1. Use as a library","text":"<p><code>DCR-CORE</code> is installable using the package manager PyPI.  Your package manager will find a version that works with your interpreter. </p> <p>Example installation with <code>pip</code>:</p> <pre><code>  pip install dcr-core\n</code></pre>"},{"location":"application_installation/#2-use-of-a-docker-container","title":"2. Use of a Docker container","text":"<p>A fully functional Docker image is available here on DockerHub.  From this, a local Docker container can be created with the following command:</p> <pre><code>docker run -it --name dcr-core -v &lt;local directory&gt;:/dcr-core/data/inbox_prod konnexionsgmbh/dcr-core:0.9.7\n</code></pre> <p><code>&lt;local directory&gt;</code> is the local directory where the files created during the processing are stored. In addition to the software listed under prerequisites, the Docker container also contains a complete virtual environment for running <code>DCR-CORE</code> in suitable versions.</p>"},{"location":"application_operations/","title":"DCR-CORE - Application - Operations","text":"<p>The details of the method call <code>document</code> can be found in the API documentation.</p>"},{"location":"application_operations/#1-use-as-a-library","title":"1. Use as a library","text":"<p>The following sample code extracts the content structure of the pdf file <code>1910.03678.pdf</code> into a JSON file:</p> <pre><code>from dcr_core import cls_process\n\nprocess = cls_process.Process()\nprocess.document(\"data/inbox_prod/1910.03678.pdf\")\n</code></pre>"},{"location":"application_operations/#2-use-of-a-docker-container","title":"2. Use of a Docker container","text":"<p>The following steps extract the content structure of document <code>1910.03678.pdf</code> using Docker Container.</p> <p>1. Restarting the container:</p> <pre><code>docker start dcr-core\n</code></pre> <p>2. Starting Python in the Virtual Environment (inside the <code>dcr-core</code> container):</p> <pre><code>python3 -m pipenv run python3\n</code></pre> <p>3. Make the <code>dcr_core</code> module available:</p> <pre><code>from dcr_core import cls_process\n</code></pre> <p>4. Create an instance of the <code>Process</code> class:</p> <pre><code>process = cls_process.Process()\n</code></pre> <p>5. Process document files:</p> <pre><code>process.document(\"data/inbox_prod/1910.03678.pdf\")\n</code></pre>"},{"location":"application_requirements/","title":"DCR-CORE - Application - Requirements","text":"<p>The required software is listed below.  Regarding the corresponding software versions, you will find the detailed information in the Release Notes. </p>"},{"location":"application_requirements/#1-operating-system","title":"1. Operating System","text":"<p>Continuous delivery / integration (CD/CI) runs on <code>Ubuntu</code> and development is done with <code>Windows 10</code>. For the Windows operating systems, only additional the functionality of the <code>grep</code>, <code>make</code>  and <code>sed</code> tools must be made available, e.g. via Grep for Windows, Make for Windows or sed for Windows.</p>"},{"location":"application_requirements/#2-pandoc-tex-live","title":"2. Pandoc &amp; TeX Live","text":"<p>To convert the non-PDF documents into <code>pdf</code> files for PDFlib TET processing,  the universal document converter Pandoc  and the TeX typesetting system TeX Live are used and must therefore also be installed. The installation of the TeX Live Frontend is not required.</p>"},{"location":"application_requirements/#3-pdflib-tet","title":"3. PDFlib TET","text":"<p>The software library PDFlib TET is used to tokenize the <code>pdf</code> documents.  <code>DCR-CORE</code> contains the free version of PDFlib TET.  This free version is limited to files with a maximum size of 1 MB and a maximum number of pages of 10.  If larger files are to be processed, a licence must be purchased from PDFlib GmbH.  Details on the conditions can be found here.</p>"},{"location":"application_requirements/#4-poppler","title":"4. Poppler","text":"<p>To convert the scanned PDF documents into image files for Tesseract OCR, the rendering library Poppler is used and must therefore also be installed.</p>"},{"location":"application_requirements/#5-python","title":"5. Python","text":"<p>Because of the use of the new typing features, Python is required.</p>"},{"location":"application_requirements/#6-tesseract-ocr","title":"6. Tesseract OCR","text":"<p>To convert image files into <code>pdf</code> files, Tesseract OCR is required.</p>"},{"location":"code_of_conduct/","title":"DCR-CORE - Code of Conduct","text":""},{"location":"code_of_conduct/#1-our-pledge","title":"1. Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#2-our-standards","title":"2. Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or  advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic  address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a  professional setting</li> </ul>"},{"location":"code_of_conduct/#3-our-responsibilities","title":"3. Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#4-scope","title":"4. Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#5-enforcement","title":"5. Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at info@konnexions.ch. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#6-attribution","title":"6. Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available here.</p> <p>For answers to common questions about this code of conduct, see here</p>"},{"location":"contributing/","title":"DCR-CORE - Contributing Guide","text":""},{"location":"contributing/#1-license","title":"1. License","text":"<p>In case of software changes we strongly recommend you to respect the license terms.</p>"},{"location":"contributing/#2-process","title":"2. Process","text":"<ol> <li>fork it</li> <li>create your feature branch (<code>git checkout -b my-new-feature</code>)</li> <li>commit your changes (<code>git commit -am 'Add some feature'</code>)</li> <li>push to the branch (<code>git push origin my-new-feature</code>)</li> <li>create a new pull request</li> <li>Action points to be considered when adding a new database driver and / or a new programming language:<ul> <li>README.md</li> <li>Release-Notes.md</li> </ul> </li> </ol>"},{"location":"contributing/#3-notes-on-the-software-development-process","title":"3. Notes on the Software Development Process","text":"<p>See <code>Developing DCR-CORE</code> here</p>"},{"location":"development_code_formatting/","title":"DCR-CORE - Development - Code Formatting","text":"<p>The tools <code>Black</code>, <code>docformatter</code> and <code>isort</code> are used for formatting the programme code:</p> <ul> <li>Black - The uncompromising <code>Python</code> code formatter.</li> <li>docformatter - Formats docstrings to follow PEP 257.</li> <li>isort - A <code>Python</code> utility / library to sort imports.</li> </ul> <p>All these tools are included in the call <code>make format</code> as well as in the call <code>make dev</code>. They can be executed individually with <code>make black</code>,  <code>make pydocstyle</code> or <code>make isort</code>,  where the recommended order is first <code>make isort</code>, then <code>make black</code> and finally <code>make pydocstyle</code>.</p>"},{"location":"development_coding_standards/","title":"DCR-CORE - Development - Coding Standards","text":"<ul> <li>The PEP 8 style guide for <code>Python</code> code is strictly applied and enforced with static analysis tools.</li> <li>All program code must be commented with type hinting instructions.</li> <li>All public functions, modules and packages must be commented with <code>Docstring</code>.</li> <li>The program code must be covered as far as possible with appropriate tests - the aim is always 100 % test coverage.</li> <li>The successful execution of <code>make final</code> ensures that the program code meets the required standards.</li> </ul>"},{"location":"development_continuous_delivery/","title":"DCR-CORE - Development - Continuous Delivery","text":"<p>The GitHub Actions are used to enforce the following good practices of the software engineering process in the CI/CD process:</p> <ul> <li>uniform formatting of all source code,</li> <li>static source code analysis,</li> <li>execution of the software testing framework, and</li> <li>creation of up-to-date user documentation.</li> </ul> <p>The action <code>standards</code> in the GitHub Actions guarantees compliance with the required standards, the action <code>test_production</code> ensures error-free compilation for production use and the action <code>test_development</code> runs the tests against various operating system and <code>Python</code> versions. The actions <code>test_development</code> and <code>test_production</code> must be able to run error-free on operating system <code>Ubuntu</code> and with <code>Python</code> version <code>3.10</code>, the action <code>standards</code> is only required error-free for the latest versions of <code>Ubuntu</code> and <code>Python</code>.</p> <p>The individual steps to be carried out </p> <ol> <li> <p>in the action <code>standards</code> are:</p> <ol> <li>set up <code>Python</code>, <code>pip</code> and <code>pipenv</code></li> <li>install the development specific packages with <code>pipenv</code></li> <li>compile the <code>Python</code> code</li> <li>format the code with isort, Black and docformatter</li> <li>lint the code with Bandit, Flake8, Mypy and Pylint</li> <li>check the API docs with pydocstyle</li> <li>create and upload the user docs with Pydoc-Markdown and Mkdocs</li> <li>install Pandoc, Poppler, Tesseract OCR and TeX Live</li> <li>publish the code coverage results to <code>coveralls.io</code></li> </ol> </li> <li> <p>in the action <code>test_development</code> are:</p> <ol> <li>set up <code>Python</code>, <code>pip</code> and <code>pipenv</code></li> <li>install the <code>development</code> specific packages with <code>pipenv</code></li> <li>compile the <code>Python</code> code</li> <li>install Pandoc, Poppler, Tesseract OCR and TeX Live</li> <li>run pytest for writing better program</li> </ol> </li> <li> <p>in the action <code>test_production</code> are:</p> <ol> <li>set up <code>Python</code>, <code>pip</code> and <code>pipenv</code></li> <li>install the <code>production</code> specific packages with <code>pipenv</code></li> <li>compile the <code>Python</code> code</li> <li>install Pandoc, Poppler, Tesseract OCR and TeX Live</li> <li>run pytest for writing better program</li> </ol> </li> </ol>"},{"location":"development_environment/","title":"DCR-CORE - Development - Environment","text":"<p><code>DCR-CORE</code> is developed on the operating systems <code>Ubuntu</code> and <code>Microsoft Windows 10</code>. Ubuntu is used here via the <code>VM Workstation Player</code>. <code>Ubuntu</code> can also be used in conjunction with the <code>Windows Subsystem for Linux (WSL2)</code>.</p> <p>The GitHub actions for continuous integration run on <code>Ubuntu</code>.</p> <p>Version <code>3.10</code> is used for the <code>Python</code> programming language.</p> <p>To set up a suitable development environment under <code>Ubuntu</code>, on the one hand a suitable ready-made Docker image is provided and on the other hand two scripts to create the development system in a standalone system, a virtual environment or the <code>Windows Subsystem for Linux (WSL2)</code> are available.</p>"},{"location":"development_environment/#1-docker-image","title":"1. Docker Image","text":"<p>The ready-made Docker images are available on DockerHub under the following link:</p> <p>dcr_dev - Document Content Recognition Development Image</p> <p>When selecting the Docker image, care must be taken to select the appropriate version of the Docker image.</p>"},{"location":"development_environment/#2-script-based-solution","title":"2. Script-based Solution","text":"<p>Alternatively, for a <code>Ubuntu</code> environment that is as unspoiled as possible, the following two scripts are available in the <code>scripts</code> file directory:</p> <ul> <li><code>scripts/0.9.7/run_install_4-vm_wsl2_1.sh</code></li> <li><code>scripts/0.9.7/run_install_4-vm_wsl2_2.sh</code></li> </ul> <p>After a <code>cd scripts</code> command in a terminal window, the script <code>run_install_4-vm_wsl2_1.sh</code> must first be executed.  Administration rights (<code>sudo</code>) are required for this.  Afterwards, the second script <code>run_install_4-vm_wsl2_2.sh</code> must be executed in a new terminal window.</p>"},{"location":"development_line_type/","title":"DCR-CORE - Development - Line Type Algorithms","text":"<p>The granularity of the document <code>line</code> tries to classify the individual lines. The possible line types are :</p> line type Meaning b non-classifiable line, i.e. normal text body line f footer line h header line h_9 level 9 heading line lb line of a bulleted list ln line of a numbered list tab line of a table toc line of a table of content <p>The following three rule-based algorithms are used to determine the line type in the order given:</p> <ol> <li> <p><code>headers &amp; footers</code> The headers and footers are determined by a similarity comparison of the first <code>lt_header_max_lines</code> and last <code>lt_footer_max_lines</code> lines respectively. </p> </li> <li> <p><code>close together</code> A table of contents must be in the first <code>lt_toc_last_page</code> pages and consists of either a list or a table with ascending page numbers. Tables have already been marked accordingly by PDFlib TET. The elements of bulleted or numbered lists must be close together and are determined by regular expressions. </p> </li> <li> <p><code>headings</code> Headings extend across the entire document and can have hierarchical structures.  The headings are determined with rule-enriched regular expressions. </p> </li> </ol>"},{"location":"development_line_type/#1-headers-footers","title":"1 Headers &amp; Footers","text":"<p>The following parameter controls both the classification of the headers and the footers:</p> <ul> <li><code>verbose_lt_header_footer</code></li> </ul> <p>Default value: <code>false</code> - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.</p>"},{"location":"development_line_type/#11-footers","title":"1.1 Footers","text":""},{"location":"development_line_type/#111-parameters","title":"1.1.1 Parameters","text":"<p>The following parameters control the classification of the footers:</p> <ul> <li><code>lt_footer_max_distance</code></li> </ul> <p>Default value: <code>3</code> - The degree of similarity of rows is determined by means of the Levenshtein distance.  The value zero stands for identical lines.  The larger the Levenshtein distance, the more different the rows are.  If the header lines do not contain a page numbers, then the parameter should be set to <code>0</code>.</p> <ul> <li><code>lt_footer_max_lines</code></li> </ul> <p>Default value: <code>3</code> - the number of lines from the bottom of the page to be analyzed as possible candidates for footers. With the value zero the classification of footers is prevented.</p> <ul> <li><code>spacy_ignore_line_type_footer</code></li> </ul> <p>Default value: <code>true</code> -  determines whether the lines of this type are ignored (true) or not (false) during tokenization.</p>"},{"location":"development_line_type/#112-algorithm","title":"1.1.2 Algorithm","text":"<ol> <li>On all pages, the last line <code>n</code>, the line <code>n-1</code>, etc. are compared up to the maximum specified line. </li> <li>The Levenshtein distance is determined for each pair of lines in the specified range for each current page and the previous page.</li> <li>The line is considered a footer if, except for pages <code>1</code> and <code>2</code> and pages <code>n-1</code> and <code>n</code>, the Levenshtein distance is not greater than the specified maximum value.</li> </ol>"},{"location":"development_line_type/#12-headers","title":"1.2 Headers","text":""},{"location":"development_line_type/#121-parameters","title":"1.2.1 Parameters","text":"<p>The following parameters control the classification of the headers:</p> <ul> <li><code>lt_header_max_distance</code></li> </ul> <p>Default value: <code>3</code> - the degree of similarity of rows is determined by means of the Levenshtein distance.  The value zero stands for identical lines.  The larger the Levenshtein distance, the more different the rows are.  If the footer lines contain a page number, then depending on the number of pages in the document, the following values are useful:</p> document pages Levenshtein distance &lt; 10 1 &lt; 100 2 &lt; 1000 3 <ul> <li><code>lt_header_max_lines</code></li> </ul> <p>Default value: <code>3</code> - the number of lines from the top of the page to be analyzed as possible candidates for headers. A value of zero prevents the classification of headers.</p> <ul> <li><code>spacy_ignore_line_type_header</code></li> </ul> <p>Default value: <code>true</code> -  determines whether the lines of this type are ignored (true) or not (false) during tokenization.</p>"},{"location":"development_line_type/#122-algorithm","title":"1.2.2 Algorithm","text":"<ol> <li>On all pages, the first line, the second line, etc. are compared up to the maximum specified line. </li> <li>The Levenshtein distance is determined for each pair of lines in the specified range for each current page and the previous page.</li> <li>The line is considered a header if, except for pages <code>1</code> and <code>2</code> and pages <code>n-1</code> and <code>n</code>, the Levenshtein distance is not greater than the specified maximum value.</li> </ol>"},{"location":"development_line_type/#2-toc-table-of-content","title":"2 TOC (Table of Content)","text":"<p>An attempt is made here to recognise a table of contents contained in the document. There are two main reasons for this:</p> <ol> <li>there is the possibility to ignore the resulting tokens afterwards, and</li> <li>on the other hand, the table of contents could be in the form of a table, which, however, is then not to be processed as a table in the sense of 4.3.  </li> </ol>"},{"location":"development_line_type/#21-parameters","title":"2.1 Parameters","text":"<p>The following parameters control the classification of a table of contents included in the document:</p> <ul> <li><code>lt_toc_last_page</code></li> </ul> <p>Default value: <code>3</code> - sets the number of pages that will be searched for a table of contents from the beginning of the document. A value of zero prevents the search for a table of contents.</p> <ul> <li><code>lt_toc_min_entries</code></li> </ul> <p>Default value: <code>3</code> - defines the minimum number of entries that a table of contents must contain.</p> <ul> <li><code>spacy_ignore_line_type_toc</code></li> </ul> <p>Default value: <code>true</code> -  determines whether the lines of this type are ignored (true) or not (false) during tokenization.</p> <ul> <li><code>verbose_lt_toc</code></li> </ul> <p>Default value: <code>false</code> - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.</p>"},{"location":"development_line_type/#22-algorithm-table-based","title":"2.2 Algorithm Table-based","text":"<p>A table with the following properties is searched for:</p> <ul> <li>except for the first row, the last column of the table must contain an integer greater than zero,</li> <li>the number found there must be ascending,</li> <li>the number must not be greater than the last page number of the document, and</li> <li>if such a table was found, then the algorithm ends here.</li> </ul>"},{"location":"development_line_type/#23-algorithm-line-based","title":"2.3 Algorithm Line-based","text":"<p>A block of lines with the following properties is searched here:</p> <ul> <li>the last token from each line must contain an integer greater than zero,</li> <li>the number found there must be ascending, and</li> <li>the number must not be greater than the last page number of the document.</li> </ul>"},{"location":"development_line_type/#3-tables","title":"3 Tables","text":"<p>PDFlib TET determines the tables contained in the <code>pdf</code> document and marks them accordingly in its <code>xml</code> output file.  <code>DCR-CORE</code> now uses these marks to determine the line type <code>tab</code> and optionally to output the tables in a separate <code>JSON</code> file.</p>"},{"location":"development_line_type/#31-parameters","title":"3.1 Parameters","text":"<p>The following parameters control the classification of the tables:</p> <ul> <li><code>create_extra_file_table</code></li> </ul> <p>Default value: <code>true</code> - if true, a <code>JSON</code> file named <code>&lt;document_name&gt;_table.json</code> is created in the file directory <code>data_accepted</code> with the identified tables.</p> <ul> <li><code>lt_table_file_incl_empty_columns</code></li> </ul> <p>Default value: <code>true</code> - if true, the empty columns are included in the <code>JSON</code> file <code>&lt;document_name&gt;_table.json</code>.</p> <ul> <li><code>spacy_ignore_line_type_table</code></li> </ul> <p>Default value: <code>false</code> -  determines whether the lines of this type are ignored (true) or not (false) during tokenization.</p> <ul> <li><code>verbose_lt_table</code></li> </ul> <p>Default value: <code>false</code> - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.</p>"},{"location":"development_line_type/#4-bulleted-lists","title":"4 Bulleted Lists","text":"<p>An element of a bulleted list extends either over a whole line or over a complete paragraph. All elements of a bulleted list must begin with one or more of the same characters and must not be interrupted by other lines or paragraphs.</p>"},{"location":"development_line_type/#41-parameters","title":"4.1 Parameters","text":"<p>The following parameters control the classification of a bulleted list:</p> <ul> <li><code>create_extra_file_list_bullet</code></li> </ul> <p>Default value: <code>true</code> - if true, a <code>JSON</code> file named <code>&lt;document_name&gt;_list_bullet.json</code> is created in the file directory <code>data_accepted</code> with the identified bulleted lists.</p> <ul> <li><code>lt_list_bullet_min_entries</code></li> </ul> <p>Default value: <code>2</code> - the minimum number of entries in a bulleted list.</p> <ul> <li><code>lt_list_bullet_rule_file</code></li> </ul> <p>Default value: <code>none</code> - name of a file including file directory that contains the rules for determining the bulleted lists. <code>none</code> means that the given default rules are applied.</p> <ul> <li><code>lt_list_bullet_tolerance_llx</code></li> </ul> <p>Default value: <code>5</code> - percentage tolerance for the differences in indentation of an entry in a bulleted list.</p> <ul> <li><code>spacy_ignore_line_type_list_bullet</code></li> </ul> <p>Default value: <code>false</code> -  determines whether the lines of this type are ignored (true) or not (false) during tokenization.</p> <ul> <li><code>verbose_lt_list_bullet</code></li> </ul> <p>Default value: <code>false</code> - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.</p>"},{"location":"development_line_type/#42-classification-identifiers","title":"4.2 Classification Identifiers","text":"<p>The following table shows the standard identifiers in the default processing order:</p> identifier Hexadecimal \"- \" 2D20 \". \" 2E20 \"\\ufffd \" \"o \" 6F20 \"\u00b0 \" C2B020 \"\u2022 \" E280A220 \"\u2023 \" E280A320 <p>However, these default rules can also be overridden via a <code>JSON</code> file (see parameter <code>lt_list_bullet_rule_file</code>).  An example file can be found in the file directory <code>data</code> with the file name <code>line_type_list_bullet_rules.json</code>.</p> <pre><code>{\n  \"lineTypeListBulletRules\": [\n    \"- \",\n    \". \",\n    \"\\ufffd \",\n    \"o \",\n    \"\u00b0 \",\n    \"\u2022 \",\n    \"\u2023 \"\n  ]\n}\n</code></pre>"},{"location":"development_line_type/#5-numbered-lists","title":"5 Numbered Lists","text":"<p>TBD</p>"},{"location":"development_line_type/#6-headings","title":"6 Headings","text":""},{"location":"development_line_type/#61-parameters","title":"6.1 Parameters","text":"<p>The following parameters control the classification of the headings:</p> <ul> <li><code>create_extra_file_heading</code></li> </ul> <p>Default value: <code>true</code> - if true, a <code>JSON</code> file named <code>&lt;document_name&gt;_heading.json</code> is created in the file directory <code>data_accepted</code> with the identified headings.</p> <ul> <li><code>lt_heading_file_incl_no_ctx</code></li> </ul> <p>Default value: <code>1</code> - the <code>n</code> lines following the heading are included as context into the <code>JSON</code> file.</p> <ul> <li><code>lt_heading_file_incl_regexp</code></li> </ul> <p>Default value: <code>false</code> - if true, the regular expression for the heading is included in the <code>JSON</code> file.</p> <ul> <li><code>lt_heading_max_level</code></li> </ul> <p>Default value: <code>3</code> - the maximum number of hierarchical heading levels.</p> <ul> <li><code>lt_heading_min_pages</code></li> </ul> <p>Default value: <code>2</code> - the minimum number of document pages for determining headings.</p> <ul> <li><code>lt_heading_rule_file</code></li> </ul> <p>Default value: <code>none</code> - name of a file including file directory that contains the rules for determining the headings. <code>none</code> means that the given default rules are applied.</p> <ul> <li><code>lt_heading_tolerance_llx</code></li> </ul> <p>Default value: <code>5</code> - percentage tolerance for the differences in indentation of a heading at the same level.</p> <ul> <li><code>spacy_ignore_line_type_heading</code></li> </ul> <p>Default value: <code>false</code> -  determines whether the lines of this type are ignored (true) or not (false) during tokenization.</p> <ul> <li><code>verbose_lt_heading</code></li> </ul> <p>Default value: <code>false</code> - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.</p>"},{"location":"development_line_type/#62-classification-rules","title":"6.2 Classification Rules","text":"<p>A heading classification rule contains the following 5 elements:</p> Nr. element name description 1 <code>name</code> for documentation purposes, a name that characterises the rule 2 <code>isFirstToken</code> if true, the rule is applied to the first token of the line, otherwise to the beginning of the line 3 <code>regexp</code> the regular expression to be applied 4 <code>functionIsAsc</code> a comparison function for the values of the predecessor and the successor 5 <code>startValues</code> a list of allowed start values <p>The following comparison functions (<code>functionIsAsc</code>) can be used:</p> function description <code>ignore</code> no comparison is performed <code>lowercase_letters</code> two lowercase letters are compared,  whereby the ASCII difference must be exactly <code>1</code> <code>romans</code> two Roman numerals are compared, whereby the difference must be exactly <code>1</code> <code>strings</code> two strings are compared on ascending <code>string_floats</code> floating point numbers are compared, whereby the difference must be greater than <code>0</code> and less than <code>1</code> <code>string_integers</code> two integer numbers are compared, whereby the difference must be exactly <code>1</code> <code>uppercase_letters</code> two uppercase letters are compared,  whereby the ASCII difference must be exactly <code>1</code> <p>The following table shows the standard rules in the default processing order:</p> name isFirstToken regexp functionIsAsc startValues (999) True <code>\"\\(\\d+\\)$\"</code> string_integers <code>[\"(1)\"]</code> (A) True <code>\"\\([A-Z]\\)$\"</code> uppercase_letters <code>[\"(A)\"]</code> (ROM) True see a) romans <code>[\"(I)\"]</code> (a) True <code>\"\\([a-z]\\)$\"</code> lowercase_letters <code>[\"(a)\"]</code> (rom) True see b) romans <code>[\"(i)\"]</code> 999) True <code>\"\\d+\\)$\"</code> string_integers <code>[\"1)\"]</code> 999. True <code>\"\\d+\\.$\"</code> string_integers <code>[\"1.\"]</code> 999.999 True <code>\"\\d+\\.\\d\\d\\d$\"</code> string_floats <code>[\"1.000, \"1.001\"]</code> 999.99 True <code>\"\\d+\\.\\d\\d$\"</code> string_floats <code>[\"1.00\", \"1.01\"]</code> 999.9 True <code>\"\\d+\\.\\d$\"</code> string_floats <code>[\"1.0\", 1.1]</code> A) True <code>\"[A-Z]\\)$\"</code> uppercase_letters <code>[\"A)\"]</code> A. True <code>\"[A-Z]\\.$\"</code> uppercase_letters <code>[\"A, \"A.\"]</code> ROM) True see c) romans <code>[\"I)\"]</code> ROM. True see d) romans <code>[\"I.\"]</code> a) True <code>\"[a-z]\\)$\"</code> lowercase_letters <code>[\"a)\"]</code> a. True <code>\"[a-z]\\.$\"</code> lowercase_letters <code>[\"a, \"a.\"]</code> rom) True see e) romans <code>[\"i)\"]</code> rom. True see f) romans <code>[\"i.\"]</code> <p>a) <code>\"\\(M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\)$\"</code></p> <p>b) <code>\"\\(m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\)$\"</code></p> <p>c) <code>\"M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\)$\"</code></p> <p>d) <code>\"M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\.$\"</code></p> <p>e) <code>\"m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\)$\"</code></p> <p>f) <code>\"m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\.$\"</code></p> <p>However, these default rules can also be overridden via a <code>JSON</code> file (see parameter <code>lt_heading_rule_file</code>).  An example file can be found in the file directory <code>data</code> with the file name <code>heading_rules_test.json</code>.</p> <pre><code>{\n  \"lineTypeHeadingRules\": [\n    {\n      \"name\": \"(a)\",\n      \"isFirstToken\": true,\n      \"regexp\": \"\\\\([a-z]\\\\)$\",\n      \"functionIsAsc\": \"lowercase_letters\",\n      \"startValues\": [\n        \"(a)\"\n      ]\n    },\n    {\n      \"name\": \"(A)\",\n      \"isFirstToken\": true,\n      \"regexp\": \"\\\\([A-Z]\\\\)$\",\n      \"functionIsAsc\": \"uppercase_letters\",\n      \"startValues\": [\n        \"(A)\"\n      ]\n    },\n</code></pre>"},{"location":"development_line_type/#63-algorithm","title":"6.3 Algorithm","text":"<ul> <li>the document is worked through page by page and within a page line by line</li> <li>for each current heading level there is an entry in a hierarchy table</li> <li> <p>for each document line, this hierarchy table is searched from bottom to top for a matching entry</p> </li> <li> <p>an entry is considered to be matching if</p> <ul> <li>the regular expression is satisfied, and</li> <li>the indentation is within the specified tolerance (<code>lt_heading_tolerance_llx</code>), and</li> <li>the comparison function is fulfilled</li> </ul> </li> <li> <p>if there is a match, the following processing steps are carried out and then the next document line is processed</p> <ul> <li>an entry for the <code>JSON</code> file is optionally created</li> <li>any existing lower entries in the hierarchy table are deleted</li> </ul> </li> <li> <p>if no match is found, then the given heading rules are searched in the specified order</p> </li> <li> <p>a heading rule is matching if</p> <ul> <li>the regular expression is satisfied, and</li> <li>one of the optional start values matches the document line, and</li> <li>the new heading level is within the specified limit (<code>lt_heading_max_level</code>)</li> </ul> </li> <li> <p>if there is a match, the following processing steps are carried out and then the next document line is processed</p> <ul> <li>the last heading level is increased by 1,</li> <li>a new entry is added to the hierarchy table</li> <li>an entry for the <code>JSON</code> file is optionally created</li> </ul> </li> </ul>"},{"location":"development_research_notes/","title":"DCR-CORE - Development - Research Notes","text":"<p>Ideas from the following research papers have influenced the development of <code>DCR-CORE</code>.</p>"},{"location":"development_research_notes/#hegghammer-t-2021","title":"Hegghammer, T. (2021)","text":"<p>OCR with Tesseract, Amazon Textract, and Google Document AI: a benchmarking experiment. Journal of Computational Social Science, 2021, pp. 2432-2725 [Online] Available at https://doi.org/10.1007/s42001-021-00149-1 (Accessed 04 January 2022).</p> <p>Optical Character Recognition (OCR) can open up understudied historical documents to computational analysis, but the accuracy of OCR software varies. This article reports a benchmarking experiment comparing the performance of Tesseract, Amazon Textract, and Google Document AI on images of English and Arabic text. English-language book scans (n=322) and Arabic-language article scans (n=100) were replicated 43 times with different types of artificial noise for a corpus of 18,568 documents, generating 51,304 process requests. Document AI delivered the best results, and the server-based processors (Textract and Document AI) performed substantially better than Tesseract, especially on noisy documents. Accuracy for English was considerably higher than for Arabic. Specifying the relative performance of three leading OCR products and the differential effects of commonly found noise types can help scholars identify better OCR solutions for their research needs. The test materials have been preserved in the openly available \u201cNoisy OCR Dataset\u201d (NOD) for reuse in future benchmarking studies.</p>"},{"location":"development_research_notes/#minaee-s-et-al-2021","title":"Minaee, S. et al. (2021)","text":"<p>Deep Learning Based Text Classification: A Comprehensive Review. arXiv [Online] Available at https://arxiv.org/abs/2004.03705 (Accessed 04 January 2022).</p> <p>Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference.  In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths.  We also provide a summary of more than 40 popular datasets widely used for text classification.  Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.</p>"},{"location":"development_research_notes/#paa-g-konya-i-2011","title":"Paa\u00df G., Konya I. (2011)","text":"<p>Machine Learning for Document Structure Recognition.  In: Mehler A., K\u00fchnberger KU., Lobin H., L\u00fcngen H., Storrer A., Witt A. (eds) Modeling, Learning, and Processing of Text Technological Data Structures.  Studies in Computational Intelligence, vol 370.  Springer Verlag GmbH, Heidelberg, Germany.  Available at https://www.researchgate.net/publication/265487498_Machine_Learning_for_Document_Structure_Recognition (Accessed 04 January 2022).</p> <p>The backbone of the information age is digital information which may be searched, accessed, and transferred instantaneously.  Therefore, the digitization of paper documents is extremely interesting.  This chapter describes approaches for document structure recognition detecting the hierarchy of physical components in images of documents, such as pages, paragraphs, and figures, and transforms this into a hierarchy of logical components, such as titles, authors, and sections.  This structural information improves readability and is useful for indexing and retrieving information contained in documents.  First we present a rule-based system segmenting the document image and estimating the logical role of these zones.  It is extensively used for processing newspaper collections showing world-class performance.  In the second part we introduce several machine learning approaches exploring large numbers of interrelated features.  They can be adapted to geometrical models of the document structure, which may be set up as a linear sequence or a general graph.  These advanced models require far more computational resources but show a better performance than simpler alternatives and might be used in future.</p>"},{"location":"development_research_notes/#power-r-scott-d-bouayad-agha-n-2003","title":"Power R., Scott D., Bouayad-Agha, N. (2003)","text":"<p>Document Structure. Computational Linguistics, 2003, Volume 29, Issue 2, pp. 211-260 [Online] The MIT Press, Cambridge, USA. Available at https://direct.mit.edu/coli/article/29/2/211/1803/Document-Structure (Accessed 05 January 2022).</p> <p>We argue the case for abstract document structure as a separate descriptive level in the analysis and generation of written texts.  The purpose of this representation is to mediate between the message of a text (i.e., its discourse structure) and its physical presentation (i.e., its organization into graphical constituents like sections, paragraphs, sentences, bulleted lists, figures, and footnotes).  Abstract document structure can be seen as an extension of Nunberg's \u201ctext-grammar\u201d it is also closely related to \u201clogical\u201d markup in languages like HTML and LaTEX.  We show that by using this intermediate representation, several subtasks in language generation and language understanding can be defined more cleanly.</p>"},{"location":"development_research_notes/#rahman-m-finin-t-2019","title":"Rahman, M., Finin, T. (2019)","text":"<p>Unfolding the Structure of a Document using Deep Learning. arXiv [Online] Available at https://arxiv.org/abs/1910.03678 (Accessed 07 January 2022).</p> <p>Understanding and extracting of information from large documents, such as business opportunities, academic articles, medical documents and technical reports, poses challenges not present in short documents.  Such large documents may be multi-themed, complex, noisy and cover diverse topics.  We describe a framework that can analyze large documents and help people and computer systems locate desired information in them.  We aim to automatically identify and classify different sections of documents and understand their purpose within the document.  A key contribution of our research is modeling and extracting the logical and semantic structure of electronic documents using deep learning techniques.  We evaluate the effectiveness and robustness of our framework through extensive experiments on two collections: more than one million scholarly articles from arXiv and a collection of requests for proposal documents from government sources. </p>"},{"location":"development_software_documentation/","title":"DCR-CORE - Development - Software Documentation","text":""},{"location":"development_software_documentation/#1-api-documentation","title":"1. API Documentation","text":"<p>The creation of API documentation for functions, modules and packages is mandatory and enforced with the static analysis tool pydocstyle. <code>pydocstyle</code> is a static analysis tool for checking compliance with <code>Python</code> <code>Docstring</code> conventions. <code>pydocstyle</code> can be executed individually with <code>make pydocstyle</code> and is also included in both calls <code>make docs</code> and  <code>make dev</code>.</p> <p>The <code>Docstring</code> format used in <code>DCR-CORE</code> is that of type Google.  For Visual Studio Code, the extension VSCode Python Docstring Generator can be used when creating API documentation. With the mkdocstrings tool, the API documentation is extracted from the source files and put into Markdown format.  In this format, the API documentation can then be integrated into the user documentation.</p>"},{"location":"development_software_documentation/#2-examples-for-the-format-of-the-api-documentation","title":"2. Examples for the format of the API documentation","text":"<p>Package Documentation:</p> <pre><code>Package libs: DCR-CORE libraries.\n</code></pre> <p>Module Documentation:</p> <pre><code>Module pp.inbox: Check and distribute incoming documents.\n\nNew documents are made available in the file directory inbox.\nThese are then checked and moved to the accepted or\nrejected file directories depending on the result of the check.\nDepending on the file format, the accepted documents are then\nconverted into the pdf file format either with the help of Pandoc\nand TeX Live or with the help of Tesseract OCR.\n</code></pre> <p>Function  Documentation:</p> <pre><code>Load the command line arguments into memory.Pandoc and TeX Live\n\nThe command line arguments define the process steps to be executed.\nThe valid arguments are:\n\n    all   - Run the complete processing of all new documents.\n    db_c  - Create the database.\n    db_u  - Upgrade the database.\n    n_2_p - Convert non-pdf docuents to pdf documents.\n    ocr   - Convert image docuents to pdf documents.\n    p_i   - Process the inbox directory.\n    p_2_i - Convert pdf documents to image files.\n    tet   - Extract text from pdf documents.\n\nWith the option all, the following process steps are executed\nin this order:\n\n    1. p_i\n    2. p_2_i\n    3. n_2_p\n    4. ocr\n    5. tet\n\nArgs:\n    argv (List[str]): Command line arguments.\n\nReturns:\n    dict[str, bool]: The processing steps based on CLI arguments.\n</code></pre> <p>In Visual Studio Code, the VSCode Python Docstring Generator tool can be used to create a framework for API documentation.</p>"},{"location":"development_software_documentation/#3-user-documentation","title":"3. User Documentation","text":"<p>The remaining documents for the user documentation can be found in the file directory <code>docs</code> in Markdown format:</p> File Headline Remarks <code>api-docs/*</code> API Documentation <code>css/*</code> CSS files <code>img/*</code> Image files <code>application_*.md</code> Appplication Notes on the software application <code>code_of_conduct.md</code> Code of Conduct <code>contributing.md</code> Contributing Guide <code>development_*.md</code> Development Notes on the software development process <code>index.md</code> Document Content Recognition Background, installation and user guide <code>license.md</code> Text of the licence agreement <code>README.md</code> Directory content list. <code>release_history.md</code> Release History Previous release notes <code>release_notes.md</code> Release Notes Release notes of the current version <p>The MkDocs tool is used to create the user documentation.  With the command <code>make mkdocs</code> the user documentation is created by MkDocs and uploaded to the GitHub pages of the repository. The command <code>make mkdocs</code> is also included in the calls <code>make docs</code> and <code>make final</code>.</p>"},{"location":"development_software_testing/","title":"DCR-CORE - Development - Software Testing","text":"<p>pytest is used as a software testing framework with the following plugins::</p> <ul> <li>pytest-cov for coverage reporting,</li> <li>pytest-deadfixture to list unused or duplicate fixtures, and</li> <li>pytest-random-order to randomise the order of the tests.</li> </ul> <p>On the one hand, the tests must be as complete as possible, i.e. a test coverage of 100% is aimed for, but on the other hand, the scope of the test code should be minimal, i.e. unnecessary repetitions must be strictly avoided.  The best strategy for this is to first create a test case for the normal case and then add special tests for the special cases not yet covered.</p> <p>Finally, the tool Coveralls for Python is used to enable a connection to Coveralls.</p>"},{"location":"development_static_code_analysis/","title":"DCR-CORE - Development - Static Code Analysis","text":"<p>The tools <code>Bandit</code>, <code>Flake8</code>, <code>Mypy</code> and <code>Pylint</code> are used for static code analysis:</p> <ul> <li>Bandit - <code>Bandit</code> is a tool designed to find common security issues in <code>Python</code> code.</li> <li>Flake8 - A <code>Python</code> tool that glues together <code>pycodestyle</code>, <code>Pyflakes</code>, <code>McCabe</code>, and third-party plugins to check the style and quality of some <code>Python</code> code.</li> <li>Mypy - Optional static typing for <code>Python</code>.</li> <li>Pylint - It's not just a linter that annoys you!</li> </ul> <p>All these tools are included in the call <code>make lint</code> as well as in the call <code>make dev</code>. They can be executed individually with <code>make bandit</code>, <code>make flake8</code>, <code>make mypy</code> and <code>make pylint</code>.</p> <p><code>Flake8</code> includes the following tools:</p> <ul> <li>McCabe - McCabe complexity checker for <code>Python</code>.</li> <li>pycodestyle - Simple <code>Python</code> style checker in one <code>Python</code> file.</li> <li>Pyflakes - A simple program which checks <code>Python</code> source files for errors.</li> <li>Radon - Various code metrics for <code>Python</code> code.</li> </ul>"},{"location":"development_version_planning/","title":"DCR-CORE - Development - Version Planning","text":""},{"location":"development_version_planning/#1-version-planning","title":"1. Version Planning","text":""},{"location":"development_version_planning/#11-open","title":"1.1 Open","text":"Version Feature(s) 0.9.7 TBD"},{"location":"development_version_planning/#12-already-implemented","title":"1.2 Already implemented","text":"Version Feature(s) 0.9.6 Extracting an API"},{"location":"development_version_planning/#2-next-development-steps","title":"2. Next Development Steps","text":""},{"location":"development_version_planning/#21-open","title":"2.1 Open","text":""},{"location":"development_version_planning/#211-high-priority","title":"2.1.1 High Priority","text":"<ul> <li>pandoc: convert <code>doc</code> documents to <code>docx</code></li> <li>user: reconstruct original document</li> </ul>"},{"location":"development_version_planning/#212-normal-priority","title":"2.1.2 Normal Priority","text":"<ul> <li>API documentation: Content improvement</li> <li>API documentation: Layout improvement</li> <li>line type header &amp; footer:</li> <li>optional: ignore the first / last page</li> <li>optional: logging of the applied method</li> <li>optional: page number alternating in the first and last token</li> <li>optional: page number always in the first / last token</li> <li>optional: use of Levenshtein algorithm</li> <li>optional: use of language-related regular expressions to determine the header / footer with the page number</li> <li>line type table:</li> <li>check the coordinates</li> <li>table with page break</li> </ul>"},{"location":"development_version_planning/#213-low-priority","title":"2.1.3 Low Priority","text":"<ul> <li>Google Styleguide implementation</li> </ul>"},{"location":"development_version_planning/#22-already-implemented","title":"2.2 Already implemented","text":"<ul> <li>TODO</li> </ul>"},{"location":"license/","title":"DCR-CORE - License","text":"<pre><code>                       Konnexions Public License (KX-PL)\n                           Version 2020.05, May 2020\n</code></pre> <p>pdf Version</p> <p>This license governs use of the accompanying software. If you use the software, you     accept this license. If you do not accept the license, do not use the software.</p> <ol> <li> <p>Definitions.</p> <p>The terms \"reproduce\", \"reproduction\", \"derivative works\", and \"distribution\"    have the same meaning here as under U.S. copyright law.</p> <p>A \"contribution\" is the original software, or any additions or changes to the    software.</p> <p>A \"contributor\" is any person that distributes its contribution under this    license.</p> <p>\"Licensed patents\" are a contributor's patent claims that read directly on its    contribution.</p> </li> <li> <p>Grant of Rights</p> <p>(a)  Copyright Grant - Subject to the terms of this license, including the license         conditions and limitations in section 3, each contributor grants you a non-        exclusive, worldwide, royalty-free copyright license to reproduce its         contribution, prepare derivative works of its contribution, and distribute         its contribution or any derivative works that you create.</p> <p>(b)  Patent Grant - Subject to the terms of this license, including the license         conditions and limitations in section 3, each contributor grants you a non-        exclusive, worldwide, royalty-free license under its licensed patents to         make, have made, use, sell, offer for sale, import, and/or otherwise dispose        of its contribution in the software or derivative works of the contribution         in the software.</p> </li> <li> <p>Conditions and Limitations</p> <p>(a)  No Trademark License - This license does not grant you rights to use any         contributors' name, logo, or trademarks.</p> <p>(b)  If you bring a patent claim against any contributor over patents that you         claim are infringed by the software, your patent license from such         contributor to the software ends automatically.0</p> <p>(c)  If you distribute any portion of the software, you must retain all copyright,         patent, trademark, and attribution notices that are present in the software.</p> <p>(d)  If you distribute any portion of the software in source code form, you may do        so only under this license by including a complete copy of this license with         your distribution. If you distribute any portion of the software in compiled         or object code form, you may only do so under a license that complies with         this license.</p> <p>(e)  The software is licensed \"as-is.\" You bear the risk of using it. The         contributors give no express warranties, guarantees or conditions. You may         have additional consumer rights under your local laws which this license         cannot change. To the extent permitted under your local laws, the         contributors exclude the implied warranties of merchantability, fitness for a        particular purpose and non-infringement.</p> <p>(f)  Source code usage under this License is limited to review, compilation and         contributions. Contributions to Konnexions software products under this         License may only be made in consultation with Konnexions GmbH and through the        appropriate Konnexions software repositories.</p> </li> </ol>"},{"location":"release_history/","title":"DCR-CORE - Release History","text":""},{"location":"release_history/#version-096","title":"Version 0.9.6","text":"<p>Release Date: 07.08.2022</p>"},{"location":"release_history/#1-new-features","title":"1 New Features","text":"<ul> <li>Initial version.</li> </ul>"},{"location":"release_history/#2-applied-software","title":"2 Applied Software","text":"Software Version Remark Status Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.6 Python3 - pip 22.1.2 Tesseract OCR 5.1.0 base version TeX Live 2022 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version"},{"location":"release_history/#windows-specific-software","title":"Windows-specific Software","text":"Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version"},{"location":"release_notes/","title":"DCR-CORE - Release Notes","text":""},{"location":"release_notes/#1-version-097","title":"1. Version 0.9.7","text":"<p>Release Date: 07.09.2022</p>"},{"location":"release_notes/#11-new-features","title":"1.1 New Features","text":"<ul> <li>Creation and use of reference files for pytest</li> </ul>"},{"location":"release_notes/#12-modified-features","title":"1.2 Modified Features","text":"<ul> <li>Delimitation of the documentation to the <code>DCR</code> application</li> <li>Delimitation of the tests to the <code>DCR</code> application</li> <li>Updating the third party software used</li> </ul>"},{"location":"release_notes/#13-applied-software","title":"1.3 Applied Software","text":"Software Version Remark Status Git 2.34.1 base version upgrade Pandoc 2.19.2 upgrade PFlib TET 5.3 Poppler 22.02.0 upgrade Python3 3.10.7 upgrade Tesseract OCR 5.2.0-22-g0daf1 base version upgrade TeX Live 2022 base version upgrade"},{"location":"release_notes/#windows-specific-software","title":"Windows-specific Software","text":"Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version"},{"location":"api-docs/","title":"Index","text":""},{"location":"api-docs/#api-overview","title":"API Overview","text":""},{"location":"api-docs/#modules","title":"Modules","text":"<ul> <li>No modules</li> </ul>"},{"location":"api-docs/#classes","title":"Classes","text":"<ul> <li>No classes</li> </ul>"},{"location":"api-docs/#functions","title":"Functions","text":"<ul> <li>No functions</li> </ul> <p>This file was automatically generated via lazydocs.</p>"}]}