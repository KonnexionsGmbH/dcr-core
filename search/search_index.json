{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DCR-CORE - Document Content Recognition 1. Introduction Based on the paper \"Unfolding the Structure of a Document using Deep Learning\" ( Rahman and Finin, 2019 ), this software project attempts to use various software techniques to automatically recognise the structure in any pdf documents and thus make them more searchable. The processing logic is as follows: A document not in pdf format is converted to pdf format using Pandoc and TeX Live . A document based on scanning which, therefore, does not contain text elements, is scanned and converted to pdf format using the Tesseract OCR software. This process applies to all image format files e.g. jpeg , tiff etc., as well as scanned images in pdf format. From a pdf document, the text and associated metadata is extracted into document-specific xml files using PDFlib TET . The document-specific xml files are then parsed and the DCR-CORE -relevant contents are written to a JSON file. From the JSON file spaCy extracts qualified tokens and stores them either in a JSON file and / or a xml file. 1.1 Rahman & Finin Paper 1.2 Supported File Types DCR-CORE can handle the following file types based on the file extension: File extension File type Initial processing bmp bitmap image file tesseract csv comma-separated values pandoc docx Office Open XML pandoc epub e-book file format pandoc gif Graphics Interchange Format tesseract html HyperText Markup Language pandoc jp2 JPEG 2000 tesseract jpeg Joint Photographic Experts Group tesseract odt Open Document Format for Office Applications pandoc pdf Portable Document Format pdflib / pdf2image png Portable Network Graphics tesseract pnm portable any-map format tesseract rst reStructuredText (RST pandoc rtf Rich Text Format pandoc tif Tag Image File Format tesseract tiff Tag Image File Format tesseract webp Image file format with lossless and lossy compression tesseract 2. Detailed Processing Actions The documents to be processed are divided into individual steps, so-called actions. Each action has the task of changing the state of a document by transforming an input file format into a different output file format. If an error occurs during the processing of the document, the actual processing is terminated with an exception. 2.1 Preprocessor 2.1.1 Preprocessor Architecture 2.1.2 Process the input document In the first action, the file directory inbox is checked for new document files. An entry is created in the document database table for each new document, showing the current processing status of the document. The association of document and language is managed via subdirectories of the file folder inbox . In the database table language , the column directory_name_inbox specifies per language in which subdirectory the documents in this language are to be supplied. Detailed information on this can be found in the chapter Running DCR-CORE in the section Document Language . The new document files are processed based on their file extension as follows: 2.1.2.1 File extension pdf The module fitz from package PyMuPDF is used to check whether the pdf document is a scanned image or not. A pdf document consisting of a scanned image is marked for conversion from pdf format to an image format and moved to the file directory \u00ecnbox_accepted . Other pdf documents are marked for further processing with the pdf parser and then also moved to the file directory \u00ecnbox_accepted . If, however, when checking the pdf document with fitz , it turns out that the document with the file extension pdf is not really a pdf document, then the document is moved to the file directory inbox_rejected . 2.1.2.2 File extensions of documents for processing with Pandoc and TeX Live Document files with the following file extensions are moved to the file directory \u00ecnbox_accepted and marked for converting to pdf format using Pandoc and TeX Live : csv docx epub html odt rst rtf An exception are files with the file name README.md , which are ignored and not processed. 2.1.2.3 File extensions of documents for processing with Tesseract OCR Document files with the following file extensions are moved to the file directory \u00ecnbox_accepted and marked for converting to pdf format using Tesseract OCR : bmp gif jp2 jpeg png pnm tif tiff webp 2.1.2.4 Other file extensions of documents Document files that do not fall into one of the previous categories are marked as faulty and moved to the file directory \u00ecnbox_rejected . 2.1.3 Convert pdf documents to image files (action: p_2_i ) This processing action only has to be carried out if there are new pdf documents in the document input that only consist of scanned images. pdf documents consisting of scanned images must first be processed with OCR software in order to extract text they contain. Since Tesseract OCR does not support the pdf file format, such a pdf document must first be converted into one or more image files. This is done with the software pdf2image , which in turn is based on the Poppler software. The processing of the original document (parent document) is then completed and the further processing is carried out with the newly created image file(s) (child document(s)). Since an image file created here always contains only one page of a pdf document, a multi-page pdf document is distributed over several image files. After processing with Tesseract OCR , these separated files are then combined into one pdf document. 2.1.4 Convert appropriate image files to pdf files (action: ocr ) This processing action only has to be performed if there are new documents in the document entry that correspond to one of the document types listed in section 2.1.2.3. In this processing action, the documents of this document types are converted to the pdf format using Tesseract OCR . After processing with Tesseract OCR , the files split in the previous processing action are combined into a single pdf document. 2.1.5 Convert appropriate non- pdf documents to pdf files (action: n_2_p ) This processing action only has to be performed if there are new documents in the document entry that correspond to one of the document types listed in section 2.1.2.2. In this processing action, the documents of this document types are converted to pdf format using Pandoc and TeX Live . 2.2 NLP 2.2.1 NLP Architecture 2.2.2 Extract text from pdf documents (action: tet ) In this processing action, the text of the pdf documents from sections 2.1.2.1, 2.1.4 and 2.1.5 are extracted and written to xml files in tetml format for each document. The PDFlib TET library is used for this purpose. Example extract : TODO <Pages> <Page number=\"1\" width=\"594.96\" height=\"840.96\"> <Options>granularity=line</Options> <Content granularity=\"line\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"> <Para> <Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Line llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Text>19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm</Text> </Line> </Box> </Para> Example extract from granularity page : <Pages> <Page number=\"1\" width=\"594.96\" height=\"840.96\"> <Options>granularity=page</Options> <Content granularity=\"page\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"> <Para> <Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Text>19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm</Text> </Box> </Para> Example extract from granularity word : <Pages> <Page number=\"1\" width=\"594.96\" height=\"840.96\"> <Options>granularity=word tetml={elements={line}}</Options> <Content granularity=\"word\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"> <Para> <Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Line llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Word> <Text>19</Text> <Box llx=\"26.45\" lly=\"818.96\" urx=\"34.45\" ury=\"826.96\"/> </Word> <Word> <Text>/</Text> <Box llx=\"34.45\" lly=\"818.96\" urx=\"36.67\" ury=\"826.96\"/> </Word> <Word> <Text>04</Text> <Box llx=\"36.67\" lly=\"818.96\" urx=\"44.67\" ury=\"826.96\"/> </Word> 2.2.3 Store the parser result in a JSON file (action: s_p_j ) From the xml files of the granularity document line ( <file_name>_<doc_id>.line.xml ) or document word ( <file_name>_<doc_id>.word.xml ) created in the previous action, the text contained is now extracted with the existing metadata using xml parsing and stored in a JSON format in the database tables content_tetml_line and content_tetml_word . The document line granularity attempts to type the lines. Details on this process can be found in section 4. Example extract from granularity line : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noLinesFooter\": 1, \"noLinesHeader\": 1, \"noLinesInDocument\": 2220, \"noLinesToc\": 85, \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 829, \"noTablesInDocument\": 5, \"pages\": [ { \"pageNo\": 1, \"noLinesInPage\": 15, \"noParagraphsInPage\": 7, \"lines\": [ { \"coordLLX\": 26.45, \"coordURX\": 485.41, \"lineIndexPage\": 0, \"lineIndexParagraph\": 0, \"lineNo\": 1, \"lineType\": \"h\", \"paragraphNo\": 1, \"text\": \"19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm\" }, Example extract from the optional file line_list_bullet : { Example extract from the optional file line_list_number : { Example extract from the optional file line_table : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noTablesInDocument\": 5, \"tables\": [ { \"firstRowLLX\": 52.0, \"firstRowURX\": 426.45, \"noColumns\": 30, \"noRows\": 10, \"pageNoFrom\": 9, \"pageNoTill\": 9, \"tableNo\": 1, \"rows\": [ { \"firstColumnLLX\": 52.0, \"lastColumnURX\": 426.45, \"noColumns\": 3, \"rowNo\": 1, \"columns\": [ { \"columnNo\": 1, \"coordLLX\": 52.0, \"coordURX\": 63.77, \"lineIndexPage\": 18, \"lineIndexParagraph\": 0, \"lineNo\": 1, \"paragraphNo\": 4, \"text\": \"No.\" }, Example extract from the optional file line_toc : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"toc\": [ { \"headingLevel\": 1, \"headingText\": \"1. Lease Term: After the existing Tenant has vacated Landlord will allow Tenant to access the Demised\", \"pageNo\": 4, \"headingCtxLine1\": \"not delay or interfere with the completion of the Allowance Improvements by the Landlord in any material respect; and (b) prior to\", \"headingCtxLine2\": \"entering the Demised Premises the Tenant shall provide insurance coverage as required by this Lease. Landlord shall offer the\", \"headingCtxLine3\": \"existing tenant an early termination of its lease on December 31, 2011, instead of the normal expiration date of January 31, 2012.\", \"regexp\": \"\\\\d+\\\\.$\" }, Example extract from granularity page : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 829, \"pages\": [ { \"pageNo\": 1, \"noParagraphsInPage\": 7, \"paragraphs\": [ { \"paragraphNo\": 1, \"text\": \"19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm\" }, Example extract from granularity word : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noLinesInDocument\": 2217, \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 828, \"noWordsInDocument\": 38674, \"pages\": [ { \"pageNo\": 1, \"noLinesInPage\": 15, \"noParagraphsInPage\": 7, \"noWordsInPage\": 112, \"paragraphs\": [ { \"paragraphNo\": 1, \"noLinesInParagraph\": 1, \"noWordsInParagraph\": 28, \"lines\": [ { \"lineNo\": 1, \"noWordsInLine\": 28, \"words\": [ { \"wordNo\": 1, \"text\": \"19\" }, 2.2.4 Create qualified document tokens (action: tkn ) For tokenization, spaCy is used. The document text is made available to spaCy page by page. Either the granularity document line or document page can be used for this. With the granularity document line , the recognised headers and footers are left out of the token creation. spaCy provides a number of attributes for the token. Details can be found here in the spaCy documentation. The configuration parameters of the type spacy_tkn_attr_... control which of these attributes are stored to the database table content_token . By default, the following attributes are stored: spacy_tkn_attr_ent_iob_ spacy_tkn_attr_ent_type_ spacy_tkn_attr_i spacy_tkn_attr_is_currency spacy_tkn_attr_is_digit spacy_tkn_attr_is_oov spacy_tkn_attr_is_punct spacy_tkn_attr_is_sent_end spacy_tkn_attr_is_sent_start spacy_tkn_attr_is_stop spacy_tkn_attr_is_title spacy_tkn_attr_lemma_ spacy_tkn_attr_like_email spacy_tkn_attr_like_num spacy_tkn_attr_like_url spacy_tkn_attr_norm_ spacy_tkn_attr_pos_ spacy_tkn_attr_tag_ spacy_tkn_attr_text spacy_tkn_attr_whitespace_ In the event of an error, the original document is marked as erroneous and an explanatory entry is also written in the document table. Example extract from granularity line : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noLinesFooter\": 1, \"noLinesHeader\": 1, \"noLinesInDocument\": 2031, \"noLinesToc\": 85, \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 630, \"noSentencesInDocument\": 949, \"noTablesInDocument\": 5, \"noTokensInDocument\": 16495, \"pages\": [ { \"pageNo\": 1, \"noLinesInPage\": 13, \"noParagraphsInPage\": 5, \"noSentencesInPage\": 5, \"noTokensInPage\": 39, \"paragraphs\": [ { \"paragraphNo\": 2, \"noLinesInParagraph\": 2, \"noSentencesInParagraph\": 1, \"noTokensInParagraph\": 7, \"sentences\": [ { \"sentenceNo\": 1, \"coordLLX\": 34.0, \"coordURX\": 244.18, \"lineType\": \"b\", \"noTokensInSentence\": 7, \"text\": \"EX-10.19 3 t1700141_ex10-19.htm EXHIBIT 10.19 Exhibit 10.19\", \"tokens\": [ { \"tknI\": 0, \"tknIsOov\": true, \"tknLemma_\": \"ex-10.19\", \"tknNorm_\": \"ex-10.19\", \"tknPos_\": \"NUM\", \"tknTag_\": \"CD\", \"tknText\": \"EX-10.19\", \"tknWhitespace_\": \" \" }, 3. Auxiliary File Names The processing actions are based on different flat files, each of which is generated from the original document on an action-related basis. Apart from the JSON files optionally created during the 'tokenizer' action, these can be automatically deleted after error-free processing. 3.1 Naming System Action p_i - process the inbox directory in : <ost>.<oft> out: <ost>_<di>.<oft> Action p_2_i - convert pdf documents to image files in : <ost>_<di>.pdf out: <ost>_<di>.<jpeg|png> Action ocr - convert image files to pdf documents in : <ost>_<di>.<oft> or : <ost>_<di>.<jpeg|png> out: <ost>_<di>_<pn>.pdf <ost>_<di>_0.pdf Action n_2_p - convert non-pdf documents to pdf documents in : <ost>_<di>.<oft> out: <ost>_<di>.pdf Action tet - extract text and metadata from pdf documents in : <ost>_<di>[_<pn>|_0].pdf out: <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml Action s_p_j - store the parser result in a JSON file in : <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml out: <ost>_<di>[_<pn>|_0]_line.json <ost>_<di>[_<pn>|_0]_line.list_bullet.json <ost>_<di>[_<pn>|_0]_line.list_number.json <ost>_<di>[_<pn>|_0]_line.table.json <ost>_<di>[_<pn>|_0]_line.toc.json <ost>_<di>[_<pn>|_0]_page.json <ost>_<di>[_<pn>|_0]_word.json Action tkn - create qualified document tokens in : <ost>_<di>[_<pn>|_0]_line.json out: <ost>_<di>[_<pn>|_0]_line_token.json Abbr. Meaning oft original file type osn original stem name di document identifier pn page number 3.2 Examples 3.2.1 Possible intermediate files from a docx document: case_2_docx_route_inbox_pandoc_pdflib_2.docx case_2_docx_route_inbox_pandoc_pdflib_2.pdf case_2_docx_route_inbox_pandoc_pdflib_2.line.xml case_2_docx_route_inbox_pandoc_pdflib_2.page.xml case_2_docx_route_inbox_pandoc_pdflib_2.word.xml case_2_docx_route_inbox_pandoc_pdflib_2.line.json case_2_docx_route_inbox_pandoc_pdflib_2.line_list_bullet.json case_2_docx_route_inbox_pandoc_pdflib_2.line_list_number.json case_2_docx_route_inbox_pandoc_pdflib_2.line_table.json case_2_docx_route_inbox_pandoc_pdflib_2.line_toc.json case_2_docx_route_inbox_pandoc_pdflib_2.page.json case_2_docx_route_inbox_pandoc_pdflib_2.word.json case_2_docx_route_inbox_pandoc_pdflib_2.line.token.json 3.2.2 Possible intermediate files from a jpg document: case_6_jpg_route_inbox_tesseract_pdflib_6.jpg case_6_jpg_route_inbox_tesseract_pdflib_6.pdf case_6_jpg_route_inbox_tesseract_pdflib_6.line.xml case_6_jpg_route_inbox_tesseract_pdflib_6.page.xml case_6_jpg_route_inbox_tesseract_pdflib_6.word.xml case_6_jpg_route_inbox_tesseract_pdflib_6.line.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_list_bullet.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_list_number.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_table.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_toc.json case_6_jpg_route_inbox_tesseract_pdflib_6.page.json case_6_jpg_route_inbox_tesseract_pdflib_6.word.json case_6_jpg_route_inbox_tesseract_pdflib_6.line.token.json 3.2.3 Possible intermediate files from a proper pdf document: case_3_pdf_text_route_inbox_pdflib_3.pdf case_3_pdf_text_route_inbox_pdflib_3.line.xml case_3_pdf_text_route_inbox_pdflib_3.page.xml case_3_pdf_text_route_inbox_pdflib_3.word.xml case_3_pdf_text_route_inbox_pdflib_3.line.json case_3_pdf_text_route_inbox_pdflib_3.line_list_bullet.json case_3_pdf_text_route_inbox_pdflib_3.line_list_number.json case_3_pdf_text_route_inbox_pdflib_3.line_table.json case_3_pdf_text_route_inbox_pdflib_3.line_toc.json case_3_pdf_text_route_inbox_pdflib_3.page.json case_3_pdf_text_route_inbox_pdflib_3.word.json case_3_pdf_text_route_inbox_pdflib_3.line.token.json 3.2.4 Possible intermediate files from a single page scanned image pdf document: case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4.pdf case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.jpeg case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.pdf case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.xml case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.page.xml case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.word.xml case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_list_bullet.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_list_number.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_table.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_toc.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.page.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.word.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.token.json 3.2.5 Possible intermediate files from a multi page scanned image pdf document: case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_1.jpeg case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_2.jpeg case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_1.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_2.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.xml case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.page.xml case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.word.xml case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_list_bullet.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_list_number.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_table.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_toc.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.page.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.word.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.token.json","title":"Home"},{"location":"#dcr-core-document-content-recognition","text":"","title":"DCR-CORE - Document Content Recognition"},{"location":"#1-introduction","text":"Based on the paper \"Unfolding the Structure of a Document using Deep Learning\" ( Rahman and Finin, 2019 ), this software project attempts to use various software techniques to automatically recognise the structure in any pdf documents and thus make them more searchable. The processing logic is as follows: A document not in pdf format is converted to pdf format using Pandoc and TeX Live . A document based on scanning which, therefore, does not contain text elements, is scanned and converted to pdf format using the Tesseract OCR software. This process applies to all image format files e.g. jpeg , tiff etc., as well as scanned images in pdf format. From a pdf document, the text and associated metadata is extracted into document-specific xml files using PDFlib TET . The document-specific xml files are then parsed and the DCR-CORE -relevant contents are written to a JSON file. From the JSON file spaCy extracts qualified tokens and stores them either in a JSON file and / or a xml file.","title":"1. Introduction"},{"location":"#11-rahman-finin-paper","text":"","title":"1.1 Rahman &amp; Finin Paper"},{"location":"#12-supported-file-types","text":"DCR-CORE can handle the following file types based on the file extension: File extension File type Initial processing bmp bitmap image file tesseract csv comma-separated values pandoc docx Office Open XML pandoc epub e-book file format pandoc gif Graphics Interchange Format tesseract html HyperText Markup Language pandoc jp2 JPEG 2000 tesseract jpeg Joint Photographic Experts Group tesseract odt Open Document Format for Office Applications pandoc pdf Portable Document Format pdflib / pdf2image png Portable Network Graphics tesseract pnm portable any-map format tesseract rst reStructuredText (RST pandoc rtf Rich Text Format pandoc tif Tag Image File Format tesseract tiff Tag Image File Format tesseract webp Image file format with lossless and lossy compression tesseract","title":"1.2 Supported File Types"},{"location":"#2-detailed-processing-actions","text":"The documents to be processed are divided into individual steps, so-called actions. Each action has the task of changing the state of a document by transforming an input file format into a different output file format. If an error occurs during the processing of the document, the actual processing is terminated with an exception.","title":"2. Detailed Processing Actions"},{"location":"#21-preprocessor","text":"","title":"2.1 Preprocessor"},{"location":"#22-nlp","text":"","title":"2.2 NLP"},{"location":"#3-auxiliary-file-names","text":"The processing actions are based on different flat files, each of which is generated from the original document on an action-related basis. Apart from the JSON files optionally created during the 'tokenizer' action, these can be automatically deleted after error-free processing.","title":"3. Auxiliary File Names"},{"location":"#31-naming-system","text":"Action p_i - process the inbox directory in : <ost>.<oft> out: <ost>_<di>.<oft> Action p_2_i - convert pdf documents to image files in : <ost>_<di>.pdf out: <ost>_<di>.<jpeg|png> Action ocr - convert image files to pdf documents in : <ost>_<di>.<oft> or : <ost>_<di>.<jpeg|png> out: <ost>_<di>_<pn>.pdf <ost>_<di>_0.pdf Action n_2_p - convert non-pdf documents to pdf documents in : <ost>_<di>.<oft> out: <ost>_<di>.pdf Action tet - extract text and metadata from pdf documents in : <ost>_<di>[_<pn>|_0].pdf out: <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml Action s_p_j - store the parser result in a JSON file in : <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml out: <ost>_<di>[_<pn>|_0]_line.json <ost>_<di>[_<pn>|_0]_line.list_bullet.json <ost>_<di>[_<pn>|_0]_line.list_number.json <ost>_<di>[_<pn>|_0]_line.table.json <ost>_<di>[_<pn>|_0]_line.toc.json <ost>_<di>[_<pn>|_0]_page.json <ost>_<di>[_<pn>|_0]_word.json Action tkn - create qualified document tokens in : <ost>_<di>[_<pn>|_0]_line.json out: <ost>_<di>[_<pn>|_0]_line_token.json Abbr. Meaning oft original file type osn original stem name di document identifier pn page number","title":"3.1 Naming System"},{"location":"#32-examples","text":"","title":"3.2 Examples"},{"location":"application_api_documentation/","text":"DCR-CORE - Application - API Documentation Documentation for cls_process Process utility class. Source code in src/dcr_core/cls_process.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 class Process : \"\"\"Process utility class.\"\"\" # ------------------------------------------------------------------ # Class variables. # ------------------------------------------------------------------ PANDOC_PDF_ENGINE_LULATEX : ClassVar [ str ] = \"lulatex\" PANDOC_PDF_ENGINE_XELATEX : ClassVar [ str ] = \"xelatex\" # ------------------------------------------------------------------ # Initialise the instance. # ------------------------------------------------------------------ def __init__ ( self ) -> None : \"\"\"Initialise the instance.\"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) self . _document_id : int = 0 self . _full_name_in = \"\" self . _full_name_in_directory = \"\" self . _full_name_in_extension = \"\" self . _full_name_in_extension_int = \"\" self . _full_name_in_next_step = \"\" self . _full_name_in_pandoc = \"\" self . _full_name_in_parser_line = \"\" self . _full_name_in_parser_word = \"\" self . _full_name_in_pdf2image = \"\" self . _full_name_in_pdflib = \"\" self . _full_name_in_stem_name = \"\" self . _full_name_in_tesseract = \"\" self . _full_name_in_tokenizer_line = \"\" self . _full_name_in_tokenizer_word = \"\" self . _full_name_orig = \"\" self . _is_delete_auxiliary_files = False self . _is_lt_heading_required = False self . _is_lt_list_bullet_required = False self . _is_lt_list_number_required = False self . _is_lt_toc_required = False self . _is_pandoc = False self . _is_pdf2image = False self . _is_tesseract = False self . _is_verbose = False self . _language_pandoc : str = \"\" self . _language_spacy : str = \"\" self . _language_tesseract : str = \"\" self . _no_lines_footer : int = 0 self . _no_lines_header : int = 0 self . _no_lines_toc : int = 0 self . _no_pdf_pages : int = 0 self . _exist = True core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Check the document by the file extension and determine further # processing. # ------------------------------------------------------------------ def _document_check_extension ( self ): \"\"\"Document processing control. Check the document by the file extension and determine further processing. Raises: RuntimeError: ERROR_01_903 \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) if self . _full_name_in_extension_int == core_glob . FILE_TYPE_PDF : try : if bool ( \"\" . join ([ page . get_text () for page in fitz . open ( self . _full_name_in )])): self . _full_name_in_pdflib = self . _full_name_in else : self . _is_pdf2image = True self . _is_tesseract = True self . _full_name_in_pdf2image = self . _full_name_in except RuntimeError as exc : raise RuntimeError ( core_utils . ERROR_01_903 . replace ( \" {file_name} \" , self . _full_name_in ) . replace ( \" {error_msg} \" , str ( exc )), ) from exc elif self . _full_name_in_extension_int in core_glob . FILE_TYPE_PANDOC : self . _is_pandoc = True self . _full_name_in_pandoc = self . _full_name_in elif self . _full_name_in_extension_int in core_glob . FILE_TYPE_TESSERACT : self . _is_tesseract = True self . _full_name_in_tesseract = self . _full_name_in else : raise RuntimeError ( core_utils . ERROR_01_901 . replace ( \" {extension} \" , self . _full_name_in_extension_int )) core_glob . logger . debug ( core_glob . LOGGER_END ) # ----------------------------------------------------------------------------- # Delete the given auxiliary file. # ----------------------------------------------------------------------------- def _document_delete_auxiliary_file ( self , full_name : str ) -> None : \"\"\"Delete the given auxiliary file. Args: full_name (str): File name. \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name= %s \" , full_name ) if not self . _is_delete_auxiliary_files : return # Don't remove the base document !!! if full_name == self . _full_name_in : return if os . path . isfile ( full_name ): os . remove ( full_name ) core_utils . progress_msg ( self . _is_verbose , f \"Auxiliary file ' { full_name } ' deleted\" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Initialize the document recognition process. # ------------------------------------------------------------------ def _document_init ( self ) -> None : \"\"\"Initialize the document recognition process.\"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) self . _document_id : int = 0 self . _full_name_in : str = \"\" self . _full_name_in_directory : str = \"\" self . _full_name_in_extension : str = \"\" self . _full_name_in_extension_int : str = \"\" self . _full_name_in_next_step : str = \"\" self . _full_name_in_pandoc : str = \"\" self . _full_name_in_parser_line : str = \"\" self . _full_name_in_parser_word : str = \"\" self . _full_name_in_pdf2image : str = \"\" self . _full_name_in_pdflib : str = \"\" self . _full_name_in_stem_name : str = \"\" self . _full_name_in_tesseract : str = \"\" self . _full_name_in_tokenizer : str = \"\" self . _full_name_orig : str = \"\" self . _is_pandoc : bool = False self . _is_pdf2image : bool = False self . _is_tesseract : bool = False self . _language_pandoc : str = \"\" self . _language_spacy : str = \"\" self . _language_tesseract : str = \"\" self . _no_lines_footer : int = 0 self . _no_lines_header : int = 0 self . _no_lines_toc : int = 0 self . _no_pdf_pages : int = 0 core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert the document to PDF format using Pandoc. # ------------------------------------------------------------------ def _document_pandoc ( self ): \"\"\"Convert the document to PDF format using Pandoc. Raises: RuntimeError: Any Pandoc issue. \"\"\" if self . _is_pandoc : core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing Pandoc { self . _full_name_in_pandoc } \" ) self . _full_name_in_pdflib = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pandoc\" , core_glob . FILE_TYPE_PDF ) return_code , error_msg = Process . pandoc ( self . _full_name_in_pandoc , self . _full_name_in_pdflib , self . _language_pandoc , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_pandoc ) core_utils . progress_msg ( self . _is_verbose , f \"End processing Pandoc { self . _full_name_in_pdflib } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Extract the text for all granularities from the PDF document. # ------------------------------------------------------------------ def _document_parser ( self ): \"\"\"Extract the text for all granularities from the PDF document.\"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , \"Start processing Parser\" ) self . _full_name_in_tokenizer = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".parser.\" + core_glob . FILE_TYPE_JSON , ) return_code , error_msg = Process . parser ( document_id = self . _document_id , full_name_in = self . _full_name_in_parser_word , full_name_orig = self . _full_name_orig , full_name_out = self . _full_name_in_tokenizer , is_lt_heading_required = self . _is_lt_heading_required , is_lt_list_bullet_required = self . _is_lt_list_bullet_required , is_lt_list_number_required = self . _is_lt_list_number_required , is_lt_toc_required = self . _is_lt_toc_required , no_pdf_pages = self . _no_pdf_pages , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_parser_line ) self . _document_delete_auxiliary_file ( self . _full_name_in_parser_word ) self . _no_lines_footer = core_glob . inst_lt_hf . no_lines_footer self . _no_lines_header = core_glob . inst_lt_hf . no_lines_header if self . _is_lt_toc_required : self . _no_lines_toc = core_glob . inst_lt_toc . no_lines_toc core_utils . progress_msg ( self . _is_verbose , f \"End processing Parser { self . _full_name_in_tokenizer } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert the PDF document to an image file using pdf2image. # ------------------------------------------------------------------ def _document_pdf2image ( self ): \"\"\"Convert the PDF document to an image file using pdf2image. Raises: RuntimeError: Any pdf2image issue. \"\"\" if self . _is_pdf2image : core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing pdf2image { self . _full_name_in_pdf2image } \" ) self . _full_name_in_tesseract = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pdf2image\" + \"_[0-9]*.\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ), ) return_code , error_msg , _ = Process . pdf2image ( self . _full_name_in_pdf2image , self . _full_name_in_directory , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_pdf2image ) core_utils . progress_msg ( self . _is_verbose , f \"End processing pdf2image { self . _full_name_in_tesseract } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Extract the text and metadata from a PDF document to an XML file. # ------------------------------------------------------------------ def _document_pdflib ( self ): \"\"\"Extract the text and metadata from a PDF document to an XML file. Raises: RuntimeError: Any PDFlib TET issue. \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing PDFlib TET { self . _full_name_in_pdflib } \" ) # noinspection PyUnresolvedReferences self . _no_pdf_pages = len ( PyPDF2 . PdfReader ( self . _full_name_in_pdflib ) . pages ) if self . _no_pdf_pages == 0 : raise RuntimeError ( f \"The number of pages of the PDF document { self . _full_name_in_pdflib } cannot be determined\" ) # ------------------------------------------------------------------ # Granularity 'line'. # ------------------------------------------------------------------ self . _full_name_in_parser_line = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pdflib.\" + nlp_core . NLPCore . LINE_XML_VARIATION + core_glob . FILE_TYPE_XML , ) return_code , error_msg = Process . pdflib ( full_name_in = self . _full_name_in_pdflib , full_name_out = self . _full_name_in_parser_line , document_opt_list = nlp_core . NLPCore . LINE_TET_DOCUMENT_OPT_LIST , page_opt_list = nlp_core . NLPCore . LINE_TET_PAGE_OPT_LIST , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) core_utils . progress_msg ( self . _is_verbose , f \"TETML line granularity created { self . _full_name_in_parser_line } \" ) # ------------------------------------------------------------------ # Granularity 'word'. # ------------------------------------------------------------------ self . _full_name_in_parser_word = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pdflib.\" + nlp_core . NLPCore . WORD_XML_VARIATION + core_glob . FILE_TYPE_XML , ) return_code , error_msg = Process . pdflib ( full_name_in = self . _full_name_in_pdflib , full_name_out = self . _full_name_in_parser_word , document_opt_list = nlp_core . NLPCore . WORD_TET_DOCUMENT_OPT_LIST , page_opt_list = nlp_core . NLPCore . WORD_TET_PAGE_OPT_LIST , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) core_utils . progress_msg ( self . _is_verbose , f \"TETML word granularity created { self . _full_name_in_parser_word } \" ) core_utils . progress_msg ( self . _is_verbose , \"End processing PDFlib TET\" ) self . _document_delete_auxiliary_file ( self . _full_name_in_pdflib ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert one or more image files to a PDF file using Tesseract OCR. # ------------------------------------------------------------------ def _document_tesseract ( self ): \"\"\"Process the document with Tesseract OCR. Convert one or more image files to a PDF file using Tesseract OCR. Raises: RuntimeError: Any Tesseract OCR issue. \"\"\" if self . _is_tesseract : core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing Tesseract OCR { self . _full_name_in_tesseract } \" ) if self . _is_pdf2image : self . _full_name_in_stem_name += \"_0\" self . _full_name_in_pdflib = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".tesseract\" , core_glob . FILE_TYPE_PDF ) return_code , error_msg , children = Process . tesseract ( self . _full_name_in_tesseract , self . _full_name_in_pdflib , self . _language_tesseract , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) # noinspection PyUnresolvedReferences self . _no_pdf_pages = len ( PyPDF2 . PdfReader ( self . _full_name_in_pdflib ) . pages ) if self . _no_pdf_pages == 0 : raise RuntimeError ( f \"The number of pages of the PDF document { self . _full_name_in_pdflib } cannot be determined\" ) for child in children : self . _document_delete_auxiliary_file ( child ) core_utils . progress_msg ( self . _is_verbose , f \"End processing Tesseract OCR { self . _full_name_in_pdflib } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert the PDF document to an image file using pdf2image. # ------------------------------------------------------------------ def _document_tokenizer ( self ) -> None : \"\"\"Tokenize the document with spaCy. Raises: RuntimeError: Any spaCy issue. \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing spaCy { self . _full_name_in_tokenizer } \" ) try : core_glob . inst_tokenizer . exists () except AttributeError : core_glob . inst_tokenizer = tokenizer . TokenizerSpacy () self . _full_name_in_next_step = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".tokenizer.\" + core_glob . FILE_TYPE_JSON , ) return_code , error_msg = Process . tokenizer ( full_name_in = self . _full_name_in_tokenizer , full_name_out = self . _full_name_in_next_step , pipeline_name = self . _language_spacy , document_id = self . _document_id , full_name_orig = self . _full_name_orig , no_lines_footer = self . _no_lines_footer , no_lines_header = self . _no_lines_header , no_lines_toc = self . _no_lines_toc , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_tokenizer ) core_utils . progress_msg ( self . _is_verbose , f \"End processing spaCy { self . _full_name_in_next_step } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Document content recognition for a specific file. # ------------------------------------------------------------------ def document ( # pylint: disable=too-many-arguments self , full_name_in : str , document_id : int = None , full_name_orig : str = None , is_delete_auxiliary_files : bool = None , is_lt_heading_required : bool = None , is_lt_list_bullet_required : bool = None , is_lt_list_number_required : bool = None , is_lt_toc_required : bool = None , is_verbose : bool = None , language_pandoc : str = None , language_spacy : str = None , language_tesseract : str = None , output_directory : str = None , ) -> None : \"\"\"Document content recognition for a specific file. This method extracts the document content structure from a given document and stores it in JSON format. For this purpose, all non-pdf documents and all scanned pdf documents are first converted into a searchable pdf format. Depending on the file format, the tools Pandoc, pdf2image or Tesseract OCR are used for this purpose. PDFlib TET then extracts the text and metadata from the searchable pdf file and makes them available in XML format. spaCY generates qualified tokens from the document text, and these token data are then made available together with the metadata in a JSON format. Args: full_name_in (str): Full file name of the document file. document_id (int, optional): Document identification. Defaults to -1 i.e. no document identification. full_name_orig (str, optional): Original full file name. Defaults to the full file name of the document file. is_delete_auxiliary_files (bool, optional): Delete the auxiliary files after a successful processing step. Defaults to parameter `delete_auxiliary_files` in `setup.cfg`. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to parameter `lt_heading_required` in `setup.cfg`. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to parameter `lt_list_bullet_required` in `setup.cfg`. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to parameter `lt_list_number_required` in `setup.cfg`. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to parameter `lt_toc_required` in `setup.cfg`. is_verbose (bool, optional): Display progress messages for processing. Defaults to parameter `verbose` in `setup.cfg`. language_pandoc (str, optional): Pandoc language code. Defaults to English. language_spacy (str, optional): spaCy language code. Defaults to English transformer pipeline (roberta-base).. language_tesseract (str, optional): Tesseract OCR language code. Defaults to English. output_directory (str, optional): Directory for the flat files to be created. Defaults to the directory of the document file. Raises: RuntimeError: Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR. \"\"\" # Initialise the logging functionality. core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param is_delete_auxiliary_files = %s \" , is_delete_auxiliary_files ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param is_verbose = %s \" , is_verbose ) core_glob . logger . debug ( \"param language_pandoc = %s \" , language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , language_tesseract ) core_glob . logger . debug ( \"param output_directory = %s \" , output_directory ) self . _document_init () self . _document_id = document_id if document_id else - 1 self . _full_name_in = full_name_in self . _full_name_orig = full_name_orig if full_name_orig else full_name_in # Load the configuration parameters. core_glob . inst_setup = setup . Setup () self . _is_delete_auxiliary_files = ( is_delete_auxiliary_files if is_delete_auxiliary_files is not None else core_glob . inst_setup . is_delete_auxiliary_files ) self . _is_lt_heading_required = ( is_lt_heading_required if is_lt_heading_required is not None else core_glob . inst_setup . is_lt_heading_required ) self . _is_lt_list_bullet_required = ( is_lt_list_bullet_required if is_lt_list_bullet_required is not None else core_glob . inst_setup . is_lt_list_bullet_required ) self . _is_lt_list_number_required = ( is_lt_list_number_required if is_lt_list_number_required is not None else core_glob . inst_setup . is_lt_list_number_required ) self . _is_lt_toc_required = is_lt_toc_required if is_lt_toc_required is not None else core_glob . inst_setup . is_lt_toc_required self . _is_verbose = is_verbose if is_verbose is not None else core_glob . inst_setup . is_verbose self . _language_pandoc = language_pandoc if language_pandoc else nlp_core . NLPCore . LANGUAGE_PANDOC_DEFAULT self . _language_spacy = language_spacy if language_spacy else nlp_core . NLPCore . LANGUAGE_SPACY_DEFAULT self . _language_tesseract = language_tesseract if language_tesseract else nlp_core . NLPCore . LANGUAGE_TESSERACT_DEFAULT core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , self . _is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , self . _is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , self . _is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , self . _is_lt_toc_required ) core_glob . logger . debug ( \"param full_name_orig = %s \" , self . _full_name_orig ) core_glob . logger . debug ( \"param language_pandoc = %s \" , self . _language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , self . _language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , self . _language_tesseract ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Pandoc { self . _language_pandoc } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key spaCy { self . _language_spacy } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Tesseract OCR { self . _language_tesseract } \" ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) ( full_name_in_directory , self . _full_name_in_stem_name , self . _full_name_in_extension , ) = core_utils . get_components_from_full_name ( self . _full_name_in ) self . _full_name_in_directory = output_directory if output_directory is not None else full_name_in_directory self . _full_name_in_extension_int = ( self . _full_name_in_extension . lower () if self . _full_name_in_extension else self . _full_name_in_extension ) self . _document_check_extension () self . _document_pandoc () self . _document_pdf2image () self . _document_tesseract () self . _document_pdflib () self . _document_parser () self . _document_tokenizer () core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"End processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , \"=\" * 80 ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Converting a Non-PDF file to a PDF file. # ------------------------------------------------------------------ @classmethod def pandoc ( cls , full_name_in : str , full_name_out : str , language_pandoc : str , ) -> tuple [ str , str ]: \"\"\"Convert a Non-PDF file to a PDF file. The following file formats are converted into PDF format here with the help of Pandoc: - csv - comma-separated values - docx - Office Open XML - epub - e-book file format - html - HyperText Markup Language - odt - Open Document Format for Office Applications - rst - reStructuredText (RST - rtf - Rich Text Format Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_pandoc (str): The Pandoc name of the document language. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_pandoc= %s \" , language_pandoc ) # Convert the document extra_args = [ f \"--pdf-engine= { Process . PANDOC_PDF_ENGINE_XELATEX } \" , \"-V\" , f \"lang: { language_pandoc } \" , ] try : pypandoc . convert_file ( full_name_in , core_glob . FILE_TYPE_PDF , extra_args = extra_args , outputfile = full_name_out , ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_31_911 . replace ( \" {full_name} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except FileNotFoundError : error_msg = core_utils . ERROR_31_902 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except RuntimeError as err : error_msg = core_utils . ERROR_31_903 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK # ------------------------------------------------------------------ # Extracting the text from the PDF document. # ------------------------------------------------------------------ @classmethod def parser ( # noqa: C901 cls , full_name_in : str , full_name_out : str , no_pdf_pages : int , document_id : int = - 1 , full_name_orig : str = None , is_lt_heading_required : bool = False , is_lt_list_bullet_required : bool = False , is_lt_list_number_required : bool = False , is_lt_toc_required : bool = False , ) -> tuple [ str , str ]: \"\"\"Extract the text from the PDF document. From the line-oriented XML output file of PDFlib TET, the text and relevant metadata are extracted with the help of an XML parser and stored in a JSON file. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. no_pdf_pages (int): Total number of PDF pages. document_id (int, optional): The identification number of the document. Defaults to None. full_name_orig (str, optional): The file name of the originating document. Defaults to None. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to False. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to False. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to False. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to False. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Raises: RuntimeError: If the parser has detected any errors. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param no_pdf_pages = %i \" , no_pdf_pages ) try : # ------------------------------------------------------------------ # Granularity 'word'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in ) # Get the root Element root = tree . getroot () core_glob . inst_parser = parser . TextParser () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_word ( directory_name = os . path . dirname ( full_name_in ), document_id = document_id , environment_variant = core_glob . inst_setup . environment_variant , file_name_curr = os . path . basename ( full_name_in ), file_name_next = full_name_out , file_name_orig = full_name_orig , is_lt_heading_required = is_lt_heading_required , is_lt_list_bullet_required = is_lt_list_bullet_required , is_lt_list_number_required = is_lt_list_number_required , is_lt_toc_required = is_lt_toc_required , no_pdf_pages = no_pdf_pages , parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML word granularity parsed { full_name_in } \" ) # ------------------------------------------------------------------ # Granularity 'line'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in . replace ( \"word.xml\" , \"line.xml\" )) # Get the root Element root = tree . getroot () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_line ( parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML line granularity parsed { full_name_in } \" ) except FileNotFoundError : error_msg = core_utils . ERROR_61_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK # ------------------------------------------------------------------ # Converting a scanned PDF file to a set of image files. # ------------------------------------------------------------------ @classmethod def pdf2image ( cls , full_name_in : str , output_directory : str , ) -> tuple [ str , str , list [ tuple [ str , str ]]]: \"\"\"Convert a scanned PDF file to a set of image files. To extract the text from a scanned PDF document, it must first be converted into one or more image files, depending on the number of pages. Then these image files are converted into a normal PDF document with the help of an OCR programme. The input file for this method must be a scanned PDF document, which is then converted into image files with the help of PDF2Image. Args: full_name_in (str): The directory name and file name of the input file. output_directory (str): Directory for the flat files to be created. Returns: tuple[str, str, list[tuple[str,str]]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in= %s \" , full_name_in ) try : images = pdf2image . convert_from_path ( full_name_in ) children : list [ tuple [ str , str ]] = [] no_children = 0 directory_name = os . path . dirname ( full_name_in ) stem_name = os . path . splitext ( os . path . basename ( full_name_in ))[ 0 ] try : os . remove ( core_utils . get_full_name_from_components ( directory_name , stem_name + \"_*.\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ), ) ) except OSError : pass # Store the image pages for img in images : no_children += 1 file_name_next = ( stem_name + \".pdf2image_\" + str ( no_children ) + \".\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ) ) full_name_next = core_utils . get_full_name_from_components ( output_directory , file_name_next , ) img . save ( full_name_next , core_glob . inst_setup . pdf2image_type , ) children . append (( file_name_next , full_name_next )) except PDFPageCountError as err : error_msg = ( core_utils . ERROR_21_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_type} \" , str ( type ( err ))) . replace ( \" {error_msg} \" , str ( err )) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children # ------------------------------------------------------------------ # Processing a PDF file with PDFlib TET. # ------------------------------------------------------------------ @classmethod def pdflib ( cls , full_name_in : str , full_name_out : str , document_opt_list : str , page_opt_list : str , ) -> tuple [ str , str ]: \"\"\"Process a PDF file with PDFlib TET. The data from a PDF file is made available in XML files with the help of PDFlib TET. The granularity of the XML files can be word, line or paragraph depending on the document and page options selected. Args: full_name_in (str): Directory name and file name of the input file. full_name_out (str): Directory name and file name of the output file. document_opt_list (str): Document level options. page_opt_list (str): Page level options. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param document_opt_list= %s \" , document_opt_list ) core_glob . logger . debug ( \"param page_opt_list = %s \" , page_opt_list ) tet = TET . TET () doc_opt_list = f \"tetml= {{ filename= {{ { full_name_out } }}}} { document_opt_list } \" if ( file_curr := tet . open_document ( full_name_in , doc_opt_list )) == - 1 : error_msg = ( core_utils . ERROR_51_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_no} \" , str ( tet . get_errnum ())) . replace ( \" {api_name} \" , tet . get_apiname () + \"()\" ) . replace ( \" {error_msg} \" , tet . get_errmsg ()) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg # get number of pages in the document */ no_pages = tet . pcos_get_number ( file_curr , \"length:pages\" ) # loop over pages in the document */ for page_no in range ( 1 , int ( no_pages ) + 1 ): tet . process_page ( file_curr , page_no , page_opt_list ) # This could be combined with the last page-related call tet . process_page ( file_curr , 0 , \"tetml= {trailer} \" ) tet . close_document ( file_curr ) tet . delete () core_glob . logger . debug ( \"return = %s \" , core_glob . LOGGER_END ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK # ------------------------------------------------------------------ # Converting image files to PDF files via OCR. # ------------------------------------------------------------------ @classmethod def tesseract ( cls , full_name_in : str , full_name_out : str , language_tesseract : str , ) -> tuple [ str , str , list [ str ]]: \"\"\"Convert image files to PDF files via OCR. The documents of the following document types are converted to the PDF format using Tesseract OCR: - bmp - bitmap image file - gif - Graphics Interchange Format - jp2 - JPEG 2000 - jpeg - Joint Photographic Experts Group - png - Portable Network Graphics - pnm - portable any-map format - tif - Tag Image File Format - tiff - Tag Image File Format - webp - Image file format with lossless and lossy compression After processing with Tesseract OCR, the files split previously into multiple image files are combined into a single PDF document. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_tesseract (str): The Tesseract name of the document language. Returns: tuple[str, str, list[str]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_tesseract= %s \" , language_tesseract ) children : list [ str ] = [] pdf_writer = PyPDF2 . PdfWriter () for full_name in sorted ( glob . glob ( full_name_in )): try : pdf = pytesseract . image_to_pdf_or_hocr ( extension = \"pdf\" , image = full_name , lang = language_tesseract , timeout = core_glob . inst_setup . tesseract_timeout , ) with open ( full_name_out , \"w+b\" ) as file_handle : # PDF type is bytes by default file_handle . write ( pdf ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_41_911 . replace ( \" {full_name_out} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] pdf_reader = PyPDF2 . PdfReader ( full_name_out ) for page in pdf_reader . pages : # Add each page to the writer object pdf_writer . add_page ( page ) children . append ( full_name ) except RuntimeError as err : error_msg = core_utils . ERROR_41_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] # Write out the merged PDF with open ( full_name_out , \"wb\" ) as file_handle : pdf_writer . write ( file_handle ) core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children # ------------------------------------------------------------------ # Tokenizing the text from the PDF document. # ------------------------------------------------------------------ @classmethod def tokenizer ( cls , full_name_in : str , full_name_out : str , pipeline_name : str , document_id : int = - 1 , full_name_orig : str = \"\" , no_lines_footer : int = - 1 , no_lines_header : int = - 1 , no_lines_toc : int = - 1 , ) -> tuple [ str , str ]: \"\"\"Tokenizing the text from the PDF document. The line-oriented text is broken down into qualified tokens with the means of SpaCy. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. pipeline_name (str): The loaded SpaCy pipeline. document_id (int, optional): The identification number of the document. Defaults to -1. full_name_orig (str, optional): The file name of the originating document. Defaults to \"\". no_lines_footer (int, optional): Total number of footer lines. Defaults to -1. no_lines_header (int, optional): Total number of header lines. Defaults to -1. no_lines_toc (int, optional): Total number of TOC lines. Defaults to -1. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param no_lines_footer= %i \" , no_lines_footer ) core_glob . logger . debug ( \"param no_lines_header= %i \" , no_lines_header ) core_glob . logger . debug ( \"param no_lines_toc = %i \" , no_lines_toc ) core_glob . logger . debug ( \"param pipeline_name = %s \" , pipeline_name ) try : core_glob . inst_tokenizer . process_document ( document_id = document_id , file_name_next = full_name_out , file_name_orig = full_name_orig , no_lines_footer = no_lines_footer , no_lines_header = no_lines_header , no_lines_toc = no_lines_toc , pipeline_name = pipeline_name , ) except FileNotFoundError : error_msg = core_utils . ERROR_71_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK document ( full_name_in , document_id = None , full_name_orig = None , is_delete_auxiliary_files = None , is_lt_heading_required = None , is_lt_list_bullet_required = None , is_lt_list_number_required = None , is_lt_toc_required = None , is_verbose = None , language_pandoc = None , language_spacy = None , language_tesseract = None , output_directory = None ) Document content recognition for a specific file. This method extracts the document content structure from a given document and stores it in JSON format. For this purpose, all non-pdf documents and all scanned pdf documents are first converted into a searchable pdf format. Depending on the file format, the tools Pandoc, pdf2image or Tesseract OCR are used for this purpose. PDFlib TET then extracts the text and metadata from the searchable pdf file and makes them available in XML format. spaCY generates qualified tokens from the document text, and these token data are then made available together with the metadata in a JSON format. Parameters: Name Type Description Default full_name_in str Full file name of the document file. required document_id int Document identification. Defaults to -1 i.e. no document identification. None full_name_orig str Original full file name. Defaults to the full file name of the document file. None is_delete_auxiliary_files bool Delete the auxiliary files after a successful processing step. Defaults to parameter delete_auxiliary_files in setup.cfg . None is_lt_heading_required bool If it is set to true , the determination of the heading lines is performed. Defaults to parameter lt_heading_required in setup.cfg . None is_lt_list_bullet_required bool If it is set to true , the determination of the bulleted lists is performed. Defaults to parameter lt_list_bullet_required in setup.cfg . None is_lt_list_number_required bool If it is set to true , the determination of the numbered lists is performed. Defaults to parameter lt_list_number_required in setup.cfg . None is_lt_toc_required bool If it is set to true , the determination of the TOC lines is performed. Defaults to parameter lt_toc_required in setup.cfg . None is_verbose bool Display progress messages for processing. Defaults to parameter verbose in setup.cfg . None language_pandoc str Pandoc language code. Defaults to English. None language_spacy str spaCy language code. Defaults to English transformer pipeline (roberta-base).. None language_tesseract str Tesseract OCR language code. Defaults to English. None output_directory str Directory for the flat files to be created. Defaults to the directory of the document file. None Raises: Type Description RuntimeError Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR. Source code in src/dcr_core/cls_process.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 def document ( # pylint: disable=too-many-arguments self , full_name_in : str , document_id : int = None , full_name_orig : str = None , is_delete_auxiliary_files : bool = None , is_lt_heading_required : bool = None , is_lt_list_bullet_required : bool = None , is_lt_list_number_required : bool = None , is_lt_toc_required : bool = None , is_verbose : bool = None , language_pandoc : str = None , language_spacy : str = None , language_tesseract : str = None , output_directory : str = None , ) -> None : \"\"\"Document content recognition for a specific file. This method extracts the document content structure from a given document and stores it in JSON format. For this purpose, all non-pdf documents and all scanned pdf documents are first converted into a searchable pdf format. Depending on the file format, the tools Pandoc, pdf2image or Tesseract OCR are used for this purpose. PDFlib TET then extracts the text and metadata from the searchable pdf file and makes them available in XML format. spaCY generates qualified tokens from the document text, and these token data are then made available together with the metadata in a JSON format. Args: full_name_in (str): Full file name of the document file. document_id (int, optional): Document identification. Defaults to -1 i.e. no document identification. full_name_orig (str, optional): Original full file name. Defaults to the full file name of the document file. is_delete_auxiliary_files (bool, optional): Delete the auxiliary files after a successful processing step. Defaults to parameter `delete_auxiliary_files` in `setup.cfg`. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to parameter `lt_heading_required` in `setup.cfg`. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to parameter `lt_list_bullet_required` in `setup.cfg`. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to parameter `lt_list_number_required` in `setup.cfg`. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to parameter `lt_toc_required` in `setup.cfg`. is_verbose (bool, optional): Display progress messages for processing. Defaults to parameter `verbose` in `setup.cfg`. language_pandoc (str, optional): Pandoc language code. Defaults to English. language_spacy (str, optional): spaCy language code. Defaults to English transformer pipeline (roberta-base).. language_tesseract (str, optional): Tesseract OCR language code. Defaults to English. output_directory (str, optional): Directory for the flat files to be created. Defaults to the directory of the document file. Raises: RuntimeError: Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR. \"\"\" # Initialise the logging functionality. core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param is_delete_auxiliary_files = %s \" , is_delete_auxiliary_files ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param is_verbose = %s \" , is_verbose ) core_glob . logger . debug ( \"param language_pandoc = %s \" , language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , language_tesseract ) core_glob . logger . debug ( \"param output_directory = %s \" , output_directory ) self . _document_init () self . _document_id = document_id if document_id else - 1 self . _full_name_in = full_name_in self . _full_name_orig = full_name_orig if full_name_orig else full_name_in # Load the configuration parameters. core_glob . inst_setup = setup . Setup () self . _is_delete_auxiliary_files = ( is_delete_auxiliary_files if is_delete_auxiliary_files is not None else core_glob . inst_setup . is_delete_auxiliary_files ) self . _is_lt_heading_required = ( is_lt_heading_required if is_lt_heading_required is not None else core_glob . inst_setup . is_lt_heading_required ) self . _is_lt_list_bullet_required = ( is_lt_list_bullet_required if is_lt_list_bullet_required is not None else core_glob . inst_setup . is_lt_list_bullet_required ) self . _is_lt_list_number_required = ( is_lt_list_number_required if is_lt_list_number_required is not None else core_glob . inst_setup . is_lt_list_number_required ) self . _is_lt_toc_required = is_lt_toc_required if is_lt_toc_required is not None else core_glob . inst_setup . is_lt_toc_required self . _is_verbose = is_verbose if is_verbose is not None else core_glob . inst_setup . is_verbose self . _language_pandoc = language_pandoc if language_pandoc else nlp_core . NLPCore . LANGUAGE_PANDOC_DEFAULT self . _language_spacy = language_spacy if language_spacy else nlp_core . NLPCore . LANGUAGE_SPACY_DEFAULT self . _language_tesseract = language_tesseract if language_tesseract else nlp_core . NLPCore . LANGUAGE_TESSERACT_DEFAULT core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , self . _is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , self . _is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , self . _is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , self . _is_lt_toc_required ) core_glob . logger . debug ( \"param full_name_orig = %s \" , self . _full_name_orig ) core_glob . logger . debug ( \"param language_pandoc = %s \" , self . _language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , self . _language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , self . _language_tesseract ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Pandoc { self . _language_pandoc } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key spaCy { self . _language_spacy } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Tesseract OCR { self . _language_tesseract } \" ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) ( full_name_in_directory , self . _full_name_in_stem_name , self . _full_name_in_extension , ) = core_utils . get_components_from_full_name ( self . _full_name_in ) self . _full_name_in_directory = output_directory if output_directory is not None else full_name_in_directory self . _full_name_in_extension_int = ( self . _full_name_in_extension . lower () if self . _full_name_in_extension else self . _full_name_in_extension ) self . _document_check_extension () self . _document_pandoc () self . _document_pdf2image () self . _document_tesseract () self . _document_pdflib () self . _document_parser () self . _document_tokenizer () core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"End processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , \"=\" * 80 ) core_glob . logger . debug ( core_glob . LOGGER_END ) pandoc ( full_name_in , full_name_out , language_pandoc ) classmethod Convert a Non-PDF file to a PDF file. The following file formats are converted into PDF format here with the help of Pandoc: csv - comma-separated values docx - Office Open XML epub - e-book file format html - HyperText Markup Language odt - Open Document Format for Office Applications rst - reStructuredText (RST rtf - Rich Text Format Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required language_pandoc str The Pandoc name of the document language. required Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 @classmethod def pandoc ( cls , full_name_in : str , full_name_out : str , language_pandoc : str , ) -> tuple [ str , str ]: \"\"\"Convert a Non-PDF file to a PDF file. The following file formats are converted into PDF format here with the help of Pandoc: - csv - comma-separated values - docx - Office Open XML - epub - e-book file format - html - HyperText Markup Language - odt - Open Document Format for Office Applications - rst - reStructuredText (RST - rtf - Rich Text Format Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_pandoc (str): The Pandoc name of the document language. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_pandoc= %s \" , language_pandoc ) # Convert the document extra_args = [ f \"--pdf-engine= { Process . PANDOC_PDF_ENGINE_XELATEX } \" , \"-V\" , f \"lang: { language_pandoc } \" , ] try : pypandoc . convert_file ( full_name_in , core_glob . FILE_TYPE_PDF , extra_args = extra_args , outputfile = full_name_out , ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_31_911 . replace ( \" {full_name} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except FileNotFoundError : error_msg = core_utils . ERROR_31_902 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except RuntimeError as err : error_msg = core_utils . ERROR_31_903 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK parser ( full_name_in , full_name_out , no_pdf_pages , document_id =- 1 , full_name_orig = None , is_lt_heading_required = False , is_lt_list_bullet_required = False , is_lt_list_number_required = False , is_lt_toc_required = False ) classmethod Extract the text from the PDF document. From the line-oriented XML output file of PDFlib TET, the text and relevant metadata are extracted with the help of an XML parser and stored in a JSON file. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required no_pdf_pages int Total number of PDF pages. required document_id int The identification number of the document. Defaults to None. -1 full_name_orig str The file name of the originating document. Defaults to None. None is_lt_heading_required bool If it is set to true , the determination of the heading lines is performed. Defaults to False. False is_lt_list_bullet_required bool If it is set to true , the determination of the bulleted lists is performed. Defaults to False. False is_lt_list_number_required bool If it is set to true , the determination of the numbered lists is performed. Defaults to False. False is_lt_toc_required bool If it is set to true , the determination of the TOC lines is performed. Defaults to False. False Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Raises: Type Description RuntimeError If the parser has detected any errors. Source code in src/dcr_core/cls_process.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 @classmethod def parser ( # noqa: C901 cls , full_name_in : str , full_name_out : str , no_pdf_pages : int , document_id : int = - 1 , full_name_orig : str = None , is_lt_heading_required : bool = False , is_lt_list_bullet_required : bool = False , is_lt_list_number_required : bool = False , is_lt_toc_required : bool = False , ) -> tuple [ str , str ]: \"\"\"Extract the text from the PDF document. From the line-oriented XML output file of PDFlib TET, the text and relevant metadata are extracted with the help of an XML parser and stored in a JSON file. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. no_pdf_pages (int): Total number of PDF pages. document_id (int, optional): The identification number of the document. Defaults to None. full_name_orig (str, optional): The file name of the originating document. Defaults to None. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to False. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to False. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to False. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to False. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Raises: RuntimeError: If the parser has detected any errors. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param no_pdf_pages = %i \" , no_pdf_pages ) try : # ------------------------------------------------------------------ # Granularity 'word'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in ) # Get the root Element root = tree . getroot () core_glob . inst_parser = parser . TextParser () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_word ( directory_name = os . path . dirname ( full_name_in ), document_id = document_id , environment_variant = core_glob . inst_setup . environment_variant , file_name_curr = os . path . basename ( full_name_in ), file_name_next = full_name_out , file_name_orig = full_name_orig , is_lt_heading_required = is_lt_heading_required , is_lt_list_bullet_required = is_lt_list_bullet_required , is_lt_list_number_required = is_lt_list_number_required , is_lt_toc_required = is_lt_toc_required , no_pdf_pages = no_pdf_pages , parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML word granularity parsed { full_name_in } \" ) # ------------------------------------------------------------------ # Granularity 'line'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in . replace ( \"word.xml\" , \"line.xml\" )) # Get the root Element root = tree . getroot () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_line ( parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML line granularity parsed { full_name_in } \" ) except FileNotFoundError : error_msg = core_utils . ERROR_61_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK pdf2image ( full_name_in , output_directory ) classmethod Convert a scanned PDF file to a set of image files. To extract the text from a scanned PDF document, it must first be converted into one or more image files, depending on the number of pages. Then these image files are converted into a normal PDF document with the help of an OCR programme. The input file for this method must be a scanned PDF document, which is then converted into image files with the help of PDF2Image. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required output_directory str Directory for the flat files to be created. required Returns: Type Description tuple [ str , str , list [ tuple [ str , str ]]] tuple[str, str, list[tuple[str,str]]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 @classmethod def pdf2image ( cls , full_name_in : str , output_directory : str , ) -> tuple [ str , str , list [ tuple [ str , str ]]]: \"\"\"Convert a scanned PDF file to a set of image files. To extract the text from a scanned PDF document, it must first be converted into one or more image files, depending on the number of pages. Then these image files are converted into a normal PDF document with the help of an OCR programme. The input file for this method must be a scanned PDF document, which is then converted into image files with the help of PDF2Image. Args: full_name_in (str): The directory name and file name of the input file. output_directory (str): Directory for the flat files to be created. Returns: tuple[str, str, list[tuple[str,str]]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in= %s \" , full_name_in ) try : images = pdf2image . convert_from_path ( full_name_in ) children : list [ tuple [ str , str ]] = [] no_children = 0 directory_name = os . path . dirname ( full_name_in ) stem_name = os . path . splitext ( os . path . basename ( full_name_in ))[ 0 ] try : os . remove ( core_utils . get_full_name_from_components ( directory_name , stem_name + \"_*.\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ), ) ) except OSError : pass # Store the image pages for img in images : no_children += 1 file_name_next = ( stem_name + \".pdf2image_\" + str ( no_children ) + \".\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ) ) full_name_next = core_utils . get_full_name_from_components ( output_directory , file_name_next , ) img . save ( full_name_next , core_glob . inst_setup . pdf2image_type , ) children . append (( file_name_next , full_name_next )) except PDFPageCountError as err : error_msg = ( core_utils . ERROR_21_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_type} \" , str ( type ( err ))) . replace ( \" {error_msg} \" , str ( err )) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children pdflib ( full_name_in , full_name_out , document_opt_list , page_opt_list ) classmethod Process a PDF file with PDFlib TET. The data from a PDF file is made available in XML files with the help of PDFlib TET. The granularity of the XML files can be word, line or paragraph depending on the document and page options selected. Parameters: Name Type Description Default full_name_in str Directory name and file name of the input file. required full_name_out str Directory name and file name of the output file. required document_opt_list str Document level options. required page_opt_list str Page level options. required Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 @classmethod def pdflib ( cls , full_name_in : str , full_name_out : str , document_opt_list : str , page_opt_list : str , ) -> tuple [ str , str ]: \"\"\"Process a PDF file with PDFlib TET. The data from a PDF file is made available in XML files with the help of PDFlib TET. The granularity of the XML files can be word, line or paragraph depending on the document and page options selected. Args: full_name_in (str): Directory name and file name of the input file. full_name_out (str): Directory name and file name of the output file. document_opt_list (str): Document level options. page_opt_list (str): Page level options. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param document_opt_list= %s \" , document_opt_list ) core_glob . logger . debug ( \"param page_opt_list = %s \" , page_opt_list ) tet = TET . TET () doc_opt_list = f \"tetml= {{ filename= {{ { full_name_out } }}}} { document_opt_list } \" if ( file_curr := tet . open_document ( full_name_in , doc_opt_list )) == - 1 : error_msg = ( core_utils . ERROR_51_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_no} \" , str ( tet . get_errnum ())) . replace ( \" {api_name} \" , tet . get_apiname () + \"()\" ) . replace ( \" {error_msg} \" , tet . get_errmsg ()) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg # get number of pages in the document */ no_pages = tet . pcos_get_number ( file_curr , \"length:pages\" ) # loop over pages in the document */ for page_no in range ( 1 , int ( no_pages ) + 1 ): tet . process_page ( file_curr , page_no , page_opt_list ) # This could be combined with the last page-related call tet . process_page ( file_curr , 0 , \"tetml= {trailer} \" ) tet . close_document ( file_curr ) tet . delete () core_glob . logger . debug ( \"return = %s \" , core_glob . LOGGER_END ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK tesseract ( full_name_in , full_name_out , language_tesseract ) classmethod Convert image files to PDF files via OCR. The documents of the following document types are converted to the PDF format using Tesseract OCR: bmp - bitmap image file gif - Graphics Interchange Format jp2 - JPEG 2000 jpeg - Joint Photographic Experts Group png - Portable Network Graphics pnm - portable any-map format tif - Tag Image File Format tiff - Tag Image File Format webp - Image file format with lossless and lossy compression After processing with Tesseract OCR, the files split previously into multiple image files are combined into a single PDF document. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required language_tesseract str The Tesseract name of the document language. required Returns: Type Description tuple [ str , str , list [ str ]] tuple[str, str, list[str]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 @classmethod def tesseract ( cls , full_name_in : str , full_name_out : str , language_tesseract : str , ) -> tuple [ str , str , list [ str ]]: \"\"\"Convert image files to PDF files via OCR. The documents of the following document types are converted to the PDF format using Tesseract OCR: - bmp - bitmap image file - gif - Graphics Interchange Format - jp2 - JPEG 2000 - jpeg - Joint Photographic Experts Group - png - Portable Network Graphics - pnm - portable any-map format - tif - Tag Image File Format - tiff - Tag Image File Format - webp - Image file format with lossless and lossy compression After processing with Tesseract OCR, the files split previously into multiple image files are combined into a single PDF document. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_tesseract (str): The Tesseract name of the document language. Returns: tuple[str, str, list[str]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_tesseract= %s \" , language_tesseract ) children : list [ str ] = [] pdf_writer = PyPDF2 . PdfWriter () for full_name in sorted ( glob . glob ( full_name_in )): try : pdf = pytesseract . image_to_pdf_or_hocr ( extension = \"pdf\" , image = full_name , lang = language_tesseract , timeout = core_glob . inst_setup . tesseract_timeout , ) with open ( full_name_out , \"w+b\" ) as file_handle : # PDF type is bytes by default file_handle . write ( pdf ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_41_911 . replace ( \" {full_name_out} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] pdf_reader = PyPDF2 . PdfReader ( full_name_out ) for page in pdf_reader . pages : # Add each page to the writer object pdf_writer . add_page ( page ) children . append ( full_name ) except RuntimeError as err : error_msg = core_utils . ERROR_41_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] # Write out the merged PDF with open ( full_name_out , \"wb\" ) as file_handle : pdf_writer . write ( file_handle ) core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children tokenizer ( full_name_in , full_name_out , pipeline_name , document_id =- 1 , full_name_orig = '' , no_lines_footer =- 1 , no_lines_header =- 1 , no_lines_toc =- 1 ) classmethod Tokenizing the text from the PDF document. The line-oriented text is broken down into qualified tokens with the means of SpaCy. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required pipeline_name str The loaded SpaCy pipeline. required document_id int The identification number of the document. Defaults to -1. -1 full_name_orig str The file name of the originating document. Defaults to \"\". '' no_lines_footer int Total number of footer lines. Defaults to -1. -1 no_lines_header int Total number of header lines. Defaults to -1. -1 no_lines_toc int Total number of TOC lines. Defaults to -1. -1 Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 @classmethod def tokenizer ( cls , full_name_in : str , full_name_out : str , pipeline_name : str , document_id : int = - 1 , full_name_orig : str = \"\" , no_lines_footer : int = - 1 , no_lines_header : int = - 1 , no_lines_toc : int = - 1 , ) -> tuple [ str , str ]: \"\"\"Tokenizing the text from the PDF document. The line-oriented text is broken down into qualified tokens with the means of SpaCy. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. pipeline_name (str): The loaded SpaCy pipeline. document_id (int, optional): The identification number of the document. Defaults to -1. full_name_orig (str, optional): The file name of the originating document. Defaults to \"\". no_lines_footer (int, optional): Total number of footer lines. Defaults to -1. no_lines_header (int, optional): Total number of header lines. Defaults to -1. no_lines_toc (int, optional): Total number of TOC lines. Defaults to -1. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param no_lines_footer= %i \" , no_lines_footer ) core_glob . logger . debug ( \"param no_lines_header= %i \" , no_lines_header ) core_glob . logger . debug ( \"param no_lines_toc = %i \" , no_lines_toc ) core_glob . logger . debug ( \"param pipeline_name = %s \" , pipeline_name ) try : core_glob . inst_tokenizer . process_document ( document_id = document_id , file_name_next = full_name_out , file_name_orig = full_name_orig , no_lines_footer = no_lines_footer , no_lines_header = no_lines_header , no_lines_toc = no_lines_toc , pipeline_name = pipeline_name , ) except FileNotFoundError : error_msg = core_utils . ERROR_71_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK","title":"API Documentation"},{"location":"application_api_documentation/#dcr-core-application-api-documentation","text":"","title":"DCR-CORE - Application - API Documentation"},{"location":"application_api_documentation/#documentation-for-cls_process","text":"Process utility class. Source code in src/dcr_core/cls_process.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 class Process : \"\"\"Process utility class.\"\"\" # ------------------------------------------------------------------ # Class variables. # ------------------------------------------------------------------ PANDOC_PDF_ENGINE_LULATEX : ClassVar [ str ] = \"lulatex\" PANDOC_PDF_ENGINE_XELATEX : ClassVar [ str ] = \"xelatex\" # ------------------------------------------------------------------ # Initialise the instance. # ------------------------------------------------------------------ def __init__ ( self ) -> None : \"\"\"Initialise the instance.\"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) self . _document_id : int = 0 self . _full_name_in = \"\" self . _full_name_in_directory = \"\" self . _full_name_in_extension = \"\" self . _full_name_in_extension_int = \"\" self . _full_name_in_next_step = \"\" self . _full_name_in_pandoc = \"\" self . _full_name_in_parser_line = \"\" self . _full_name_in_parser_word = \"\" self . _full_name_in_pdf2image = \"\" self . _full_name_in_pdflib = \"\" self . _full_name_in_stem_name = \"\" self . _full_name_in_tesseract = \"\" self . _full_name_in_tokenizer_line = \"\" self . _full_name_in_tokenizer_word = \"\" self . _full_name_orig = \"\" self . _is_delete_auxiliary_files = False self . _is_lt_heading_required = False self . _is_lt_list_bullet_required = False self . _is_lt_list_number_required = False self . _is_lt_toc_required = False self . _is_pandoc = False self . _is_pdf2image = False self . _is_tesseract = False self . _is_verbose = False self . _language_pandoc : str = \"\" self . _language_spacy : str = \"\" self . _language_tesseract : str = \"\" self . _no_lines_footer : int = 0 self . _no_lines_header : int = 0 self . _no_lines_toc : int = 0 self . _no_pdf_pages : int = 0 self . _exist = True core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Check the document by the file extension and determine further # processing. # ------------------------------------------------------------------ def _document_check_extension ( self ): \"\"\"Document processing control. Check the document by the file extension and determine further processing. Raises: RuntimeError: ERROR_01_903 \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) if self . _full_name_in_extension_int == core_glob . FILE_TYPE_PDF : try : if bool ( \"\" . join ([ page . get_text () for page in fitz . open ( self . _full_name_in )])): self . _full_name_in_pdflib = self . _full_name_in else : self . _is_pdf2image = True self . _is_tesseract = True self . _full_name_in_pdf2image = self . _full_name_in except RuntimeError as exc : raise RuntimeError ( core_utils . ERROR_01_903 . replace ( \" {file_name} \" , self . _full_name_in ) . replace ( \" {error_msg} \" , str ( exc )), ) from exc elif self . _full_name_in_extension_int in core_glob . FILE_TYPE_PANDOC : self . _is_pandoc = True self . _full_name_in_pandoc = self . _full_name_in elif self . _full_name_in_extension_int in core_glob . FILE_TYPE_TESSERACT : self . _is_tesseract = True self . _full_name_in_tesseract = self . _full_name_in else : raise RuntimeError ( core_utils . ERROR_01_901 . replace ( \" {extension} \" , self . _full_name_in_extension_int )) core_glob . logger . debug ( core_glob . LOGGER_END ) # ----------------------------------------------------------------------------- # Delete the given auxiliary file. # ----------------------------------------------------------------------------- def _document_delete_auxiliary_file ( self , full_name : str ) -> None : \"\"\"Delete the given auxiliary file. Args: full_name (str): File name. \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name= %s \" , full_name ) if not self . _is_delete_auxiliary_files : return # Don't remove the base document !!! if full_name == self . _full_name_in : return if os . path . isfile ( full_name ): os . remove ( full_name ) core_utils . progress_msg ( self . _is_verbose , f \"Auxiliary file ' { full_name } ' deleted\" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Initialize the document recognition process. # ------------------------------------------------------------------ def _document_init ( self ) -> None : \"\"\"Initialize the document recognition process.\"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) self . _document_id : int = 0 self . _full_name_in : str = \"\" self . _full_name_in_directory : str = \"\" self . _full_name_in_extension : str = \"\" self . _full_name_in_extension_int : str = \"\" self . _full_name_in_next_step : str = \"\" self . _full_name_in_pandoc : str = \"\" self . _full_name_in_parser_line : str = \"\" self . _full_name_in_parser_word : str = \"\" self . _full_name_in_pdf2image : str = \"\" self . _full_name_in_pdflib : str = \"\" self . _full_name_in_stem_name : str = \"\" self . _full_name_in_tesseract : str = \"\" self . _full_name_in_tokenizer : str = \"\" self . _full_name_orig : str = \"\" self . _is_pandoc : bool = False self . _is_pdf2image : bool = False self . _is_tesseract : bool = False self . _language_pandoc : str = \"\" self . _language_spacy : str = \"\" self . _language_tesseract : str = \"\" self . _no_lines_footer : int = 0 self . _no_lines_header : int = 0 self . _no_lines_toc : int = 0 self . _no_pdf_pages : int = 0 core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert the document to PDF format using Pandoc. # ------------------------------------------------------------------ def _document_pandoc ( self ): \"\"\"Convert the document to PDF format using Pandoc. Raises: RuntimeError: Any Pandoc issue. \"\"\" if self . _is_pandoc : core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing Pandoc { self . _full_name_in_pandoc } \" ) self . _full_name_in_pdflib = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pandoc\" , core_glob . FILE_TYPE_PDF ) return_code , error_msg = Process . pandoc ( self . _full_name_in_pandoc , self . _full_name_in_pdflib , self . _language_pandoc , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_pandoc ) core_utils . progress_msg ( self . _is_verbose , f \"End processing Pandoc { self . _full_name_in_pdflib } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Extract the text for all granularities from the PDF document. # ------------------------------------------------------------------ def _document_parser ( self ): \"\"\"Extract the text for all granularities from the PDF document.\"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , \"Start processing Parser\" ) self . _full_name_in_tokenizer = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".parser.\" + core_glob . FILE_TYPE_JSON , ) return_code , error_msg = Process . parser ( document_id = self . _document_id , full_name_in = self . _full_name_in_parser_word , full_name_orig = self . _full_name_orig , full_name_out = self . _full_name_in_tokenizer , is_lt_heading_required = self . _is_lt_heading_required , is_lt_list_bullet_required = self . _is_lt_list_bullet_required , is_lt_list_number_required = self . _is_lt_list_number_required , is_lt_toc_required = self . _is_lt_toc_required , no_pdf_pages = self . _no_pdf_pages , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_parser_line ) self . _document_delete_auxiliary_file ( self . _full_name_in_parser_word ) self . _no_lines_footer = core_glob . inst_lt_hf . no_lines_footer self . _no_lines_header = core_glob . inst_lt_hf . no_lines_header if self . _is_lt_toc_required : self . _no_lines_toc = core_glob . inst_lt_toc . no_lines_toc core_utils . progress_msg ( self . _is_verbose , f \"End processing Parser { self . _full_name_in_tokenizer } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert the PDF document to an image file using pdf2image. # ------------------------------------------------------------------ def _document_pdf2image ( self ): \"\"\"Convert the PDF document to an image file using pdf2image. Raises: RuntimeError: Any pdf2image issue. \"\"\" if self . _is_pdf2image : core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing pdf2image { self . _full_name_in_pdf2image } \" ) self . _full_name_in_tesseract = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pdf2image\" + \"_[0-9]*.\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ), ) return_code , error_msg , _ = Process . pdf2image ( self . _full_name_in_pdf2image , self . _full_name_in_directory , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_pdf2image ) core_utils . progress_msg ( self . _is_verbose , f \"End processing pdf2image { self . _full_name_in_tesseract } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Extract the text and metadata from a PDF document to an XML file. # ------------------------------------------------------------------ def _document_pdflib ( self ): \"\"\"Extract the text and metadata from a PDF document to an XML file. Raises: RuntimeError: Any PDFlib TET issue. \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing PDFlib TET { self . _full_name_in_pdflib } \" ) # noinspection PyUnresolvedReferences self . _no_pdf_pages = len ( PyPDF2 . PdfReader ( self . _full_name_in_pdflib ) . pages ) if self . _no_pdf_pages == 0 : raise RuntimeError ( f \"The number of pages of the PDF document { self . _full_name_in_pdflib } cannot be determined\" ) # ------------------------------------------------------------------ # Granularity 'line'. # ------------------------------------------------------------------ self . _full_name_in_parser_line = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pdflib.\" + nlp_core . NLPCore . LINE_XML_VARIATION + core_glob . FILE_TYPE_XML , ) return_code , error_msg = Process . pdflib ( full_name_in = self . _full_name_in_pdflib , full_name_out = self . _full_name_in_parser_line , document_opt_list = nlp_core . NLPCore . LINE_TET_DOCUMENT_OPT_LIST , page_opt_list = nlp_core . NLPCore . LINE_TET_PAGE_OPT_LIST , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) core_utils . progress_msg ( self . _is_verbose , f \"TETML line granularity created { self . _full_name_in_parser_line } \" ) # ------------------------------------------------------------------ # Granularity 'word'. # ------------------------------------------------------------------ self . _full_name_in_parser_word = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".pdflib.\" + nlp_core . NLPCore . WORD_XML_VARIATION + core_glob . FILE_TYPE_XML , ) return_code , error_msg = Process . pdflib ( full_name_in = self . _full_name_in_pdflib , full_name_out = self . _full_name_in_parser_word , document_opt_list = nlp_core . NLPCore . WORD_TET_DOCUMENT_OPT_LIST , page_opt_list = nlp_core . NLPCore . WORD_TET_PAGE_OPT_LIST , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) core_utils . progress_msg ( self . _is_verbose , f \"TETML word granularity created { self . _full_name_in_parser_word } \" ) core_utils . progress_msg ( self . _is_verbose , \"End processing PDFlib TET\" ) self . _document_delete_auxiliary_file ( self . _full_name_in_pdflib ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert one or more image files to a PDF file using Tesseract OCR. # ------------------------------------------------------------------ def _document_tesseract ( self ): \"\"\"Process the document with Tesseract OCR. Convert one or more image files to a PDF file using Tesseract OCR. Raises: RuntimeError: Any Tesseract OCR issue. \"\"\" if self . _is_tesseract : core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing Tesseract OCR { self . _full_name_in_tesseract } \" ) if self . _is_pdf2image : self . _full_name_in_stem_name += \"_0\" self . _full_name_in_pdflib = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".tesseract\" , core_glob . FILE_TYPE_PDF ) return_code , error_msg , children = Process . tesseract ( self . _full_name_in_tesseract , self . _full_name_in_pdflib , self . _language_tesseract , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) # noinspection PyUnresolvedReferences self . _no_pdf_pages = len ( PyPDF2 . PdfReader ( self . _full_name_in_pdflib ) . pages ) if self . _no_pdf_pages == 0 : raise RuntimeError ( f \"The number of pages of the PDF document { self . _full_name_in_pdflib } cannot be determined\" ) for child in children : self . _document_delete_auxiliary_file ( child ) core_utils . progress_msg ( self . _is_verbose , f \"End processing Tesseract OCR { self . _full_name_in_pdflib } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Convert the PDF document to an image file using pdf2image. # ------------------------------------------------------------------ def _document_tokenizer ( self ) -> None : \"\"\"Tokenize the document with spaCy. Raises: RuntimeError: Any spaCy issue. \"\"\" core_glob . logger . debug ( core_glob . LOGGER_START ) core_utils . progress_msg ( self . _is_verbose , \"\" ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing spaCy { self . _full_name_in_tokenizer } \" ) try : core_glob . inst_tokenizer . exists () except AttributeError : core_glob . inst_tokenizer = tokenizer . TokenizerSpacy () self . _full_name_in_next_step = core_utils . get_full_name_from_components ( self . _full_name_in_directory , self . _full_name_in_stem_name + \".tokenizer.\" + core_glob . FILE_TYPE_JSON , ) return_code , error_msg = Process . tokenizer ( full_name_in = self . _full_name_in_tokenizer , full_name_out = self . _full_name_in_next_step , pipeline_name = self . _language_spacy , document_id = self . _document_id , full_name_orig = self . _full_name_orig , no_lines_footer = self . _no_lines_footer , no_lines_header = self . _no_lines_header , no_lines_toc = self . _no_lines_toc , ) if return_code != \"ok\" : raise RuntimeError ( error_msg ) self . _document_delete_auxiliary_file ( self . _full_name_in_tokenizer ) core_utils . progress_msg ( self . _is_verbose , f \"End processing spaCy { self . _full_name_in_next_step } \" ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Document content recognition for a specific file. # ------------------------------------------------------------------ def document ( # pylint: disable=too-many-arguments self , full_name_in : str , document_id : int = None , full_name_orig : str = None , is_delete_auxiliary_files : bool = None , is_lt_heading_required : bool = None , is_lt_list_bullet_required : bool = None , is_lt_list_number_required : bool = None , is_lt_toc_required : bool = None , is_verbose : bool = None , language_pandoc : str = None , language_spacy : str = None , language_tesseract : str = None , output_directory : str = None , ) -> None : \"\"\"Document content recognition for a specific file. This method extracts the document content structure from a given document and stores it in JSON format. For this purpose, all non-pdf documents and all scanned pdf documents are first converted into a searchable pdf format. Depending on the file format, the tools Pandoc, pdf2image or Tesseract OCR are used for this purpose. PDFlib TET then extracts the text and metadata from the searchable pdf file and makes them available in XML format. spaCY generates qualified tokens from the document text, and these token data are then made available together with the metadata in a JSON format. Args: full_name_in (str): Full file name of the document file. document_id (int, optional): Document identification. Defaults to -1 i.e. no document identification. full_name_orig (str, optional): Original full file name. Defaults to the full file name of the document file. is_delete_auxiliary_files (bool, optional): Delete the auxiliary files after a successful processing step. Defaults to parameter `delete_auxiliary_files` in `setup.cfg`. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to parameter `lt_heading_required` in `setup.cfg`. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to parameter `lt_list_bullet_required` in `setup.cfg`. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to parameter `lt_list_number_required` in `setup.cfg`. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to parameter `lt_toc_required` in `setup.cfg`. is_verbose (bool, optional): Display progress messages for processing. Defaults to parameter `verbose` in `setup.cfg`. language_pandoc (str, optional): Pandoc language code. Defaults to English. language_spacy (str, optional): spaCy language code. Defaults to English transformer pipeline (roberta-base).. language_tesseract (str, optional): Tesseract OCR language code. Defaults to English. output_directory (str, optional): Directory for the flat files to be created. Defaults to the directory of the document file. Raises: RuntimeError: Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR. \"\"\" # Initialise the logging functionality. core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param is_delete_auxiliary_files = %s \" , is_delete_auxiliary_files ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param is_verbose = %s \" , is_verbose ) core_glob . logger . debug ( \"param language_pandoc = %s \" , language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , language_tesseract ) core_glob . logger . debug ( \"param output_directory = %s \" , output_directory ) self . _document_init () self . _document_id = document_id if document_id else - 1 self . _full_name_in = full_name_in self . _full_name_orig = full_name_orig if full_name_orig else full_name_in # Load the configuration parameters. core_glob . inst_setup = setup . Setup () self . _is_delete_auxiliary_files = ( is_delete_auxiliary_files if is_delete_auxiliary_files is not None else core_glob . inst_setup . is_delete_auxiliary_files ) self . _is_lt_heading_required = ( is_lt_heading_required if is_lt_heading_required is not None else core_glob . inst_setup . is_lt_heading_required ) self . _is_lt_list_bullet_required = ( is_lt_list_bullet_required if is_lt_list_bullet_required is not None else core_glob . inst_setup . is_lt_list_bullet_required ) self . _is_lt_list_number_required = ( is_lt_list_number_required if is_lt_list_number_required is not None else core_glob . inst_setup . is_lt_list_number_required ) self . _is_lt_toc_required = is_lt_toc_required if is_lt_toc_required is not None else core_glob . inst_setup . is_lt_toc_required self . _is_verbose = is_verbose if is_verbose is not None else core_glob . inst_setup . is_verbose self . _language_pandoc = language_pandoc if language_pandoc else nlp_core . NLPCore . LANGUAGE_PANDOC_DEFAULT self . _language_spacy = language_spacy if language_spacy else nlp_core . NLPCore . LANGUAGE_SPACY_DEFAULT self . _language_tesseract = language_tesseract if language_tesseract else nlp_core . NLPCore . LANGUAGE_TESSERACT_DEFAULT core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , self . _is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , self . _is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , self . _is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , self . _is_lt_toc_required ) core_glob . logger . debug ( \"param full_name_orig = %s \" , self . _full_name_orig ) core_glob . logger . debug ( \"param language_pandoc = %s \" , self . _language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , self . _language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , self . _language_tesseract ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Pandoc { self . _language_pandoc } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key spaCy { self . _language_spacy } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Tesseract OCR { self . _language_tesseract } \" ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) ( full_name_in_directory , self . _full_name_in_stem_name , self . _full_name_in_extension , ) = core_utils . get_components_from_full_name ( self . _full_name_in ) self . _full_name_in_directory = output_directory if output_directory is not None else full_name_in_directory self . _full_name_in_extension_int = ( self . _full_name_in_extension . lower () if self . _full_name_in_extension else self . _full_name_in_extension ) self . _document_check_extension () self . _document_pandoc () self . _document_pdf2image () self . _document_tesseract () self . _document_pdflib () self . _document_parser () self . _document_tokenizer () core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"End processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , \"=\" * 80 ) core_glob . logger . debug ( core_glob . LOGGER_END ) # ------------------------------------------------------------------ # Converting a Non-PDF file to a PDF file. # ------------------------------------------------------------------ @classmethod def pandoc ( cls , full_name_in : str , full_name_out : str , language_pandoc : str , ) -> tuple [ str , str ]: \"\"\"Convert a Non-PDF file to a PDF file. The following file formats are converted into PDF format here with the help of Pandoc: - csv - comma-separated values - docx - Office Open XML - epub - e-book file format - html - HyperText Markup Language - odt - Open Document Format for Office Applications - rst - reStructuredText (RST - rtf - Rich Text Format Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_pandoc (str): The Pandoc name of the document language. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_pandoc= %s \" , language_pandoc ) # Convert the document extra_args = [ f \"--pdf-engine= { Process . PANDOC_PDF_ENGINE_XELATEX } \" , \"-V\" , f \"lang: { language_pandoc } \" , ] try : pypandoc . convert_file ( full_name_in , core_glob . FILE_TYPE_PDF , extra_args = extra_args , outputfile = full_name_out , ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_31_911 . replace ( \" {full_name} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except FileNotFoundError : error_msg = core_utils . ERROR_31_902 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except RuntimeError as err : error_msg = core_utils . ERROR_31_903 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK # ------------------------------------------------------------------ # Extracting the text from the PDF document. # ------------------------------------------------------------------ @classmethod def parser ( # noqa: C901 cls , full_name_in : str , full_name_out : str , no_pdf_pages : int , document_id : int = - 1 , full_name_orig : str = None , is_lt_heading_required : bool = False , is_lt_list_bullet_required : bool = False , is_lt_list_number_required : bool = False , is_lt_toc_required : bool = False , ) -> tuple [ str , str ]: \"\"\"Extract the text from the PDF document. From the line-oriented XML output file of PDFlib TET, the text and relevant metadata are extracted with the help of an XML parser and stored in a JSON file. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. no_pdf_pages (int): Total number of PDF pages. document_id (int, optional): The identification number of the document. Defaults to None. full_name_orig (str, optional): The file name of the originating document. Defaults to None. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to False. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to False. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to False. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to False. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Raises: RuntimeError: If the parser has detected any errors. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param no_pdf_pages = %i \" , no_pdf_pages ) try : # ------------------------------------------------------------------ # Granularity 'word'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in ) # Get the root Element root = tree . getroot () core_glob . inst_parser = parser . TextParser () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_word ( directory_name = os . path . dirname ( full_name_in ), document_id = document_id , environment_variant = core_glob . inst_setup . environment_variant , file_name_curr = os . path . basename ( full_name_in ), file_name_next = full_name_out , file_name_orig = full_name_orig , is_lt_heading_required = is_lt_heading_required , is_lt_list_bullet_required = is_lt_list_bullet_required , is_lt_list_number_required = is_lt_list_number_required , is_lt_toc_required = is_lt_toc_required , no_pdf_pages = no_pdf_pages , parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML word granularity parsed { full_name_in } \" ) # ------------------------------------------------------------------ # Granularity 'line'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in . replace ( \"word.xml\" , \"line.xml\" )) # Get the root Element root = tree . getroot () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_line ( parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML line granularity parsed { full_name_in } \" ) except FileNotFoundError : error_msg = core_utils . ERROR_61_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK # ------------------------------------------------------------------ # Converting a scanned PDF file to a set of image files. # ------------------------------------------------------------------ @classmethod def pdf2image ( cls , full_name_in : str , output_directory : str , ) -> tuple [ str , str , list [ tuple [ str , str ]]]: \"\"\"Convert a scanned PDF file to a set of image files. To extract the text from a scanned PDF document, it must first be converted into one or more image files, depending on the number of pages. Then these image files are converted into a normal PDF document with the help of an OCR programme. The input file for this method must be a scanned PDF document, which is then converted into image files with the help of PDF2Image. Args: full_name_in (str): The directory name and file name of the input file. output_directory (str): Directory for the flat files to be created. Returns: tuple[str, str, list[tuple[str,str]]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in= %s \" , full_name_in ) try : images = pdf2image . convert_from_path ( full_name_in ) children : list [ tuple [ str , str ]] = [] no_children = 0 directory_name = os . path . dirname ( full_name_in ) stem_name = os . path . splitext ( os . path . basename ( full_name_in ))[ 0 ] try : os . remove ( core_utils . get_full_name_from_components ( directory_name , stem_name + \"_*.\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ), ) ) except OSError : pass # Store the image pages for img in images : no_children += 1 file_name_next = ( stem_name + \".pdf2image_\" + str ( no_children ) + \".\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ) ) full_name_next = core_utils . get_full_name_from_components ( output_directory , file_name_next , ) img . save ( full_name_next , core_glob . inst_setup . pdf2image_type , ) children . append (( file_name_next , full_name_next )) except PDFPageCountError as err : error_msg = ( core_utils . ERROR_21_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_type} \" , str ( type ( err ))) . replace ( \" {error_msg} \" , str ( err )) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children # ------------------------------------------------------------------ # Processing a PDF file with PDFlib TET. # ------------------------------------------------------------------ @classmethod def pdflib ( cls , full_name_in : str , full_name_out : str , document_opt_list : str , page_opt_list : str , ) -> tuple [ str , str ]: \"\"\"Process a PDF file with PDFlib TET. The data from a PDF file is made available in XML files with the help of PDFlib TET. The granularity of the XML files can be word, line or paragraph depending on the document and page options selected. Args: full_name_in (str): Directory name and file name of the input file. full_name_out (str): Directory name and file name of the output file. document_opt_list (str): Document level options. page_opt_list (str): Page level options. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param document_opt_list= %s \" , document_opt_list ) core_glob . logger . debug ( \"param page_opt_list = %s \" , page_opt_list ) tet = TET . TET () doc_opt_list = f \"tetml= {{ filename= {{ { full_name_out } }}}} { document_opt_list } \" if ( file_curr := tet . open_document ( full_name_in , doc_opt_list )) == - 1 : error_msg = ( core_utils . ERROR_51_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_no} \" , str ( tet . get_errnum ())) . replace ( \" {api_name} \" , tet . get_apiname () + \"()\" ) . replace ( \" {error_msg} \" , tet . get_errmsg ()) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg # get number of pages in the document */ no_pages = tet . pcos_get_number ( file_curr , \"length:pages\" ) # loop over pages in the document */ for page_no in range ( 1 , int ( no_pages ) + 1 ): tet . process_page ( file_curr , page_no , page_opt_list ) # This could be combined with the last page-related call tet . process_page ( file_curr , 0 , \"tetml= {trailer} \" ) tet . close_document ( file_curr ) tet . delete () core_glob . logger . debug ( \"return = %s \" , core_glob . LOGGER_END ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK # ------------------------------------------------------------------ # Converting image files to PDF files via OCR. # ------------------------------------------------------------------ @classmethod def tesseract ( cls , full_name_in : str , full_name_out : str , language_tesseract : str , ) -> tuple [ str , str , list [ str ]]: \"\"\"Convert image files to PDF files via OCR. The documents of the following document types are converted to the PDF format using Tesseract OCR: - bmp - bitmap image file - gif - Graphics Interchange Format - jp2 - JPEG 2000 - jpeg - Joint Photographic Experts Group - png - Portable Network Graphics - pnm - portable any-map format - tif - Tag Image File Format - tiff - Tag Image File Format - webp - Image file format with lossless and lossy compression After processing with Tesseract OCR, the files split previously into multiple image files are combined into a single PDF document. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_tesseract (str): The Tesseract name of the document language. Returns: tuple[str, str, list[str]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_tesseract= %s \" , language_tesseract ) children : list [ str ] = [] pdf_writer = PyPDF2 . PdfWriter () for full_name in sorted ( glob . glob ( full_name_in )): try : pdf = pytesseract . image_to_pdf_or_hocr ( extension = \"pdf\" , image = full_name , lang = language_tesseract , timeout = core_glob . inst_setup . tesseract_timeout , ) with open ( full_name_out , \"w+b\" ) as file_handle : # PDF type is bytes by default file_handle . write ( pdf ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_41_911 . replace ( \" {full_name_out} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] pdf_reader = PyPDF2 . PdfReader ( full_name_out ) for page in pdf_reader . pages : # Add each page to the writer object pdf_writer . add_page ( page ) children . append ( full_name ) except RuntimeError as err : error_msg = core_utils . ERROR_41_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] # Write out the merged PDF with open ( full_name_out , \"wb\" ) as file_handle : pdf_writer . write ( file_handle ) core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children # ------------------------------------------------------------------ # Tokenizing the text from the PDF document. # ------------------------------------------------------------------ @classmethod def tokenizer ( cls , full_name_in : str , full_name_out : str , pipeline_name : str , document_id : int = - 1 , full_name_orig : str = \"\" , no_lines_footer : int = - 1 , no_lines_header : int = - 1 , no_lines_toc : int = - 1 , ) -> tuple [ str , str ]: \"\"\"Tokenizing the text from the PDF document. The line-oriented text is broken down into qualified tokens with the means of SpaCy. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. pipeline_name (str): The loaded SpaCy pipeline. document_id (int, optional): The identification number of the document. Defaults to -1. full_name_orig (str, optional): The file name of the originating document. Defaults to \"\". no_lines_footer (int, optional): Total number of footer lines. Defaults to -1. no_lines_header (int, optional): Total number of header lines. Defaults to -1. no_lines_toc (int, optional): Total number of TOC lines. Defaults to -1. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param no_lines_footer= %i \" , no_lines_footer ) core_glob . logger . debug ( \"param no_lines_header= %i \" , no_lines_header ) core_glob . logger . debug ( \"param no_lines_toc = %i \" , no_lines_toc ) core_glob . logger . debug ( \"param pipeline_name = %s \" , pipeline_name ) try : core_glob . inst_tokenizer . process_document ( document_id = document_id , file_name_next = full_name_out , file_name_orig = full_name_orig , no_lines_footer = no_lines_footer , no_lines_header = no_lines_header , no_lines_toc = no_lines_toc , pipeline_name = pipeline_name , ) except FileNotFoundError : error_msg = core_utils . ERROR_71_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK","title":"Documentation for cls_process"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.document","text":"Document content recognition for a specific file. This method extracts the document content structure from a given document and stores it in JSON format. For this purpose, all non-pdf documents and all scanned pdf documents are first converted into a searchable pdf format. Depending on the file format, the tools Pandoc, pdf2image or Tesseract OCR are used for this purpose. PDFlib TET then extracts the text and metadata from the searchable pdf file and makes them available in XML format. spaCY generates qualified tokens from the document text, and these token data are then made available together with the metadata in a JSON format. Parameters: Name Type Description Default full_name_in str Full file name of the document file. required document_id int Document identification. Defaults to -1 i.e. no document identification. None full_name_orig str Original full file name. Defaults to the full file name of the document file. None is_delete_auxiliary_files bool Delete the auxiliary files after a successful processing step. Defaults to parameter delete_auxiliary_files in setup.cfg . None is_lt_heading_required bool If it is set to true , the determination of the heading lines is performed. Defaults to parameter lt_heading_required in setup.cfg . None is_lt_list_bullet_required bool If it is set to true , the determination of the bulleted lists is performed. Defaults to parameter lt_list_bullet_required in setup.cfg . None is_lt_list_number_required bool If it is set to true , the determination of the numbered lists is performed. Defaults to parameter lt_list_number_required in setup.cfg . None is_lt_toc_required bool If it is set to true , the determination of the TOC lines is performed. Defaults to parameter lt_toc_required in setup.cfg . None is_verbose bool Display progress messages for processing. Defaults to parameter verbose in setup.cfg . None language_pandoc str Pandoc language code. Defaults to English. None language_spacy str spaCy language code. Defaults to English transformer pipeline (roberta-base).. None language_tesseract str Tesseract OCR language code. Defaults to English. None output_directory str Directory for the flat files to be created. Defaults to the directory of the document file. None Raises: Type Description RuntimeError Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR. Source code in src/dcr_core/cls_process.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 def document ( # pylint: disable=too-many-arguments self , full_name_in : str , document_id : int = None , full_name_orig : str = None , is_delete_auxiliary_files : bool = None , is_lt_heading_required : bool = None , is_lt_list_bullet_required : bool = None , is_lt_list_number_required : bool = None , is_lt_toc_required : bool = None , is_verbose : bool = None , language_pandoc : str = None , language_spacy : str = None , language_tesseract : str = None , output_directory : str = None , ) -> None : \"\"\"Document content recognition for a specific file. This method extracts the document content structure from a given document and stores it in JSON format. For this purpose, all non-pdf documents and all scanned pdf documents are first converted into a searchable pdf format. Depending on the file format, the tools Pandoc, pdf2image or Tesseract OCR are used for this purpose. PDFlib TET then extracts the text and metadata from the searchable pdf file and makes them available in XML format. spaCY generates qualified tokens from the document text, and these token data are then made available together with the metadata in a JSON format. Args: full_name_in (str): Full file name of the document file. document_id (int, optional): Document identification. Defaults to -1 i.e. no document identification. full_name_orig (str, optional): Original full file name. Defaults to the full file name of the document file. is_delete_auxiliary_files (bool, optional): Delete the auxiliary files after a successful processing step. Defaults to parameter `delete_auxiliary_files` in `setup.cfg`. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to parameter `lt_heading_required` in `setup.cfg`. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to parameter `lt_list_bullet_required` in `setup.cfg`. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to parameter `lt_list_number_required` in `setup.cfg`. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to parameter `lt_toc_required` in `setup.cfg`. is_verbose (bool, optional): Display progress messages for processing. Defaults to parameter `verbose` in `setup.cfg`. language_pandoc (str, optional): Pandoc language code. Defaults to English. language_spacy (str, optional): spaCy language code. Defaults to English transformer pipeline (roberta-base).. language_tesseract (str, optional): Tesseract OCR language code. Defaults to English. output_directory (str, optional): Directory for the flat files to be created. Defaults to the directory of the document file. Raises: RuntimeError: Any issue from Pandoc, pdf2image, PDFlib TET, spaCy, or Tesseract OCR. \"\"\" # Initialise the logging functionality. core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param is_delete_auxiliary_files = %s \" , is_delete_auxiliary_files ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param is_verbose = %s \" , is_verbose ) core_glob . logger . debug ( \"param language_pandoc = %s \" , language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , language_tesseract ) core_glob . logger . debug ( \"param output_directory = %s \" , output_directory ) self . _document_init () self . _document_id = document_id if document_id else - 1 self . _full_name_in = full_name_in self . _full_name_orig = full_name_orig if full_name_orig else full_name_in # Load the configuration parameters. core_glob . inst_setup = setup . Setup () self . _is_delete_auxiliary_files = ( is_delete_auxiliary_files if is_delete_auxiliary_files is not None else core_glob . inst_setup . is_delete_auxiliary_files ) self . _is_lt_heading_required = ( is_lt_heading_required if is_lt_heading_required is not None else core_glob . inst_setup . is_lt_heading_required ) self . _is_lt_list_bullet_required = ( is_lt_list_bullet_required if is_lt_list_bullet_required is not None else core_glob . inst_setup . is_lt_list_bullet_required ) self . _is_lt_list_number_required = ( is_lt_list_number_required if is_lt_list_number_required is not None else core_glob . inst_setup . is_lt_list_number_required ) self . _is_lt_toc_required = is_lt_toc_required if is_lt_toc_required is not None else core_glob . inst_setup . is_lt_toc_required self . _is_verbose = is_verbose if is_verbose is not None else core_glob . inst_setup . is_verbose self . _language_pandoc = language_pandoc if language_pandoc else nlp_core . NLPCore . LANGUAGE_PANDOC_DEFAULT self . _language_spacy = language_spacy if language_spacy else nlp_core . NLPCore . LANGUAGE_SPACY_DEFAULT self . _language_tesseract = language_tesseract if language_tesseract else nlp_core . NLPCore . LANGUAGE_TESSERACT_DEFAULT core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , self . _is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , self . _is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , self . _is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , self . _is_lt_toc_required ) core_glob . logger . debug ( \"param full_name_orig = %s \" , self . _full_name_orig ) core_glob . logger . debug ( \"param language_pandoc = %s \" , self . _language_pandoc ) core_glob . logger . debug ( \"param language_spacy = %s \" , self . _language_spacy ) core_glob . logger . debug ( \"param language_tesseract = %s \" , self . _language_tesseract ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"Start processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Pandoc { self . _language_pandoc } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key spaCy { self . _language_spacy } \" ) core_utils . progress_msg ( self . _is_verbose , f \"Language key Tesseract OCR { self . _language_tesseract } \" ) core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) ( full_name_in_directory , self . _full_name_in_stem_name , self . _full_name_in_extension , ) = core_utils . get_components_from_full_name ( self . _full_name_in ) self . _full_name_in_directory = output_directory if output_directory is not None else full_name_in_directory self . _full_name_in_extension_int = ( self . _full_name_in_extension . lower () if self . _full_name_in_extension else self . _full_name_in_extension ) self . _document_check_extension () self . _document_pandoc () self . _document_pdf2image () self . _document_tesseract () self . _document_pdflib () self . _document_parser () self . _document_tokenizer () core_utils . progress_msg ( self . _is_verbose , \"-\" * 80 ) core_utils . progress_msg ( self . _is_verbose , f \"End processing document file { self . _full_name_orig } \" ) core_utils . progress_msg ( self . _is_verbose , \"=\" * 80 ) core_glob . logger . debug ( core_glob . LOGGER_END )","title":"document()"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.pandoc","text":"Convert a Non-PDF file to a PDF file. The following file formats are converted into PDF format here with the help of Pandoc: csv - comma-separated values docx - Office Open XML epub - e-book file format html - HyperText Markup Language odt - Open Document Format for Office Applications rst - reStructuredText (RST rtf - Rich Text Format Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required language_pandoc str The Pandoc name of the document language. required Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 @classmethod def pandoc ( cls , full_name_in : str , full_name_out : str , language_pandoc : str , ) -> tuple [ str , str ]: \"\"\"Convert a Non-PDF file to a PDF file. The following file formats are converted into PDF format here with the help of Pandoc: - csv - comma-separated values - docx - Office Open XML - epub - e-book file format - html - HyperText Markup Language - odt - Open Document Format for Office Applications - rst - reStructuredText (RST - rtf - Rich Text Format Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_pandoc (str): The Pandoc name of the document language. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_pandoc= %s \" , language_pandoc ) # Convert the document extra_args = [ f \"--pdf-engine= { Process . PANDOC_PDF_ENGINE_XELATEX } \" , \"-V\" , f \"lang: { language_pandoc } \" , ] try : pypandoc . convert_file ( full_name_in , core_glob . FILE_TYPE_PDF , extra_args = extra_args , outputfile = full_name_out , ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_31_911 . replace ( \" {full_name} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except FileNotFoundError : error_msg = core_utils . ERROR_31_902 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg except RuntimeError as err : error_msg = core_utils . ERROR_31_903 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK","title":"pandoc()"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.parser","text":"Extract the text from the PDF document. From the line-oriented XML output file of PDFlib TET, the text and relevant metadata are extracted with the help of an XML parser and stored in a JSON file. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required no_pdf_pages int Total number of PDF pages. required document_id int The identification number of the document. Defaults to None. -1 full_name_orig str The file name of the originating document. Defaults to None. None is_lt_heading_required bool If it is set to true , the determination of the heading lines is performed. Defaults to False. False is_lt_list_bullet_required bool If it is set to true , the determination of the bulleted lists is performed. Defaults to False. False is_lt_list_number_required bool If it is set to true , the determination of the numbered lists is performed. Defaults to False. False is_lt_toc_required bool If it is set to true , the determination of the TOC lines is performed. Defaults to False. False Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Raises: Type Description RuntimeError If the parser has detected any errors. Source code in src/dcr_core/cls_process.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 @classmethod def parser ( # noqa: C901 cls , full_name_in : str , full_name_out : str , no_pdf_pages : int , document_id : int = - 1 , full_name_orig : str = None , is_lt_heading_required : bool = False , is_lt_list_bullet_required : bool = False , is_lt_list_number_required : bool = False , is_lt_toc_required : bool = False , ) -> tuple [ str , str ]: \"\"\"Extract the text from the PDF document. From the line-oriented XML output file of PDFlib TET, the text and relevant metadata are extracted with the help of an XML parser and stored in a JSON file. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. no_pdf_pages (int): Total number of PDF pages. document_id (int, optional): The identification number of the document. Defaults to None. full_name_orig (str, optional): The file name of the originating document. Defaults to None. is_lt_heading_required (bool, optional): If it is set to **`true`**, the determination of the heading lines is performed. Defaults to False. is_lt_list_bullet_required (bool, optional): If it is set to **`true`**, the determination of the bulleted lists is performed. Defaults to False. is_lt_list_number_required (bool, optional): If it is set to **`true`**, the determination of the numbered lists is performed. Defaults to False. is_lt_toc_required (bool, optional): If it is set to **`true`**, the determination of the TOC lines is performed. Defaults to False. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Raises: RuntimeError: If the parser has detected any errors. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param is_lt_heading_required = %s \" , is_lt_heading_required ) core_glob . logger . debug ( \"param is_lt_list_bullet_required= %s \" , is_lt_list_bullet_required ) core_glob . logger . debug ( \"param is_lt_list_number_required= %s \" , is_lt_list_number_required ) core_glob . logger . debug ( \"param is_lt_toc_required = %s \" , is_lt_toc_required ) core_glob . logger . debug ( \"param no_pdf_pages = %i \" , no_pdf_pages ) try : # ------------------------------------------------------------------ # Granularity 'word'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in ) # Get the root Element root = tree . getroot () core_glob . inst_parser = parser . TextParser () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_word ( directory_name = os . path . dirname ( full_name_in ), document_id = document_id , environment_variant = core_glob . inst_setup . environment_variant , file_name_curr = os . path . basename ( full_name_in ), file_name_next = full_name_out , file_name_orig = full_name_orig , is_lt_heading_required = is_lt_heading_required , is_lt_list_bullet_required = is_lt_list_bullet_required , is_lt_list_number_required = is_lt_list_number_required , is_lt_toc_required = is_lt_toc_required , no_pdf_pages = no_pdf_pages , parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML word granularity parsed { full_name_in } \" ) # ------------------------------------------------------------------ # Granularity 'line'. # ------------------------------------------------------------------ # Create the Element tree object tree = defusedxml . ElementTree . parse ( full_name_in . replace ( \"word.xml\" , \"line.xml\" )) # Get the root Element root = tree . getroot () for child in root : child_tag = child . tag [ nlp_core . NLPCore . PARSE_ELEM_FROM :] match child_tag : case nlp_core . NLPCore . PARSE_ELEM_DOCUMENT : core_glob . inst_parser . parse_tag_document_line ( parent = child , parent_tag = child_tag , ) case nlp_core . NLPCore . PARSE_ELEM_CREATION : pass case other : core_utils . progress_msg_core ( core_utils . ERROR_61_902 . replace ( \" {parent_tag} \" , \"XML root\" ) . replace ( \"{child_tag\" , other ) ) core_glob . inst_parser . no_errors += 1 if core_glob . inst_parser . no_errors != 0 : raise RuntimeError ( core_utils . ERROR_61_903 . replace ( \" {no_errors} \" , str ( core_glob . inst_parser . no_errors ))) core_utils . progress_msg ( core_glob . inst_setup . is_verbose , f \"TETML line granularity parsed { full_name_in } \" ) except FileNotFoundError : error_msg = core_utils . ERROR_61_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK","title":"parser()"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.pdf2image","text":"Convert a scanned PDF file to a set of image files. To extract the text from a scanned PDF document, it must first be converted into one or more image files, depending on the number of pages. Then these image files are converted into a normal PDF document with the help of an OCR programme. The input file for this method must be a scanned PDF document, which is then converted into image files with the help of PDF2Image. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required output_directory str Directory for the flat files to be created. required Returns: Type Description tuple [ str , str , list [ tuple [ str , str ]]] tuple[str, str, list[tuple[str,str]]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 @classmethod def pdf2image ( cls , full_name_in : str , output_directory : str , ) -> tuple [ str , str , list [ tuple [ str , str ]]]: \"\"\"Convert a scanned PDF file to a set of image files. To extract the text from a scanned PDF document, it must first be converted into one or more image files, depending on the number of pages. Then these image files are converted into a normal PDF document with the help of an OCR programme. The input file for this method must be a scanned PDF document, which is then converted into image files with the help of PDF2Image. Args: full_name_in (str): The directory name and file name of the input file. output_directory (str): Directory for the flat files to be created. Returns: tuple[str, str, list[tuple[str,str]]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in= %s \" , full_name_in ) try : images = pdf2image . convert_from_path ( full_name_in ) children : list [ tuple [ str , str ]] = [] no_children = 0 directory_name = os . path . dirname ( full_name_in ) stem_name = os . path . splitext ( os . path . basename ( full_name_in ))[ 0 ] try : os . remove ( core_utils . get_full_name_from_components ( directory_name , stem_name + \"_*.\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ), ) ) except OSError : pass # Store the image pages for img in images : no_children += 1 file_name_next = ( stem_name + \".pdf2image_\" + str ( no_children ) + \".\" + ( core_glob . FILE_TYPE_PNG if core_glob . inst_setup . pdf2image_type == setup . Setup . PDF2IMAGE_TYPE_PNG else core_glob . FILE_TYPE_JPEG ) ) full_name_next = core_utils . get_full_name_from_components ( output_directory , file_name_next , ) img . save ( full_name_next , core_glob . inst_setup . pdf2image_type , ) children . append (( file_name_next , full_name_next )) except PDFPageCountError as err : error_msg = ( core_utils . ERROR_21_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_type} \" , str ( type ( err ))) . replace ( \" {error_msg} \" , str ( err )) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children","title":"pdf2image()"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.pdflib","text":"Process a PDF file with PDFlib TET. The data from a PDF file is made available in XML files with the help of PDFlib TET. The granularity of the XML files can be word, line or paragraph depending on the document and page options selected. Parameters: Name Type Description Default full_name_in str Directory name and file name of the input file. required full_name_out str Directory name and file name of the output file. required document_opt_list str Document level options. required page_opt_list str Page level options. required Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 @classmethod def pdflib ( cls , full_name_in : str , full_name_out : str , document_opt_list : str , page_opt_list : str , ) -> tuple [ str , str ]: \"\"\"Process a PDF file with PDFlib TET. The data from a PDF file is made available in XML files with the help of PDFlib TET. The granularity of the XML files can be word, line or paragraph depending on the document and page options selected. Args: full_name_in (str): Directory name and file name of the input file. full_name_out (str): Directory name and file name of the output file. document_opt_list (str): Document level options. page_opt_list (str): Page level options. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param document_opt_list= %s \" , document_opt_list ) core_glob . logger . debug ( \"param page_opt_list = %s \" , page_opt_list ) tet = TET . TET () doc_opt_list = f \"tetml= {{ filename= {{ { full_name_out } }}}} { document_opt_list } \" if ( file_curr := tet . open_document ( full_name_in , doc_opt_list )) == - 1 : error_msg = ( core_utils . ERROR_51_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_no} \" , str ( tet . get_errnum ())) . replace ( \" {api_name} \" , tet . get_apiname () + \"()\" ) . replace ( \" {error_msg} \" , tet . get_errmsg ()) ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg # get number of pages in the document */ no_pages = tet . pcos_get_number ( file_curr , \"length:pages\" ) # loop over pages in the document */ for page_no in range ( 1 , int ( no_pages ) + 1 ): tet . process_page ( file_curr , page_no , page_opt_list ) # This could be combined with the last page-related call tet . process_page ( file_curr , 0 , \"tetml= {trailer} \" ) tet . close_document ( file_curr ) tet . delete () core_glob . logger . debug ( \"return = %s \" , core_glob . LOGGER_END ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK","title":"pdflib()"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.tesseract","text":"Convert image files to PDF files via OCR. The documents of the following document types are converted to the PDF format using Tesseract OCR: bmp - bitmap image file gif - Graphics Interchange Format jp2 - JPEG 2000 jpeg - Joint Photographic Experts Group png - Portable Network Graphics pnm - portable any-map format tif - Tag Image File Format tiff - Tag Image File Format webp - Image file format with lossless and lossy compression After processing with Tesseract OCR, the files split previously into multiple image files are combined into a single PDF document. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required language_tesseract str The Tesseract name of the document language. required Returns: Type Description tuple [ str , str , list [ str ]] tuple[str, str, list[str]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 @classmethod def tesseract ( cls , full_name_in : str , full_name_out : str , language_tesseract : str , ) -> tuple [ str , str , list [ str ]]: \"\"\"Convert image files to PDF files via OCR. The documents of the following document types are converted to the PDF format using Tesseract OCR: - bmp - bitmap image file - gif - Graphics Interchange Format - jp2 - JPEG 2000 - jpeg - Joint Photographic Experts Group - png - Portable Network Graphics - pnm - portable any-map format - tif - Tag Image File Format - tiff - Tag Image File Format - webp - Image file format with lossless and lossy compression After processing with Tesseract OCR, the files split previously into multiple image files are combined into a single PDF document. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. language_tesseract (str): The Tesseract name of the document language. Returns: tuple[str, str, list[str]]: (\"ok\", \"\", [...]) if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param language_tesseract= %s \" , language_tesseract ) children : list [ str ] = [] pdf_writer = PyPDF2 . PdfWriter () for full_name in sorted ( glob . glob ( full_name_in )): try : pdf = pytesseract . image_to_pdf_or_hocr ( extension = \"pdf\" , image = full_name , lang = language_tesseract , timeout = core_glob . inst_setup . tesseract_timeout , ) with open ( full_name_out , \"w+b\" ) as file_handle : # PDF type is bytes by default file_handle . write ( pdf ) if len ( PyPDF2 . PdfReader ( full_name_out ) . pages ) == 0 : error_msg = core_utils . ERROR_41_911 . replace ( \" {full_name_out} \" , full_name_out ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] pdf_reader = PyPDF2 . PdfReader ( full_name_out ) for page in pdf_reader . pages : # Add each page to the writer object pdf_writer . add_page ( page ) children . append ( full_name ) except RuntimeError as err : error_msg = core_utils . ERROR_41_901 . replace ( \" {full_name} \" , full_name_in ) . replace ( \" {error_msg} \" , str ( err )) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg , [])) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg , [] # Write out the merged PDF with open ( full_name_out , \"wb\" ) as file_handle : pdf_writer . write ( file_handle ) core_glob . logger . debug ( \"return = %s \" , ( core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children )) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK [ 0 ], core_glob . RETURN_OK [ 1 ], children","title":"tesseract()"},{"location":"application_api_documentation/#src.dcr_core.cls_process.Process.tokenizer","text":"Tokenizing the text from the PDF document. The line-oriented text is broken down into qualified tokens with the means of SpaCy. Parameters: Name Type Description Default full_name_in str The directory name and file name of the input file. required full_name_out str The directory name and file name of the output file. required pipeline_name str The loaded SpaCy pipeline. required document_id int The identification number of the document. Defaults to -1. -1 full_name_orig str The file name of the originating document. Defaults to \"\". '' no_lines_footer int Total number of footer lines. Defaults to -1. -1 no_lines_header int Total number of header lines. Defaults to -1. -1 no_lines_toc int Total number of TOC lines. Defaults to -1. -1 Returns: Type Description tuple [ str , str ] tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. Source code in src/dcr_core/cls_process.py 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 @classmethod def tokenizer ( cls , full_name_in : str , full_name_out : str , pipeline_name : str , document_id : int = - 1 , full_name_orig : str = \"\" , no_lines_footer : int = - 1 , no_lines_header : int = - 1 , no_lines_toc : int = - 1 , ) -> tuple [ str , str ]: \"\"\"Tokenizing the text from the PDF document. The line-oriented text is broken down into qualified tokens with the means of SpaCy. Args: full_name_in (str): The directory name and file name of the input file. full_name_out (str): The directory name and file name of the output file. pipeline_name (str): The loaded SpaCy pipeline. document_id (int, optional): The identification number of the document. Defaults to -1. full_name_orig (str, optional): The file name of the originating document. Defaults to \"\". no_lines_footer (int, optional): Total number of footer lines. Defaults to -1. no_lines_header (int, optional): Total number of header lines. Defaults to -1. no_lines_toc (int, optional): Total number of TOC lines. Defaults to -1. Returns: tuple[str, str]: (\"ok\", \"\") if the processing has been completed successfully, otherwise a corresponding error code and error message. \"\"\" try : core_glob . logger . debug ( core_glob . LOGGER_START ) except AttributeError : core_glob . initialise_logger () core_glob . logger . debug ( core_glob . LOGGER_START ) core_glob . logger . debug ( \"param document_id = %i \" , document_id ) core_glob . logger . debug ( \"param full_name_in = %s \" , full_name_in ) core_glob . logger . debug ( \"param full_name_orig = %s \" , full_name_orig ) core_glob . logger . debug ( \"param full_name_out = %s \" , full_name_out ) core_glob . logger . debug ( \"param no_lines_footer= %i \" , no_lines_footer ) core_glob . logger . debug ( \"param no_lines_header= %i \" , no_lines_header ) core_glob . logger . debug ( \"param no_lines_toc = %i \" , no_lines_toc ) core_glob . logger . debug ( \"param pipeline_name = %s \" , pipeline_name ) try : core_glob . inst_tokenizer . process_document ( document_id = document_id , file_name_next = full_name_out , file_name_orig = full_name_orig , no_lines_footer = no_lines_footer , no_lines_header = no_lines_header , no_lines_toc = no_lines_toc , pipeline_name = pipeline_name , ) except FileNotFoundError : error_msg = core_utils . ERROR_71_901 . replace ( \" {full_name} \" , full_name_in ) core_glob . logger . debug ( \"return = %s \" , ( error_msg [: 6 ], error_msg )) core_glob . logger . debug ( core_glob . LOGGER_END ) return error_msg [: 6 ], error_msg core_glob . logger . debug ( \"return = %s \" , core_glob . RETURN_OK ) core_glob . logger . debug ( core_glob . LOGGER_END ) return core_glob . RETURN_OK","title":"tokenizer()"},{"location":"application_configuration/","text":"DCR-CORE - Application - Configuration 1. logging_cfg.yaml This file controls the logging behaviour of the application. Default content : version: 1 formatters: simple: format: \"%(asctime)s [%(module)s.py ] %(levelname)-5s %(funcName)s:%(lineno)d %(message)s\" extended: format: \"%(asctime)s [%(module)s.py ] %(levelname)-5s %(funcName)s:%(lineno)d \\n%(message)s\" handlers: console: class: logging.StreamHandler level: INFO formatter: simple file_handler: class: logging.FileHandler level: INFO filename: logging_dcr_core.log formatter: extended loggers: dcr_core: handlers: [ console ] root: handlers: [ file_handler ] 2. setup.cfg This file controls the behaviour of the DCR-CORE application. The customisable entries are: Parameter Default Description delete_auxiliary_files false Delete the auxiliary files after a successful processing step. directory_inbox data/inbox_prod Directory for the new documents received. json_incl_config true Include the configuration data in the JSON file. json_incl_fonts true Include the font data in the JSON file. json_incl_heading true Include the heading data in the JSON file. json_incl_list_bullet true Include the bulleted list data in the JSON file. json_incl_list_number true Include the numbered list data in the JSON file. json_incl_params true Include the parameters in the JSON file. json_incl_table true Include the table data in the JSON file. json_indent 4 Improves the readability of the JSON file. json_sort_keys true If it is set to true , the keys are set in ascending order else, they appear as in the Python object. lt_export_rule_file_heading data/lt_export_rule_heading.json File name for the export of the heading rules. lt_export_rule_file_list_bullet data/lt_export_rule_list_bullet.json File name for the export of the bulleted list rules. lt_export_rule_file_list_number data/lt_export_rule_list_number.json File name for the export of the numbered list rules. lt_footer_max_distance 3 Maximum Levenshtein distance for a footer line. lt_footer_max_lines 3 Maximum number of footers. lt_header_max_distance 3 Maximum Levenshtein distance for a header line. lt_header_max_lines 3 Maximum number of headers. lt_heading_file_incl_no_ctx 1 The number of lines following the heading to be included as context into the JSON file. lt_heading_file_incl_regexp true If it is set to true , the regular expression for the heading is included in the JSON file. lt_heading_max_level 3 Maximum level of the heading structure. lt_heading_min_pages 2 Minimum number of pages to determine the headings. lt_heading_required true If it is set to true , the determination of the heading lines is performed. lt_heading_rule_file none File with rules to determine the headings. lt_heading_tolerance_llx 10 Tolerance of vertical indentation in percent. lt_list_bullet_min_entries 2 Minimum number of entries to determine a bulleted list. lt_list_bullet_required true If it is set to true , the determination of the bulleted lists lines is performed. lt_list_bullet_rule_file none File with rules to determine the bulleted lists. lt_list_bullet_tolerance_llx 10 Tolerance of vertical indentation in percent. lt_list_number_file_incl_regexp true If it is set to true , the regular expression for the numbered list is included in the JSON file. lt_list_number_min_entries 2 Minimum number of entries to determine a numbered list. lt_list_number_required true If it is set to true , the determination of the numbered lists lines is performed. lt_list_number_rule_file none File with rules to determine the numbered lists. lt_list_number_tolerance_llx 10 Tolerance of vertical indentation in percent. lt_toc_last_page 5 Maximum number of pages for the search of the TOC (from the beginning). lt_toc_min_entries 5 Minimum number of TOC entries. lt_toc_required true If it is set to true , the determination of the TOC lines is performed. pdfimage_type jpeg Format of the image files for the scanned pdf document: jpeg or pdf . tesseract_timeout 30 Terminate the tesseract job after a period of time (seconds). tokenize_2_database true Store the tokens in the database table token . tokenize_2_jsonfile true Store the tokens in a JSON flat file. verbose true Display progress messages for processing. verbose_lt_headers_footers false Display progress messages for headers & footers line type determination. verbose_lt_heading false Display progress messages for heading line type determination. verbose_lt_list_bullet false Display progress messages for line type determination of a bulleted list. verbose_lt_list_number false Display progress messages for line type determination of a numbered list. verbose_lt_toc false Display progress messages for table of content line type determination. verbose_parser false Display progress messages for parsing xml (TETML) : all , none or text . The configuration parameters can be set differently for the individual environments ( dev , prod and test ). Examples : [dcr_core.env.dev] delete_auxiliary_files = false directory_inbox = data/inbox_dev lt_footer_max_lines = 3 lt_header_max_lines = 3 lt_heading_file_incl_no_ctx = 3 lt_heading_file_incl_regexp = true lt_heading_tolerance_llx = 5 lt_list_bullet_tolerance_llx = 5 lt_list_number_file_incl_regexp = true lt_list_number_tolerance_llx = 5 lt_table_file_incl_empty_columns = false ... 4. setup.cfg - spaCy Token Attributes The tokens derived from the documents can be qualified via various attributes. The available options are described below. [dcr_core.spacy] Parameter Default Description spacy_ignore_bracket false Ignore the tokens which are brackets ? spacy_ignore_left_punct false Ignore the tokens which are left punctuation marks, e.g. \"(\" ? spacy_ignore_line_type_footer false Ignore the tokens from line type footer ? spacy_ignore_line_type_header false Ignore the tokens from line type header ? spacy_ignore_line_type_heading false Ignore the tokens from line type heading ? spacy_ignore_line_type_list_bullet false Ignore the tokens from line type bulleted list ? spacy_ignore_line_type_list_number false Ignore the tokens from line type numbered list ? spacy_ignore_line_type_table false Ignore the tokens from line type table ? spacy_ignore_line_type_toc false Ignore the tokens from line type TOC ? spacy_ignore_punct false Ignore the tokens which are punctuations ? spacy_ignore_quote false Ignore the tokens which are quotation marks ? spacy_ignore_right_punct false Ignore the tokens which are right punctuation marks, e.g. \")\" ? spacy_ignore_space false Ignore the tokens which consist of whitespace characters ? spacy_ignore_stop false Ignore the tokens which are part of a \u201cstop list\u201d ? spacy_tkn_attr_cluster true Brown cluster ID. spacy_tkn_attr_dep_ true Syntactic dependency relation. spacy_tkn_attr_doc true The parent document. spacy_tkn_attr_ent_iob_ true IOB code of named entity tag. spacy_tkn_attr_ent_kb_id_ true Knowledge base ID that refers to the named entity this token is a part of, if any. spacy_tkn_attr_ent_type_ true Named entity type. spacy_tkn_attr_head true The syntactic parent, or \u201cgovernor\u201d, of this token. spacy_tkn_attr_i true The index of the token within the parent document. spacy_tkn_attr_idx true The character offset of the token within the parent document. spacy_tkn_attr_is_alpha true Does the token consist of alphabetic characters? spacy_tkn_attr_is_ascii true Does the token consist of ASCII characters? Equivalent to all (ord(c) < 128 for c in token.text). spacy_tkn_attr_is_bracket true Is the token a bracket? spacy_tkn_attr_is_currency true Is the token a currency symbol? spacy_tkn_attr_is_digit true Does the token consist of digits? spacy_tkn_attr_is_left_punct true Is the token a left punctuation mark, e.g. \"(\" ? spacy_tkn_attr_is_lower true Is the token in lowercase? Equivalent to token.text.islower(). spacy_tkn_attr_is_oov true Is the token out-of-vocabulary? spacy_tkn_attr_is_punct true Is the token punctuation? spacy_tkn_attr_is_quote true Is the token a quotation mark? spacy_tkn_attr_is_right_punct true Is the token a right punctuation mark, e.g. \")\" ? spacy_tkn_attr_is_sent_end true Does the token end a sentence? spacy_tkn_attr_is_sent_start true Does the token start a sentence? spacy_tkn_attr_is_space true Does the token consist of whitespace characters? Equivalent to token.text.isspace(). spacy_tkn_attr_is_stop true Is the token part of a \u201cstop list\u201d? spacy_tkn_attr_is_title true Is the token in titlecase? spacy_tkn_attr_is_upper true Is the token in uppercase? Equivalent to token.text.isupper(). spacy_tkn_attr_lang_ true Language of the parent document\u2019s vocabulary. spacy_tkn_attr_left_edge true The leftmost token of this token\u2019s syntactic descendants. spacy_tkn_attr_lemma_ true Base form of the token, with no inflectional suffixes. spacy_tkn_attr_lex true The underlying lexeme. spacy_tkn_attr_lex_id true Sequential ID of the token\u2019s lexical type, used to index into tables, e.g. for word vectors. spacy_tkn_attr_like_email true Does the token resemble an email address? spacy_tkn_attr_like_num true Does the token represent a number? spacy_tkn_attr_like_url true Does the token resemble a URL? spacy_tkn_attr_lower_ true Lowercase form of the token text. Equivalent to Token.text.lower(). spacy_tkn_attr_morph true Morphological analysis. spacy_tkn_attr_norm_ true The token\u2019s norm, i.e. a normalized form of the token text. spacy_tkn_attr_orth_ true Verbatim text content (identical to Token.text). Exists mostly for consistency with the other attributes. spacy_tkn_attr_pos_ true Coarse-grained part-of-speech from the Universal POS tag set. spacy_tkn_attr_prefix_ true A length-N substring from the start of the token. Defaults to N=1. spacy_tkn_attr_prob true Smoothed log probability estimate of token\u2019s word type (context-independent entry in the vocabulary). spacy_tkn_attr_rank true Sequential ID of the token\u2019s lexical type, used to index into tables, e.g. for word vectors. spacy_tkn_attr_right_edge true The rightmost token of this token\u2019s syntactic descendants. spacy_tkn_attr_sent true The sentence span that this token is a part of. spacy_tkn_attr_sentiment true A scalar value indicating the positivity or negativity of the token. spacy_tkn_attr_shape_ true Transform of the token\u2019s string to show orthographic features. spacy_tkn_attr_suffix_ true Length-N substring from the end of the token. Defaults to N=3. spacy_tkn_attr_tag_ true Fine-grained part-of-speech. spacy_tkn_attr_tensor true The token\u2019s slice of the parent Doc\u2019s tensor. spacy_tkn_attr_text true Verbatim text content. spacy_tkn_attr_text_with_ws true Text content, with trailing space character if present. spacy_tkn_attr_vocab true The vocab object of the parent Doc. spacy_tkn_attr_whitespace_ true Trailing space character if present. More information about the spaCy token attributes can be found here . DCR-CORE currently supports only a subset of the possible attributes, but this can easily be extended if required. Detailed information about the universal POS tags can be found here .","title":"Configuration"},{"location":"application_configuration/#dcr-core-application-configuration","text":"","title":"DCR-CORE - Application - Configuration"},{"location":"application_configuration/#1-logging_cfgyaml","text":"This file controls the logging behaviour of the application. Default content : version: 1 formatters: simple: format: \"%(asctime)s [%(module)s.py ] %(levelname)-5s %(funcName)s:%(lineno)d %(message)s\" extended: format: \"%(asctime)s [%(module)s.py ] %(levelname)-5s %(funcName)s:%(lineno)d \\n%(message)s\" handlers: console: class: logging.StreamHandler level: INFO formatter: simple file_handler: class: logging.FileHandler level: INFO filename: logging_dcr_core.log formatter: extended loggers: dcr_core: handlers: [ console ] root: handlers: [ file_handler ]","title":"1. logging_cfg.yaml"},{"location":"application_configuration/#2-setupcfg","text":"This file controls the behaviour of the DCR-CORE application. The customisable entries are: Parameter Default Description delete_auxiliary_files false Delete the auxiliary files after a successful processing step. directory_inbox data/inbox_prod Directory for the new documents received. json_incl_config true Include the configuration data in the JSON file. json_incl_fonts true Include the font data in the JSON file. json_incl_heading true Include the heading data in the JSON file. json_incl_list_bullet true Include the bulleted list data in the JSON file. json_incl_list_number true Include the numbered list data in the JSON file. json_incl_params true Include the parameters in the JSON file. json_incl_table true Include the table data in the JSON file. json_indent 4 Improves the readability of the JSON file. json_sort_keys true If it is set to true , the keys are set in ascending order else, they appear as in the Python object. lt_export_rule_file_heading data/lt_export_rule_heading.json File name for the export of the heading rules. lt_export_rule_file_list_bullet data/lt_export_rule_list_bullet.json File name for the export of the bulleted list rules. lt_export_rule_file_list_number data/lt_export_rule_list_number.json File name for the export of the numbered list rules. lt_footer_max_distance 3 Maximum Levenshtein distance for a footer line. lt_footer_max_lines 3 Maximum number of footers. lt_header_max_distance 3 Maximum Levenshtein distance for a header line. lt_header_max_lines 3 Maximum number of headers. lt_heading_file_incl_no_ctx 1 The number of lines following the heading to be included as context into the JSON file. lt_heading_file_incl_regexp true If it is set to true , the regular expression for the heading is included in the JSON file. lt_heading_max_level 3 Maximum level of the heading structure. lt_heading_min_pages 2 Minimum number of pages to determine the headings. lt_heading_required true If it is set to true , the determination of the heading lines is performed. lt_heading_rule_file none File with rules to determine the headings. lt_heading_tolerance_llx 10 Tolerance of vertical indentation in percent. lt_list_bullet_min_entries 2 Minimum number of entries to determine a bulleted list. lt_list_bullet_required true If it is set to true , the determination of the bulleted lists lines is performed. lt_list_bullet_rule_file none File with rules to determine the bulleted lists. lt_list_bullet_tolerance_llx 10 Tolerance of vertical indentation in percent. lt_list_number_file_incl_regexp true If it is set to true , the regular expression for the numbered list is included in the JSON file. lt_list_number_min_entries 2 Minimum number of entries to determine a numbered list. lt_list_number_required true If it is set to true , the determination of the numbered lists lines is performed. lt_list_number_rule_file none File with rules to determine the numbered lists. lt_list_number_tolerance_llx 10 Tolerance of vertical indentation in percent. lt_toc_last_page 5 Maximum number of pages for the search of the TOC (from the beginning). lt_toc_min_entries 5 Minimum number of TOC entries. lt_toc_required true If it is set to true , the determination of the TOC lines is performed. pdfimage_type jpeg Format of the image files for the scanned pdf document: jpeg or pdf . tesseract_timeout 30 Terminate the tesseract job after a period of time (seconds). tokenize_2_database true Store the tokens in the database table token . tokenize_2_jsonfile true Store the tokens in a JSON flat file. verbose true Display progress messages for processing. verbose_lt_headers_footers false Display progress messages for headers & footers line type determination. verbose_lt_heading false Display progress messages for heading line type determination. verbose_lt_list_bullet false Display progress messages for line type determination of a bulleted list. verbose_lt_list_number false Display progress messages for line type determination of a numbered list. verbose_lt_toc false Display progress messages for table of content line type determination. verbose_parser false Display progress messages for parsing xml (TETML) : all , none or text . The configuration parameters can be set differently for the individual environments ( dev , prod and test ). Examples : [dcr_core.env.dev] delete_auxiliary_files = false directory_inbox = data/inbox_dev lt_footer_max_lines = 3 lt_header_max_lines = 3 lt_heading_file_incl_no_ctx = 3 lt_heading_file_incl_regexp = true lt_heading_tolerance_llx = 5 lt_list_bullet_tolerance_llx = 5 lt_list_number_file_incl_regexp = true lt_list_number_tolerance_llx = 5 lt_table_file_incl_empty_columns = false ...","title":"2. setup.cfg"},{"location":"application_configuration/#4-setupcfg-spacy-token-attributes","text":"The tokens derived from the documents can be qualified via various attributes. The available options are described below. [dcr_core.spacy] Parameter Default Description spacy_ignore_bracket false Ignore the tokens which are brackets ? spacy_ignore_left_punct false Ignore the tokens which are left punctuation marks, e.g. \"(\" ? spacy_ignore_line_type_footer false Ignore the tokens from line type footer ? spacy_ignore_line_type_header false Ignore the tokens from line type header ? spacy_ignore_line_type_heading false Ignore the tokens from line type heading ? spacy_ignore_line_type_list_bullet false Ignore the tokens from line type bulleted list ? spacy_ignore_line_type_list_number false Ignore the tokens from line type numbered list ? spacy_ignore_line_type_table false Ignore the tokens from line type table ? spacy_ignore_line_type_toc false Ignore the tokens from line type TOC ? spacy_ignore_punct false Ignore the tokens which are punctuations ? spacy_ignore_quote false Ignore the tokens which are quotation marks ? spacy_ignore_right_punct false Ignore the tokens which are right punctuation marks, e.g. \")\" ? spacy_ignore_space false Ignore the tokens which consist of whitespace characters ? spacy_ignore_stop false Ignore the tokens which are part of a \u201cstop list\u201d ? spacy_tkn_attr_cluster true Brown cluster ID. spacy_tkn_attr_dep_ true Syntactic dependency relation. spacy_tkn_attr_doc true The parent document. spacy_tkn_attr_ent_iob_ true IOB code of named entity tag. spacy_tkn_attr_ent_kb_id_ true Knowledge base ID that refers to the named entity this token is a part of, if any. spacy_tkn_attr_ent_type_ true Named entity type. spacy_tkn_attr_head true The syntactic parent, or \u201cgovernor\u201d, of this token. spacy_tkn_attr_i true The index of the token within the parent document. spacy_tkn_attr_idx true The character offset of the token within the parent document. spacy_tkn_attr_is_alpha true Does the token consist of alphabetic characters? spacy_tkn_attr_is_ascii true Does the token consist of ASCII characters? Equivalent to all (ord(c) < 128 for c in token.text). spacy_tkn_attr_is_bracket true Is the token a bracket? spacy_tkn_attr_is_currency true Is the token a currency symbol? spacy_tkn_attr_is_digit true Does the token consist of digits? spacy_tkn_attr_is_left_punct true Is the token a left punctuation mark, e.g. \"(\" ? spacy_tkn_attr_is_lower true Is the token in lowercase? Equivalent to token.text.islower(). spacy_tkn_attr_is_oov true Is the token out-of-vocabulary? spacy_tkn_attr_is_punct true Is the token punctuation? spacy_tkn_attr_is_quote true Is the token a quotation mark? spacy_tkn_attr_is_right_punct true Is the token a right punctuation mark, e.g. \")\" ? spacy_tkn_attr_is_sent_end true Does the token end a sentence? spacy_tkn_attr_is_sent_start true Does the token start a sentence? spacy_tkn_attr_is_space true Does the token consist of whitespace characters? Equivalent to token.text.isspace(). spacy_tkn_attr_is_stop true Is the token part of a \u201cstop list\u201d? spacy_tkn_attr_is_title true Is the token in titlecase? spacy_tkn_attr_is_upper true Is the token in uppercase? Equivalent to token.text.isupper(). spacy_tkn_attr_lang_ true Language of the parent document\u2019s vocabulary. spacy_tkn_attr_left_edge true The leftmost token of this token\u2019s syntactic descendants. spacy_tkn_attr_lemma_ true Base form of the token, with no inflectional suffixes. spacy_tkn_attr_lex true The underlying lexeme. spacy_tkn_attr_lex_id true Sequential ID of the token\u2019s lexical type, used to index into tables, e.g. for word vectors. spacy_tkn_attr_like_email true Does the token resemble an email address? spacy_tkn_attr_like_num true Does the token represent a number? spacy_tkn_attr_like_url true Does the token resemble a URL? spacy_tkn_attr_lower_ true Lowercase form of the token text. Equivalent to Token.text.lower(). spacy_tkn_attr_morph true Morphological analysis. spacy_tkn_attr_norm_ true The token\u2019s norm, i.e. a normalized form of the token text. spacy_tkn_attr_orth_ true Verbatim text content (identical to Token.text). Exists mostly for consistency with the other attributes. spacy_tkn_attr_pos_ true Coarse-grained part-of-speech from the Universal POS tag set. spacy_tkn_attr_prefix_ true A length-N substring from the start of the token. Defaults to N=1. spacy_tkn_attr_prob true Smoothed log probability estimate of token\u2019s word type (context-independent entry in the vocabulary). spacy_tkn_attr_rank true Sequential ID of the token\u2019s lexical type, used to index into tables, e.g. for word vectors. spacy_tkn_attr_right_edge true The rightmost token of this token\u2019s syntactic descendants. spacy_tkn_attr_sent true The sentence span that this token is a part of. spacy_tkn_attr_sentiment true A scalar value indicating the positivity or negativity of the token. spacy_tkn_attr_shape_ true Transform of the token\u2019s string to show orthographic features. spacy_tkn_attr_suffix_ true Length-N substring from the end of the token. Defaults to N=3. spacy_tkn_attr_tag_ true Fine-grained part-of-speech. spacy_tkn_attr_tensor true The token\u2019s slice of the parent Doc\u2019s tensor. spacy_tkn_attr_text true Verbatim text content. spacy_tkn_attr_text_with_ws true Text content, with trailing space character if present. spacy_tkn_attr_vocab true The vocab object of the parent Doc. spacy_tkn_attr_whitespace_ true Trailing space character if present. More information about the spaCy token attributes can be found here . DCR-CORE currently supports only a subset of the possible attributes, but this can easily be extended if required. Detailed information about the universal POS tags can be found here .","title":"4. setup.cfg - spaCy Token Attributes"},{"location":"application_document_language/","text":"DCR-CORE - Application - Document Language 1. Overview DCR-CORE supports the processing of documents in different languages. The supported languages must be accepted by Pandoc respectively Babel , spaCy and Tesseract OCR . 2. Default Document Language The default document language is English . Application Content Pandoc en spaCy en_core_web_trf Tesseract OCR eng","title":"Document Language"},{"location":"application_document_language/#dcr-core-application-document-language","text":"","title":"DCR-CORE - Application - Document Language"},{"location":"application_document_language/#1-overview","text":"DCR-CORE supports the processing of documents in different languages. The supported languages must be accepted by Pandoc respectively Babel , spaCy and Tesseract OCR .","title":"1. Overview"},{"location":"application_document_language/#2-default-document-language","text":"The default document language is English . Application Content Pandoc en spaCy en_core_web_trf Tesseract OCR eng","title":"2. Default Document Language"},{"location":"application_installation/","text":"DCR-CORE - Application - Installation 1. Use as a library DCR-CORE is installable using the package manager PyPI . Your package manager will find a version that works with your interpreter. Example installation with pip : pip install dcr-core 2. Use of a Docker container A fully functional Docker image is available here on DockerHub. From this, a local Docker container can be created with the following command: docker run -it --name dcr-core -v <local directory>:/dcr-core/data/inbox_prod konnexionsgmbh/dcr-core:0.9.8 <local directory> is the local directory where the files created during the processing are stored. In addition to the software listed under prerequisites, the Docker container also contains a complete virtual environment for running DCR-CORE in suitable versions.","title":"Installaion"},{"location":"application_installation/#dcr-core-application-installation","text":"","title":"DCR-CORE - Application - Installation"},{"location":"application_installation/#1-use-as-a-library","text":"DCR-CORE is installable using the package manager PyPI . Your package manager will find a version that works with your interpreter. Example installation with pip : pip install dcr-core","title":"1. Use as a library"},{"location":"application_installation/#2-use-of-a-docker-container","text":"A fully functional Docker image is available here on DockerHub. From this, a local Docker container can be created with the following command: docker run -it --name dcr-core -v <local directory>:/dcr-core/data/inbox_prod konnexionsgmbh/dcr-core:0.9.8 <local directory> is the local directory where the files created during the processing are stored. In addition to the software listed under prerequisites, the Docker container also contains a complete virtual environment for running DCR-CORE in suitable versions.","title":"2. Use of a Docker container"},{"location":"application_operations/","text":"DCR-CORE - Application - Operations The details of the method call document can be found in the API documentation. 1. Use as a library The following sample code extracts the content structure of the pdf file 1910.03678.pdf into a JSON file: from dcr_core import cls_process process = cls_process.Process() process.document(\"data/inbox_prod/1910.03678.pdf\") 2. Use of a Docker container The following steps extract the content structure of document 1910.03678.pdf using Docker Container. 1. Restarting the container: docker start dcr-core 2. Starting Python in the Virtual Environment (inside the dcr-core container): python3 -m pipenv run python3 3. Make the dcr_core module available: from dcr_core import cls_process 4. Create an instance of the Process class: process = cls_process.Process() 5. Process document files: process.document(\"data/inbox_prod/1910.03678.pdf\")","title":"Operations"},{"location":"application_operations/#dcr-core-application-operations","text":"The details of the method call document can be found in the API documentation.","title":"DCR-CORE - Application - Operations"},{"location":"application_operations/#1-use-as-a-library","text":"The following sample code extracts the content structure of the pdf file 1910.03678.pdf into a JSON file: from dcr_core import cls_process process = cls_process.Process() process.document(\"data/inbox_prod/1910.03678.pdf\")","title":"1. Use as a library"},{"location":"application_operations/#2-use-of-a-docker-container","text":"The following steps extract the content structure of document 1910.03678.pdf using Docker Container. 1. Restarting the container: docker start dcr-core 2. Starting Python in the Virtual Environment (inside the dcr-core container): python3 -m pipenv run python3 3. Make the dcr_core module available: from dcr_core import cls_process 4. Create an instance of the Process class: process = cls_process.Process() 5. Process document files: process.document(\"data/inbox_prod/1910.03678.pdf\")","title":"2. Use of a Docker container"},{"location":"application_requirements/","text":"DCR-CORE - Application - Requirements The required software is listed below. Regarding the corresponding software versions, you will find the detailed information in the Release Notes . 1. Operating System Continuous delivery / integration (CD/CI) runs on Ubuntu and development is done with Windows 10 . For the Windows operating systems, only additional the functionality of the grep , make and sed tools must be made available, e.g. via Grep for Windows , Make for Windows or sed for Windows . 2. Pandoc & TeX Live To convert the non-PDF documents into pdf files for PDFlib TET processing, the universal document converter Pandoc and the TeX typesetting system TeX Live are used and must therefore also be installed. The installation of the TeX Live Frontend is not required. 3. PDFlib TET The software library PDFlib TET is used to tokenize the pdf documents. DCR-CORE contains the free version of PDFlib TET . This free version is limited to files with a maximum size of 1 MB and a maximum number of pages of 10. If larger files are to be processed, a licence must be purchased from PDFlib GmbH . Details on the conditions can be found here . 4. Poppler To convert the scanned PDF documents into image files for Tesseract OCR, the rendering library Poppler is used and must therefore also be installed. 5. Python Because of the use of the new typing features, Python is required. 6. Tesseract OCR To convert image files into pdf files, Tesseract OCR is required.","title":"Requirements"},{"location":"application_requirements/#dcr-core-application-requirements","text":"The required software is listed below. Regarding the corresponding software versions, you will find the detailed information in the Release Notes .","title":"DCR-CORE - Application - Requirements"},{"location":"application_requirements/#1-operating-system","text":"Continuous delivery / integration (CD/CI) runs on Ubuntu and development is done with Windows 10 . For the Windows operating systems, only additional the functionality of the grep , make and sed tools must be made available, e.g. via Grep for Windows , Make for Windows or sed for Windows .","title":"1. Operating System"},{"location":"application_requirements/#2-pandoc-tex-live","text":"To convert the non-PDF documents into pdf files for PDFlib TET processing, the universal document converter Pandoc and the TeX typesetting system TeX Live are used and must therefore also be installed. The installation of the TeX Live Frontend is not required.","title":"2. Pandoc &amp; TeX Live"},{"location":"application_requirements/#3-pdflib-tet","text":"The software library PDFlib TET is used to tokenize the pdf documents. DCR-CORE contains the free version of PDFlib TET . This free version is limited to files with a maximum size of 1 MB and a maximum number of pages of 10. If larger files are to be processed, a licence must be purchased from PDFlib GmbH . Details on the conditions can be found here .","title":"3. PDFlib TET"},{"location":"application_requirements/#4-poppler","text":"To convert the scanned PDF documents into image files for Tesseract OCR, the rendering library Poppler is used and must therefore also be installed.","title":"4. Poppler"},{"location":"application_requirements/#5-python","text":"Because of the use of the new typing features, Python is required.","title":"5. Python"},{"location":"application_requirements/#6-tesseract-ocr","text":"To convert image files into pdf files, Tesseract OCR is required.","title":"6. Tesseract OCR"},{"location":"code_of_conduct/","text":"DCR-CORE - Code of Conduct 1. Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. 2. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting 3. Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. 4. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. 5. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at info@konnexions.ch. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. 6. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available here . For answers to common questions about this code of conduct, see here","title":"Code of Conduct"},{"location":"code_of_conduct/#dcr-core-code-of-conduct","text":"","title":"DCR-CORE - Code of Conduct"},{"location":"code_of_conduct/#1-our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"1. Our Pledge"},{"location":"code_of_conduct/#2-our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"2. Our Standards"},{"location":"code_of_conduct/#3-our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"3. Our Responsibilities"},{"location":"code_of_conduct/#4-scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"4. Scope"},{"location":"code_of_conduct/#5-enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at info@konnexions.ch. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"5. Enforcement"},{"location":"code_of_conduct/#6-attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available here . For answers to common questions about this code of conduct, see here","title":"6. Attribution"},{"location":"contributing/","text":"DCR-CORE - Contributing Guide 1. License In case of software changes we strongly recommend you to respect the license terms. 2. Process fork it create your feature branch ( git checkout -b my-new-feature ) commit your changes ( git commit -am 'Add some feature' ) push to the branch ( git push origin my-new-feature ) create a new pull request Action points to be considered when adding a new database driver and / or a new programming language: README.md Release-Notes.md 3. Notes on the Software Development Process See Developing DCR-CORE here","title":"Contributing Guide"},{"location":"contributing/#dcr-core-contributing-guide","text":"","title":"DCR-CORE - Contributing Guide"},{"location":"contributing/#1-license","text":"In case of software changes we strongly recommend you to respect the license terms.","title":"1. License"},{"location":"contributing/#2-process","text":"fork it create your feature branch ( git checkout -b my-new-feature ) commit your changes ( git commit -am 'Add some feature' ) push to the branch ( git push origin my-new-feature ) create a new pull request Action points to be considered when adding a new database driver and / or a new programming language: README.md Release-Notes.md","title":"2. Process"},{"location":"contributing/#3-notes-on-the-software-development-process","text":"See Developing DCR-CORE here","title":"3. Notes on the Software Development Process"},{"location":"development_code_formatting/","text":"DCR-CORE - Development - Code Formatting The tools Black , docformatter and isort are used for formatting the programme code: Black - The uncompromising Python code formatter. docformatter - Formats docstrings to follow PEP 257 . isort - A Python utility / library to sort imports. All these tools are included in the call make format as well as in the call make dev . They can be executed individually with make black , make pydocstyle or make isort , where the recommended order is first make isort , then make black and finally make pydocstyle .","title":"Code Formatting"},{"location":"development_code_formatting/#dcr-core-development-code-formatting","text":"The tools Black , docformatter and isort are used for formatting the programme code: Black - The uncompromising Python code formatter. docformatter - Formats docstrings to follow PEP 257 . isort - A Python utility / library to sort imports. All these tools are included in the call make format as well as in the call make dev . They can be executed individually with make black , make pydocstyle or make isort , where the recommended order is first make isort , then make black and finally make pydocstyle .","title":"DCR-CORE - Development - Code Formatting"},{"location":"development_coding_standards/","text":"DCR-CORE - Development - Coding Standards The PEP 8 style guide for Python code is strictly applied and enforced with static analysis tools. All program code must be commented with type hinting instructions. All public functions, modules and packages must be commented with Docstring . The program code must be covered as far as possible with appropriate tests - the aim is always 100 % test coverage. The successful execution of make final ensures that the program code meets the required standards.","title":"Coding Standards"},{"location":"development_coding_standards/#dcr-core-development-coding-standards","text":"The PEP 8 style guide for Python code is strictly applied and enforced with static analysis tools. All program code must be commented with type hinting instructions. All public functions, modules and packages must be commented with Docstring . The program code must be covered as far as possible with appropriate tests - the aim is always 100 % test coverage. The successful execution of make final ensures that the program code meets the required standards.","title":"DCR-CORE - Development - Coding Standards"},{"location":"development_continuous_delivery/","text":"DCR-CORE - Development - Continuous Delivery The GitHub Actions are used to enforce the following good practices of the software engineering process in the CI/CD process: uniform formatting of all source code, static source code analysis, execution of the software testing framework, and creation of up-to-date user documentation. The action standards in the GitHub Actions guarantees compliance with the required standards, the action test_production ensures error-free compilation for production use and the action test_development runs the tests against various operating system and Python versions. The actions test_development and test_production must be able to run error-free on operating system Ubuntu and with Python version 3.10 , the action standards is only required error-free for the latest versions of Ubuntu and Python . The individual steps to be carried out in the action standards are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code format the code with isort, Black and docformatter lint the code with Bandit, Flake8, Mypy and Pylint check the API docs with pydocstyle create and upload the user docs with Pydoc-Markdown and Mkdocs install Pandoc , Poppler , Tesseract OCR and TeX Live publish the code coverage results to coveralls.io in the action test_development are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program in the action test_production are: set up Python , pip and pipenv install the production specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program","title":"Continuous Delivery"},{"location":"development_continuous_delivery/#dcr-core-development-continuous-delivery","text":"The GitHub Actions are used to enforce the following good practices of the software engineering process in the CI/CD process: uniform formatting of all source code, static source code analysis, execution of the software testing framework, and creation of up-to-date user documentation. The action standards in the GitHub Actions guarantees compliance with the required standards, the action test_production ensures error-free compilation for production use and the action test_development runs the tests against various operating system and Python versions. The actions test_development and test_production must be able to run error-free on operating system Ubuntu and with Python version 3.10 , the action standards is only required error-free for the latest versions of Ubuntu and Python . The individual steps to be carried out in the action standards are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code format the code with isort, Black and docformatter lint the code with Bandit, Flake8, Mypy and Pylint check the API docs with pydocstyle create and upload the user docs with Pydoc-Markdown and Mkdocs install Pandoc , Poppler , Tesseract OCR and TeX Live publish the code coverage results to coveralls.io in the action test_development are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program in the action test_production are: set up Python , pip and pipenv install the production specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program","title":"DCR-CORE - Development - Continuous Delivery"},{"location":"development_environment/","text":"DCR-CORE - Development - Environment DCR-CORE is developed on the operating systems Ubuntu and Microsoft Windows 10 . Ubuntu is used here via the VM Workstation Player . Ubuntu can also be used in conjunction with the Windows Subsystem for Linux (WSL2) . The GitHub actions for continuous integration run on Ubuntu . Version 3.10 is used for the Python programming language. To set up a suitable development environment under Ubuntu , on the one hand a suitable ready-made Docker image is provided and on the other hand two scripts to create the development system in a standalone system, a virtual environment or the Windows Subsystem for Linux (WSL2) are available. 1. Docker Image The ready-made Docker images are available on DockerHub under the following link: dcr_dev - Document Content Recognition Development Image When selecting the Docker image, care must be taken to select the appropriate version of the Docker image. 2. Script-based Solution Alternatively, for a Ubuntu environment that is as unspoiled as possible, the following two scripts are available in the scripts file directory: scripts/0.9.8/run_install_4-vm_wsl2_1.sh scripts/0.9.8/run_install_4-vm_wsl2_2.sh After a cd scripts command in a terminal window, the script run_install_4-vm_wsl2_1.sh must first be executed. Administration rights ( sudo ) are required for this. Afterwards, the second script run_install_4-vm_wsl2_2.sh must be executed in a new terminal window.","title":"Environment"},{"location":"development_environment/#dcr-core-development-environment","text":"DCR-CORE is developed on the operating systems Ubuntu and Microsoft Windows 10 . Ubuntu is used here via the VM Workstation Player . Ubuntu can also be used in conjunction with the Windows Subsystem for Linux (WSL2) . The GitHub actions for continuous integration run on Ubuntu . Version 3.10 is used for the Python programming language. To set up a suitable development environment under Ubuntu , on the one hand a suitable ready-made Docker image is provided and on the other hand two scripts to create the development system in a standalone system, a virtual environment or the Windows Subsystem for Linux (WSL2) are available.","title":"DCR-CORE - Development - Environment"},{"location":"development_environment/#1-docker-image","text":"The ready-made Docker images are available on DockerHub under the following link: dcr_dev - Document Content Recognition Development Image When selecting the Docker image, care must be taken to select the appropriate version of the Docker image.","title":"1. Docker Image"},{"location":"development_environment/#2-script-based-solution","text":"Alternatively, for a Ubuntu environment that is as unspoiled as possible, the following two scripts are available in the scripts file directory: scripts/0.9.8/run_install_4-vm_wsl2_1.sh scripts/0.9.8/run_install_4-vm_wsl2_2.sh After a cd scripts command in a terminal window, the script run_install_4-vm_wsl2_1.sh must first be executed. Administration rights ( sudo ) are required for this. Afterwards, the second script run_install_4-vm_wsl2_2.sh must be executed in a new terminal window.","title":"2. Script-based Solution"},{"location":"development_line_type/","text":"DCR-CORE - Development - Line Type Algorithms The granularity of the document line tries to classify the individual lines. The possible line types are : line type Meaning b non-classifiable line, i.e. normal text body line f footer line h header line h_9 level 9 heading line lb bulleted list ln numbered list tab table toc table of content The following three rule-based algorithms are used to determine the line type in the order given: headers & footers The headers and footers are determined by a similarity comparison of the first lt_header_max_lines and last lt_footer_max_lines lines respectively. close together A table of contents must be in the first lt_toc_last_page pages and consists of either a list or a table with ascending page numbers. Tables have already been marked accordingly by PDFlib TET. The elements of bulleted or numbered lists must be close together and are determined by regular expressions. headings Headings extend across the entire document and can have hierarchical structures. The headings are determined with rule-enriched regular expressions. 1 Headers & Footers The following parameter controls both the classification of the headers and the footers: verbose_lt_header_footer The verbose mode is an option that provides additional details as to what the processing algorithm is doing. 1.1 Footers 1.1.1 Parameters The following parameters control the classification of the footers: lt_footer_max_distance The degree of similarity of rows is determined by means of the Levenshtein distance . The value zero stands for identical lines. The larger the Levenshtein distance, the more different the rows are. If the header lines do not contain a page numbers, then the parameter should be set to 0 . lt_footer_max_lines The number of lines from the bottom of the page to be analyzed as possible candidates for footers. With the value zero the classification of footers is prevented. lt_footer_required If it is set to true , the determination of the footer lines is performed, else the classification of footers is prevented. spacy_ignore_line_type_footer Default value: true - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. 1.1.2 Algorithm On all pages, the last line n , the line n-1 , etc. are compared up to the maximum specified line. The Levenshtein distance is determined for each pair of lines in the specified range for each current page and the previous page. The line is considered a footer if, except for pages 1 and 2 and pages n-1 and n , the Levenshtein distance is not greater than the specified maximum value. 1.2 Headers 1.2.1 Parameters The following parameters control the classification of the headers: lt_header_max_distance Default value: 3 - the degree of similarity of rows is determined by means of the Levenshtein distance . The value zero stands for identical lines. The larger the Levenshtein distance, the more different the rows are. If the footer lines contain a page number, then depending on the number of pages in the document, the following values are useful: document pages Levenshtein distance < 10 1 < 100 2 < 1000 3 lt_header_max_lines Default value: 3 - the number of lines from the top of the page to be analyzed as possible candidates for headers. A value of zero prevents the classification of headers. lt_header_required If it is set to true , the determination of the header lines is performed, else the classification of headers is prevented. spacy_ignore_line_type_header Default value: true - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. 1.2.2 Algorithm On all pages, the first line, the second line, etc. are compared up to the maximum specified line. The Levenshtein distance is determined for each pair of lines in the specified range for each current page and the previous page. The line is considered a header if, except for pages 1 and 2 and pages n-1 and n , the Levenshtein distance is not greater than the specified maximum value. 2 TOC (Table of Content) An attempt is made here to recognise a table of contents contained in the document. There are two main reasons for this: there is the possibility to ignore the resulting tokens afterwards, and on the other hand, the table of contents could be in the form of a table, which, however, is then not to be processed as a table in the sense of 4.3. 2.1 Parameters The following parameters control the classification of a table of contents included in the document: lt_toc_last_page Default value: 3 - sets the number of pages that will be searched for a table of contents from the beginning of the document. A value of zero prevents the search for a table of contents. lt_toc_min_entries Default value: 3 - defines the minimum number of entries that a table of contents must contain. lt_toc_required If it is set to true , the determination of the table of contents is performed, else the classification of table of contents is prevented. spacy_ignore_line_type_toc Default value: true - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. verbose_lt_toc Default value: false - the verbose mode is an option that provides additional details as to what the processing algorithm is doing. 2.2 Algorithm Table-based A table with the following properties is searched for: except for the first row, the last column of the table must contain an integer greater than zero, the number found there must be ascending, the number must not be greater than the last page number of the document, and if such a table was found, then the algorithm ends here. 2.3 Algorithm Line-based A block of lines with the following properties is searched here: the last token from each line must contain an integer greater than zero, the number found there must be ascending, and the number must not be greater than the last page number of the document. 3 Bulleted Lists An element of a bulleted list extends either over a whole line or over a complete paragraph. All elements of a bulleted list must begin with one or more of the same characters and must not be interrupted by other lines or paragraphs. 3.1 Parameters The following parameters control the classification of a bulleted list: lt_list_bullet_min_entries Default value: 2 - the minimum number of entries in a bulleted list. lt_list_bullet_required If it is set to true , the determination of the bulleted lists is performed, else the classification of bulleted lists is prevented. lt_list_bullet_rule_file Default value: none - name of a file including file directory that contains the rules for determining the bulleted lists. none means that the given default rules are applied. lt_list_bullet_tolerance_llx Default value: 5 - percentage tolerance for the differences in indentation of an entry in a bulleted list. spacy_ignore_line_type_list_bullet Default value: false - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. verbose_lt_list_bullet Default value: false - the verbose mode is an option that provides additional details as to what the processing algorithm is doing. 3.2 Classification Identifiers The following table shows the standard identifiers in the default processing order: identifier Hexadecimal \"- \" 2D20 \". \" 2E20 \"\\ufffd \" \"o \" 6F20 \"\u00b0 \" C2B020 \"\u2022 \" E280A220 \"\u2023 \" E280A320 However, these default rules can also be overridden via a JSON file (see parameter lt_list_bullet_rule_file ). An example file can be found in the file directory data with the file name line_type_list_bullet_rules.json . { \"lineTypeListBulletRules\": [ \"- \", \". \", \"\\ufffd \", \"o \", \"\u00b0 \", \"\u2022 \", \"\u2023 \" ] } 4 Numbered Lists TODO 5 Headings 5.1 Parameters The following parameters control the classification of the headings: lt_heading_file_incl_no_ctx Default value: 1 - the n lines following the heading are included as context into the JSON file. lt_heading_file_incl_regexp Default value: false - if true, the regular expression for the heading is included in the JSON file. lt_heading_max_level Default value: 3 - the maximum number of hierarchical heading levels. lt_heading_min_pages Default value: 2 - the minimum number of document pages for determining headings. lt_heading_required If it is set to true , the determination of the headings is performed, else the classification of headings is prevented. lt_heading_rule_file Default value: none - name of a file including file directory that contains the rules for determining the headings. none means that the given default rules are applied. lt_heading_tolerance_llx Default value: 5 - percentage tolerance for the differences in indentation of a heading at the same level. spacy_ignore_line_type_heading Default value: false - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. verbose_lt_heading Default value: false - the verbose mode is an option that provides additional details as to what the processing algorithm is doing. 5.2 Classification Rules A heading classification rule contains the following 5 elements: Nr. element name description 1 name for documentation purposes, a name that characterises the rule 2 isFirstToken if true, the rule is applied to the first token of the line, otherwise to the beginning of the line 3 regexp the regular expression to be applied 4 functionIsAsc a comparison function for the values of the predecessor and the successor 5 startValues a list of allowed start values The following comparison functions ( functionIsAsc ) can be used: function description ignore no comparison is performed lowercase_letters two lowercase letters are compared, whereby the ASCII difference must be exactly 1 romans two Roman numerals are compared, whereby the difference must be exactly 1 strings two strings are compared on ascending string_floats floating point numbers are compared, whereby the difference must be greater than 0 and less than 1 string_integers two integer numbers are compared, whereby the difference must be exactly 1 uppercase_letters two uppercase letters are compared, whereby the ASCII difference must be exactly 1 The following table shows the standard rules in the default processing order: name isFirstToken regexp functionIsAsc startValues (999) True \"\\(\\d+\\)$\" string_integers [\"(1)\"] (A) True \"\\([A-Z]\\)$\" uppercase_letters [\"(A)\"] (ROM) True see a) romans [\"(I)\"] (a) True \"\\([a-z]\\)$\" lowercase_letters [\"(a)\"] (rom) True see b) romans [\"(i)\"] 999) True \"\\d+\\)$\" string_integers [\"1)\"] 999. True \"\\d+\\.$\" string_integers [\"1.\"] 999.999 True \"\\d+\\.\\d\\d\\d$\" string_floats [\"1.000, \"1.001\"] 999.99 True \"\\d+\\.\\d\\d$\" string_floats [\"1.00\", \"1.01\"] 999.9 True \"\\d+\\.\\d$\" string_floats [\"1.0\", 1.1] A) True \"[A-Z]\\)$\" uppercase_letters [\"A)\"] A. True \"[A-Z]\\.$\" uppercase_letters [\"A, \"A.\"] ROM) True see c) romans [\"I)\"] ROM. True see d) romans [\"I.\"] a) True \"[a-z]\\)$\" lowercase_letters [\"a)\"] a. True \"[a-z]\\.$\" lowercase_letters [\"a, \"a.\"] rom) True see e) romans [\"i)\"] rom. True see f) romans [\"i.\"] a) \"\\(M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\)$\" b) \"\\(m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\)$\" c) \"M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\)$\" d) \"M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\.$\" e) \"m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\)$\" f) \"m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\.$\" However, these default rules can also be overridden via a JSON file (see parameter lt_heading_rule_file ). An example file can be found in the file directory data with the file name heading_rules_test.json . { \"lineTypeHeadingRules\": [ { \"name\": \"(a)\", \"isFirstToken\": true, \"regexp\": \"\\\\([a-z]\\\\)$\", \"functionIsAsc\": \"lowercase_letters\", \"startValues\": [ \"(a)\" ] }, { \"name\": \"(A)\", \"isFirstToken\": true, \"regexp\": \"\\\\([A-Z]\\\\)$\", \"functionIsAsc\": \"uppercase_letters\", \"startValues\": [ \"(A)\" ] }, 5.3 Algorithm the document is worked through page by page and within a page line by line for each current heading level there is an entry in a hierarchy table for each document line, this hierarchy table is searched from bottom to top for a matching entry an entry is considered to be matching if the regular expression is satisfied, and the indentation is within the specified tolerance ( lt_heading_tolerance_llx ), and the comparison function is fulfilled if there is a match, the following processing steps are carried out and then the next document line is processed an entry for the JSON file is optionally created any existing lower entries in the hierarchy table are deleted if no match is found, then the given heading rules are searched in the specified order a heading rule is matching if the regular expression is satisfied, and one of the optional start values matches the document line, and the new heading level is within the specified limit ( lt_heading_max_level ) if there is a match, the following processing steps are carried out and then the next document line is processed the last heading level is increased by 1, a new entry is added to the hierarchy table an entry for the JSON file is optionally created","title":"Line Type Algorithms"},{"location":"development_line_type/#dcr-core-development-line-type-algorithms","text":"The granularity of the document line tries to classify the individual lines. The possible line types are : line type Meaning b non-classifiable line, i.e. normal text body line f footer line h header line h_9 level 9 heading line lb bulleted list ln numbered list tab table toc table of content The following three rule-based algorithms are used to determine the line type in the order given: headers & footers The headers and footers are determined by a similarity comparison of the first lt_header_max_lines and last lt_footer_max_lines lines respectively. close together A table of contents must be in the first lt_toc_last_page pages and consists of either a list or a table with ascending page numbers. Tables have already been marked accordingly by PDFlib TET. The elements of bulleted or numbered lists must be close together and are determined by regular expressions. headings Headings extend across the entire document and can have hierarchical structures. The headings are determined with rule-enriched regular expressions.","title":"DCR-CORE - Development - Line Type Algorithms"},{"location":"development_line_type/#1-headers-footers","text":"The following parameter controls both the classification of the headers and the footers: verbose_lt_header_footer The verbose mode is an option that provides additional details as to what the processing algorithm is doing.","title":"1 Headers &amp; Footers"},{"location":"development_line_type/#11-footers","text":"","title":"1.1 Footers"},{"location":"development_line_type/#12-headers","text":"","title":"1.2 Headers"},{"location":"development_line_type/#2-toc-table-of-content","text":"An attempt is made here to recognise a table of contents contained in the document. There are two main reasons for this: there is the possibility to ignore the resulting tokens afterwards, and on the other hand, the table of contents could be in the form of a table, which, however, is then not to be processed as a table in the sense of 4.3.","title":"2 TOC (Table of Content)"},{"location":"development_line_type/#21-parameters","text":"The following parameters control the classification of a table of contents included in the document: lt_toc_last_page Default value: 3 - sets the number of pages that will be searched for a table of contents from the beginning of the document. A value of zero prevents the search for a table of contents. lt_toc_min_entries Default value: 3 - defines the minimum number of entries that a table of contents must contain. lt_toc_required If it is set to true , the determination of the table of contents is performed, else the classification of table of contents is prevented. spacy_ignore_line_type_toc Default value: true - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. verbose_lt_toc Default value: false - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.","title":"2.1 Parameters"},{"location":"development_line_type/#22-algorithm-table-based","text":"A table with the following properties is searched for: except for the first row, the last column of the table must contain an integer greater than zero, the number found there must be ascending, the number must not be greater than the last page number of the document, and if such a table was found, then the algorithm ends here.","title":"2.2 Algorithm Table-based"},{"location":"development_line_type/#23-algorithm-line-based","text":"A block of lines with the following properties is searched here: the last token from each line must contain an integer greater than zero, the number found there must be ascending, and the number must not be greater than the last page number of the document.","title":"2.3 Algorithm Line-based"},{"location":"development_line_type/#3-bulleted-lists","text":"An element of a bulleted list extends either over a whole line or over a complete paragraph. All elements of a bulleted list must begin with one or more of the same characters and must not be interrupted by other lines or paragraphs.","title":"3 Bulleted Lists"},{"location":"development_line_type/#31-parameters","text":"The following parameters control the classification of a bulleted list: lt_list_bullet_min_entries Default value: 2 - the minimum number of entries in a bulleted list. lt_list_bullet_required If it is set to true , the determination of the bulleted lists is performed, else the classification of bulleted lists is prevented. lt_list_bullet_rule_file Default value: none - name of a file including file directory that contains the rules for determining the bulleted lists. none means that the given default rules are applied. lt_list_bullet_tolerance_llx Default value: 5 - percentage tolerance for the differences in indentation of an entry in a bulleted list. spacy_ignore_line_type_list_bullet Default value: false - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. verbose_lt_list_bullet Default value: false - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.","title":"3.1 Parameters"},{"location":"development_line_type/#32-classification-identifiers","text":"The following table shows the standard identifiers in the default processing order: identifier Hexadecimal \"- \" 2D20 \". \" 2E20 \"\\ufffd \" \"o \" 6F20 \"\u00b0 \" C2B020 \"\u2022 \" E280A220 \"\u2023 \" E280A320 However, these default rules can also be overridden via a JSON file (see parameter lt_list_bullet_rule_file ). An example file can be found in the file directory data with the file name line_type_list_bullet_rules.json . { \"lineTypeListBulletRules\": [ \"- \", \". \", \"\\ufffd \", \"o \", \"\u00b0 \", \"\u2022 \", \"\u2023 \" ] }","title":"3.2 Classification Identifiers"},{"location":"development_line_type/#4-numbered-lists","text":"TODO","title":"4 Numbered Lists"},{"location":"development_line_type/#5-headings","text":"","title":"5 Headings"},{"location":"development_line_type/#51-parameters","text":"The following parameters control the classification of the headings: lt_heading_file_incl_no_ctx Default value: 1 - the n lines following the heading are included as context into the JSON file. lt_heading_file_incl_regexp Default value: false - if true, the regular expression for the heading is included in the JSON file. lt_heading_max_level Default value: 3 - the maximum number of hierarchical heading levels. lt_heading_min_pages Default value: 2 - the minimum number of document pages for determining headings. lt_heading_required If it is set to true , the determination of the headings is performed, else the classification of headings is prevented. lt_heading_rule_file Default value: none - name of a file including file directory that contains the rules for determining the headings. none means that the given default rules are applied. lt_heading_tolerance_llx Default value: 5 - percentage tolerance for the differences in indentation of a heading at the same level. spacy_ignore_line_type_heading Default value: false - determines whether the lines of this type are ignored ( true ) or not ( false ) during tokenization. verbose_lt_heading Default value: false - the verbose mode is an option that provides additional details as to what the processing algorithm is doing.","title":"5.1 Parameters"},{"location":"development_line_type/#52-classification-rules","text":"A heading classification rule contains the following 5 elements: Nr. element name description 1 name for documentation purposes, a name that characterises the rule 2 isFirstToken if true, the rule is applied to the first token of the line, otherwise to the beginning of the line 3 regexp the regular expression to be applied 4 functionIsAsc a comparison function for the values of the predecessor and the successor 5 startValues a list of allowed start values The following comparison functions ( functionIsAsc ) can be used: function description ignore no comparison is performed lowercase_letters two lowercase letters are compared, whereby the ASCII difference must be exactly 1 romans two Roman numerals are compared, whereby the difference must be exactly 1 strings two strings are compared on ascending string_floats floating point numbers are compared, whereby the difference must be greater than 0 and less than 1 string_integers two integer numbers are compared, whereby the difference must be exactly 1 uppercase_letters two uppercase letters are compared, whereby the ASCII difference must be exactly 1 The following table shows the standard rules in the default processing order: name isFirstToken regexp functionIsAsc startValues (999) True \"\\(\\d+\\)$\" string_integers [\"(1)\"] (A) True \"\\([A-Z]\\)$\" uppercase_letters [\"(A)\"] (ROM) True see a) romans [\"(I)\"] (a) True \"\\([a-z]\\)$\" lowercase_letters [\"(a)\"] (rom) True see b) romans [\"(i)\"] 999) True \"\\d+\\)$\" string_integers [\"1)\"] 999. True \"\\d+\\.$\" string_integers [\"1.\"] 999.999 True \"\\d+\\.\\d\\d\\d$\" string_floats [\"1.000, \"1.001\"] 999.99 True \"\\d+\\.\\d\\d$\" string_floats [\"1.00\", \"1.01\"] 999.9 True \"\\d+\\.\\d$\" string_floats [\"1.0\", 1.1] A) True \"[A-Z]\\)$\" uppercase_letters [\"A)\"] A. True \"[A-Z]\\.$\" uppercase_letters [\"A, \"A.\"] ROM) True see c) romans [\"I)\"] ROM. True see d) romans [\"I.\"] a) True \"[a-z]\\)$\" lowercase_letters [\"a)\"] a. True \"[a-z]\\.$\" lowercase_letters [\"a, \"a.\"] rom) True see e) romans [\"i)\"] rom. True see f) romans [\"i.\"] a) \"\\(M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\)$\" b) \"\\(m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\)$\" c) \"M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\)$\" d) \"M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\.$\" e) \"m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\)$\" f) \"m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})\\.$\" However, these default rules can also be overridden via a JSON file (see parameter lt_heading_rule_file ). An example file can be found in the file directory data with the file name heading_rules_test.json . { \"lineTypeHeadingRules\": [ { \"name\": \"(a)\", \"isFirstToken\": true, \"regexp\": \"\\\\([a-z]\\\\)$\", \"functionIsAsc\": \"lowercase_letters\", \"startValues\": [ \"(a)\" ] }, { \"name\": \"(A)\", \"isFirstToken\": true, \"regexp\": \"\\\\([A-Z]\\\\)$\", \"functionIsAsc\": \"uppercase_letters\", \"startValues\": [ \"(A)\" ] },","title":"5.2 Classification Rules"},{"location":"development_line_type/#53-algorithm","text":"the document is worked through page by page and within a page line by line for each current heading level there is an entry in a hierarchy table for each document line, this hierarchy table is searched from bottom to top for a matching entry an entry is considered to be matching if the regular expression is satisfied, and the indentation is within the specified tolerance ( lt_heading_tolerance_llx ), and the comparison function is fulfilled if there is a match, the following processing steps are carried out and then the next document line is processed an entry for the JSON file is optionally created any existing lower entries in the hierarchy table are deleted if no match is found, then the given heading rules are searched in the specified order a heading rule is matching if the regular expression is satisfied, and one of the optional start values matches the document line, and the new heading level is within the specified limit ( lt_heading_max_level ) if there is a match, the following processing steps are carried out and then the next document line is processed the last heading level is increased by 1, a new entry is added to the hierarchy table an entry for the JSON file is optionally created","title":"5.3 Algorithm"},{"location":"development_research_notes/","text":"DCR-CORE - Development - Research Notes Ideas from the following research papers have influenced the development of DCR-CORE . Hegghammer, T. (2021) OCR with Tesseract, Amazon Textract, and Google Document AI: a benchmarking experiment. Journal of Computational Social Science, 2021, pp. 2432-2725 [Online] Available at https://doi.org/10.1007/s42001-021-00149-1 (Accessed 04 January 2022). Optical Character Recognition (OCR) can open up understudied historical documents to computational analysis, but the accuracy of OCR software varies. This article reports a benchmarking experiment comparing the performance of Tesseract, Amazon Textract, and Google Document AI on images of English and Arabic text. English-language book scans (n=322) and Arabic-language article scans (n=100) were replicated 43 times with different types of artificial noise for a corpus of 18,568 documents, generating 51,304 process requests. Document AI delivered the best results, and the server-based processors (Textract and Document AI) performed substantially better than Tesseract, especially on noisy documents. Accuracy for English was considerably higher than for Arabic. Specifying the relative performance of three leading OCR products and the differential effects of commonly found noise types can help scholars identify better OCR solutions for their research needs. The test materials have been preserved in the openly available \u201cNoisy OCR Dataset\u201d (NOD) for reuse in future benchmarking studies. Minaee, S. et al. (2021) Deep Learning Based Text Classification: A Comprehensive Review. arXiv [Online] Available at https://arxiv.org/abs/2004.03705 (Accessed 04 January 2022). Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions. Paa\u00df G., Konya I. (2011) Machine Learning for Document Structure Recognition. In: Mehler A., K\u00fchnberger KU., Lobin H., L\u00fcngen H., Storrer A., Witt A. (eds) Modeling, Learning, and Processing of Text Technological Data Structures. Studies in Computational Intelligence, vol 370. Springer Verlag GmbH, Heidelberg, Germany. Available at https://www.researchgate.net/publication/265487498_Machine_Learning_for_Document_Structure_Recognition (Accessed 04 January 2022). The backbone of the information age is digital information which may be searched, accessed, and transferred instantaneously. Therefore, the digitization of paper documents is extremely interesting. This chapter describes approaches for document structure recognition detecting the hierarchy of physical components in images of documents, such as pages, paragraphs, and figures, and transforms this into a hierarchy of logical components, such as titles, authors, and sections. This structural information improves readability and is useful for indexing and retrieving information contained in documents. First we present a rule-based system segmenting the document image and estimating the logical role of these zones. It is extensively used for processing newspaper collections showing world-class performance. In the second part we introduce several machine learning approaches exploring large numbers of interrelated features. They can be adapted to geometrical models of the document structure, which may be set up as a linear sequence or a general graph. These advanced models require far more computational resources but show a better performance than simpler alternatives and might be used in future. Power R., Scott D., Bouayad-Agha, N. (2003) Document Structure. Computational Linguistics, 2003, Volume 29, Issue 2, pp. 211-260 [Online] The MIT Press, Cambridge, USA. Available at https://direct.mit.edu/coli/article/29/2/211/1803/Document-Structure (Accessed 05 January 2022). We argue the case for abstract document structure as a separate descriptive level in the analysis and generation of written texts. The purpose of this representation is to mediate between the message of a text (i.e., its discourse structure) and its physical presentation (i.e., its organization into graphical constituents like sections, paragraphs, sentences, bulleted lists, figures, and footnotes). Abstract document structure can be seen as an extension of Nunberg's \u201ctext-grammar\u201d it is also closely related to \u201clogical\u201d markup in languages like HTML and LaTEX. We show that by using this intermediate representation, several subtasks in language generation and language understanding can be defined more cleanly. Rahman, M., Finin, T. (2019) Unfolding the Structure of a Document using Deep Learning. arXiv [Online] Available at https://arxiv.org/abs/1910.03678 (Accessed 07 January 2022). Understanding and extracting of information from large documents, such as business opportunities, academic articles, medical documents and technical reports, poses challenges not present in short documents. Such large documents may be multi-themed, complex, noisy and cover diverse topics. We describe a framework that can analyze large documents and help people and computer systems locate desired information in them. We aim to automatically identify and classify different sections of documents and understand their purpose within the document. A key contribution of our research is modeling and extracting the logical and semantic structure of electronic documents using deep learning techniques. We evaluate the effectiveness and robustness of our framework through extensive experiments on two collections: more than one million scholarly articles from arXiv and a collection of requests for proposal documents from government sources.","title":"Research Notes"},{"location":"development_research_notes/#dcr-core-development-research-notes","text":"Ideas from the following research papers have influenced the development of DCR-CORE .","title":"DCR-CORE - Development - Research Notes"},{"location":"development_research_notes/#hegghammer-t-2021","text":"OCR with Tesseract, Amazon Textract, and Google Document AI: a benchmarking experiment. Journal of Computational Social Science, 2021, pp. 2432-2725 [Online] Available at https://doi.org/10.1007/s42001-021-00149-1 (Accessed 04 January 2022). Optical Character Recognition (OCR) can open up understudied historical documents to computational analysis, but the accuracy of OCR software varies. This article reports a benchmarking experiment comparing the performance of Tesseract, Amazon Textract, and Google Document AI on images of English and Arabic text. English-language book scans (n=322) and Arabic-language article scans (n=100) were replicated 43 times with different types of artificial noise for a corpus of 18,568 documents, generating 51,304 process requests. Document AI delivered the best results, and the server-based processors (Textract and Document AI) performed substantially better than Tesseract, especially on noisy documents. Accuracy for English was considerably higher than for Arabic. Specifying the relative performance of three leading OCR products and the differential effects of commonly found noise types can help scholars identify better OCR solutions for their research needs. The test materials have been preserved in the openly available \u201cNoisy OCR Dataset\u201d (NOD) for reuse in future benchmarking studies.","title":"Hegghammer, T. (2021)"},{"location":"development_research_notes/#minaee-s-et-al-2021","text":"Deep Learning Based Text Classification: A Comprehensive Review. arXiv [Online] Available at https://arxiv.org/abs/2004.03705 (Accessed 04 January 2022). Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.","title":"Minaee, S. et al. (2021)"},{"location":"development_research_notes/#paa-g-konya-i-2011","text":"Machine Learning for Document Structure Recognition. In: Mehler A., K\u00fchnberger KU., Lobin H., L\u00fcngen H., Storrer A., Witt A. (eds) Modeling, Learning, and Processing of Text Technological Data Structures. Studies in Computational Intelligence, vol 370. Springer Verlag GmbH, Heidelberg, Germany. Available at https://www.researchgate.net/publication/265487498_Machine_Learning_for_Document_Structure_Recognition (Accessed 04 January 2022). The backbone of the information age is digital information which may be searched, accessed, and transferred instantaneously. Therefore, the digitization of paper documents is extremely interesting. This chapter describes approaches for document structure recognition detecting the hierarchy of physical components in images of documents, such as pages, paragraphs, and figures, and transforms this into a hierarchy of logical components, such as titles, authors, and sections. This structural information improves readability and is useful for indexing and retrieving information contained in documents. First we present a rule-based system segmenting the document image and estimating the logical role of these zones. It is extensively used for processing newspaper collections showing world-class performance. In the second part we introduce several machine learning approaches exploring large numbers of interrelated features. They can be adapted to geometrical models of the document structure, which may be set up as a linear sequence or a general graph. These advanced models require far more computational resources but show a better performance than simpler alternatives and might be used in future.","title":"Paa\u00df G., Konya I. (2011)"},{"location":"development_research_notes/#power-r-scott-d-bouayad-agha-n-2003","text":"Document Structure. Computational Linguistics, 2003, Volume 29, Issue 2, pp. 211-260 [Online] The MIT Press, Cambridge, USA. Available at https://direct.mit.edu/coli/article/29/2/211/1803/Document-Structure (Accessed 05 January 2022). We argue the case for abstract document structure as a separate descriptive level in the analysis and generation of written texts. The purpose of this representation is to mediate between the message of a text (i.e., its discourse structure) and its physical presentation (i.e., its organization into graphical constituents like sections, paragraphs, sentences, bulleted lists, figures, and footnotes). Abstract document structure can be seen as an extension of Nunberg's \u201ctext-grammar\u201d it is also closely related to \u201clogical\u201d markup in languages like HTML and LaTEX. We show that by using this intermediate representation, several subtasks in language generation and language understanding can be defined more cleanly.","title":"Power R., Scott D., Bouayad-Agha, N. (2003)"},{"location":"development_research_notes/#rahman-m-finin-t-2019","text":"Unfolding the Structure of a Document using Deep Learning. arXiv [Online] Available at https://arxiv.org/abs/1910.03678 (Accessed 07 January 2022). Understanding and extracting of information from large documents, such as business opportunities, academic articles, medical documents and technical reports, poses challenges not present in short documents. Such large documents may be multi-themed, complex, noisy and cover diverse topics. We describe a framework that can analyze large documents and help people and computer systems locate desired information in them. We aim to automatically identify and classify different sections of documents and understand their purpose within the document. A key contribution of our research is modeling and extracting the logical and semantic structure of electronic documents using deep learning techniques. We evaluate the effectiveness and robustness of our framework through extensive experiments on two collections: more than one million scholarly articles from arXiv and a collection of requests for proposal documents from government sources.","title":"Rahman, M., Finin, T. (2019)"},{"location":"development_software_documentation/","text":"DCR-CORE - Development - Software Documentation 1. API Documentation The creation of API documentation for functions, modules and packages is mandatory and enforced with the static analysis tool pydocstyle . pydocstyle is a static analysis tool for checking compliance with Python Docstring conventions. pydocstyle can be executed individually with make pydocstyle and is also included in both calls make docs and make dev . The Docstring format used in DCR-CORE is that of type Google . For Visual Studio Code, the extension VSCode Python Docstring Generator can be used when creating API documentation. With the mkdocstrings tool, the API documentation is extracted from the source files and put into Markdown format. In this format, the API documentation can then be integrated into the user documentation. 2. Examples for the format of the API documentation Package Documentation : Package libs: DCR-CORE libraries. Module Documentation : Module pp.inbox: Check and distribute incoming documents. New documents are made available in the file directory inbox. These are then checked and moved to the accepted or rejected file directories depending on the result of the check. Depending on the file format, the accepted documents are then converted into the pdf file format either with the help of Pandoc and TeX Live or with the help of Tesseract OCR. Function Documentation : Load the command line arguments into memory.Pandoc and TeX Live The command line arguments define the process steps to be executed. The valid arguments are: all - Run the complete processing of all new documents. db_c - Create the database. db_u - Upgrade the database. n_2_p - Convert non-pdf docuents to pdf documents. ocr - Convert image docuents to pdf documents. p_i - Process the inbox directory. p_2_i - Convert pdf documents to image files. tet - Extract text from pdf documents. With the option all, the following process steps are executed in this order: 1. p_i 2. p_2_i 3. n_2_p 4. ocr 5. tet Args: argv (List[str]): Command line arguments. Returns: dict[str, bool]: The processing steps based on CLI arguments. In Visual Studio Code, the VSCode Python Docstring Generator tool can be used to create a framework for API documentation. 3. User Documentation The remaining documents for the user documentation can be found in the file directory docs in Markdown format: File Headline Remarks api-docs/* API Documentation css/* CSS files img/* Image files application_*.md Appplication Notes on the software application code_of_conduct.md Code of Conduct contributing.md Contributing Guide development_*.md Development Notes on the software development process index.md Document Content Recognition Background, installation and user guide license.md Text of the licence agreement README.md Directory content list. release_history.md Release History Previous release notes release_notes.md Release Notes Release notes of the current version The MkDocs tool is used to create the user documentation. With the command make mkdocs the user documentation is created by MkDocs and uploaded to the GitHub pages of the repository. The command make mkdocs is also included in the calls make docs and make final .","title":"Software Documentation"},{"location":"development_software_documentation/#dcr-core-development-software-documentation","text":"","title":"DCR-CORE - Development - Software Documentation"},{"location":"development_software_documentation/#1-api-documentation","text":"The creation of API documentation for functions, modules and packages is mandatory and enforced with the static analysis tool pydocstyle . pydocstyle is a static analysis tool for checking compliance with Python Docstring conventions. pydocstyle can be executed individually with make pydocstyle and is also included in both calls make docs and make dev . The Docstring format used in DCR-CORE is that of type Google . For Visual Studio Code, the extension VSCode Python Docstring Generator can be used when creating API documentation. With the mkdocstrings tool, the API documentation is extracted from the source files and put into Markdown format. In this format, the API documentation can then be integrated into the user documentation.","title":"1. API Documentation"},{"location":"development_software_documentation/#2-examples-for-the-format-of-the-api-documentation","text":"Package Documentation : Package libs: DCR-CORE libraries. Module Documentation : Module pp.inbox: Check and distribute incoming documents. New documents are made available in the file directory inbox. These are then checked and moved to the accepted or rejected file directories depending on the result of the check. Depending on the file format, the accepted documents are then converted into the pdf file format either with the help of Pandoc and TeX Live or with the help of Tesseract OCR. Function Documentation : Load the command line arguments into memory.Pandoc and TeX Live The command line arguments define the process steps to be executed. The valid arguments are: all - Run the complete processing of all new documents. db_c - Create the database. db_u - Upgrade the database. n_2_p - Convert non-pdf docuents to pdf documents. ocr - Convert image docuents to pdf documents. p_i - Process the inbox directory. p_2_i - Convert pdf documents to image files. tet - Extract text from pdf documents. With the option all, the following process steps are executed in this order: 1. p_i 2. p_2_i 3. n_2_p 4. ocr 5. tet Args: argv (List[str]): Command line arguments. Returns: dict[str, bool]: The processing steps based on CLI arguments. In Visual Studio Code, the VSCode Python Docstring Generator tool can be used to create a framework for API documentation.","title":"2. Examples for the format of the API documentation"},{"location":"development_software_documentation/#3-user-documentation","text":"The remaining documents for the user documentation can be found in the file directory docs in Markdown format: File Headline Remarks api-docs/* API Documentation css/* CSS files img/* Image files application_*.md Appplication Notes on the software application code_of_conduct.md Code of Conduct contributing.md Contributing Guide development_*.md Development Notes on the software development process index.md Document Content Recognition Background, installation and user guide license.md Text of the licence agreement README.md Directory content list. release_history.md Release History Previous release notes release_notes.md Release Notes Release notes of the current version The MkDocs tool is used to create the user documentation. With the command make mkdocs the user documentation is created by MkDocs and uploaded to the GitHub pages of the repository. The command make mkdocs is also included in the calls make docs and make final .","title":"3. User Documentation"},{"location":"development_software_testing/","text":"DCR-CORE - Development - Software Testing pytest is used as a software testing framework with the following plugins:: pytest-cov for coverage reporting, pytest-deadfixture to list unused or duplicate fixtures, and pytest-random-order to randomise the order of the tests. On the one hand, the tests must be as complete as possible, i.e. a test coverage of 100% is aimed for, but on the other hand, the scope of the test code should be minimal, i.e. unnecessary repetitions must be strictly avoided. The best strategy for this is to first create a test case for the normal case and then add special tests for the special cases not yet covered. Finally, the tool Coveralls for Python is used to enable a connection to Coveralls .","title":"Software Testing"},{"location":"development_software_testing/#dcr-core-development-software-testing","text":"pytest is used as a software testing framework with the following plugins:: pytest-cov for coverage reporting, pytest-deadfixture to list unused or duplicate fixtures, and pytest-random-order to randomise the order of the tests. On the one hand, the tests must be as complete as possible, i.e. a test coverage of 100% is aimed for, but on the other hand, the scope of the test code should be minimal, i.e. unnecessary repetitions must be strictly avoided. The best strategy for this is to first create a test case for the normal case and then add special tests for the special cases not yet covered. Finally, the tool Coveralls for Python is used to enable a connection to Coveralls .","title":"DCR-CORE - Development - Software Testing"},{"location":"development_static_code_analysis/","text":"DCR-CORE - Development - Static Code Analysis The tools Bandit , Flake8 , Mypy and Pylint are used for static code analysis: Bandit - Bandit is a tool designed to find common security issues in Python code. Flake8 - A Python tool that glues together pycodestyle , Pyflakes , McCabe , and third-party plugins to check the style and quality of some Python code. Mypy - Optional static typing for Python . Pylint - It's not just a linter that annoys you! All these tools are included in the call make lint as well as in the call make dev . They can be executed individually with make bandit , make flake8 , make mypy and make pylint . Flake8 includes the following tools: McCabe - McCabe complexity checker for Python . pycodestyle - Simple Python style checker in one Python file. Pyflakes - A simple program which checks Python source files for errors. Radon - Various code metrics for Python code.","title":"Static Code Analysis"},{"location":"development_static_code_analysis/#dcr-core-development-static-code-analysis","text":"The tools Bandit , Flake8 , Mypy and Pylint are used for static code analysis: Bandit - Bandit is a tool designed to find common security issues in Python code. Flake8 - A Python tool that glues together pycodestyle , Pyflakes , McCabe , and third-party plugins to check the style and quality of some Python code. Mypy - Optional static typing for Python . Pylint - It's not just a linter that annoys you! All these tools are included in the call make lint as well as in the call make dev . They can be executed individually with make bandit , make flake8 , make mypy and make pylint . Flake8 includes the following tools: McCabe - McCabe complexity checker for Python . pycodestyle - Simple Python style checker in one Python file. Pyflakes - A simple program which checks Python source files for errors. Radon - Various code metrics for Python code.","title":"DCR-CORE - Development - Static Code Analysis"},{"location":"development_version_planning/","text":"DCR-CORE - Development - Version Planning 1. Version Planning 1.1 Open Version Feature(s) 0.9.8 TBD 1.2 Already implemented Version Feature(s) 0.9.7 Documentation and test improvements 0.9.6 Extracting an API 2. Next Development Steps 2.1 Open 2.1.1 High Priority pandoc: convert doc documents to docx user: reconstruct original document 2.1.2 Normal Priority line type header & footer: optional: ignore the first / last page optional: logging of the applied method optional: page number alternating in the first and last token optional: page number always in the first / last token optional: use of Levenshtein algorithm optional: use of language-related regular expressions to determine the header / footer with the page number line type table: check the coordinates table with page break 2.1.3 Low Priority Google Styleguide implementation 2.2 Already implemented API documentation: Content improvement API documentation: Layout improvement","title":"Version Planning"},{"location":"development_version_planning/#dcr-core-development-version-planning","text":"","title":"DCR-CORE - Development - Version Planning"},{"location":"development_version_planning/#1-version-planning","text":"","title":"1. Version Planning"},{"location":"development_version_planning/#11-open","text":"Version Feature(s) 0.9.8 TBD","title":"1.1 Open"},{"location":"development_version_planning/#12-already-implemented","text":"Version Feature(s) 0.9.7 Documentation and test improvements 0.9.6 Extracting an API","title":"1.2 Already implemented"},{"location":"development_version_planning/#2-next-development-steps","text":"","title":"2. Next Development Steps"},{"location":"development_version_planning/#21-open","text":"","title":"2.1 Open"},{"location":"development_version_planning/#22-already-implemented","text":"API documentation: Content improvement API documentation: Layout improvement","title":"2.2 Already implemented"},{"location":"license/","text":"DCR-CORE - License Konnexions Public License (KX-PL) Version 2020.05, May 2020 pdf Version This license governs use of the accompanying software. If you use the software, you accept this license. If you do not accept the license, do not use the software. Definitions. The terms \"reproduce\", \"reproduction\", \"derivative works\", and \"distribution\" have the same meaning here as under U.S. copyright law. A \"contribution\" is the original software, or any additions or changes to the software. A \"contributor\" is any person that distributes its contribution under this license. \"Licensed patents\" are a contributor's patent claims that read directly on its contribution. Grant of Rights (a) Copyright Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free copyright license to reproduce its contribution, prepare derivative works of its contribution, and distribute its contribution or any derivative works that you create. (b) Patent Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free license under its licensed patents to make, have made, use, sell, offer for sale, import, and/or otherwise dispose of its contribution in the software or derivative works of the contribution in the software. Conditions and Limitations (a) No Trademark License - This license does not grant you rights to use any contributors' name, logo, or trademarks. (b) If you bring a patent claim against any contributor over patents that you claim are infringed by the software, your patent license from such contributor to the software ends automatically.0 (c) If you distribute any portion of the software, you must retain all copyright, patent, trademark, and attribution notices that are present in the software. (d) If you distribute any portion of the software in source code form, you may do so only under this license by including a complete copy of this license with your distribution. If you distribute any portion of the software in compiled or object code form, you may only do so under a license that complies with this license. (e) The software is licensed \"as-is.\" You bear the risk of using it. The contributors give no express warranties, guarantees or conditions. You may have additional consumer rights under your local laws which this license cannot change. To the extent permitted under your local laws, the contributors exclude the implied warranties of merchantability, fitness for a particular purpose and non-infringement. (f) Source code usage under this License is limited to review, compilation and contributions. Contributions to Konnexions software products under this License may only be made in consultation with Konnexions GmbH and through the appropriate Konnexions software repositories.","title":"License"},{"location":"license/#dcr-core-license","text":"Konnexions Public License (KX-PL) Version 2020.05, May 2020 pdf Version This license governs use of the accompanying software. If you use the software, you accept this license. If you do not accept the license, do not use the software. Definitions. The terms \"reproduce\", \"reproduction\", \"derivative works\", and \"distribution\" have the same meaning here as under U.S. copyright law. A \"contribution\" is the original software, or any additions or changes to the software. A \"contributor\" is any person that distributes its contribution under this license. \"Licensed patents\" are a contributor's patent claims that read directly on its contribution. Grant of Rights (a) Copyright Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free copyright license to reproduce its contribution, prepare derivative works of its contribution, and distribute its contribution or any derivative works that you create. (b) Patent Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free license under its licensed patents to make, have made, use, sell, offer for sale, import, and/or otherwise dispose of its contribution in the software or derivative works of the contribution in the software. Conditions and Limitations (a) No Trademark License - This license does not grant you rights to use any contributors' name, logo, or trademarks. (b) If you bring a patent claim against any contributor over patents that you claim are infringed by the software, your patent license from such contributor to the software ends automatically.0 (c) If you distribute any portion of the software, you must retain all copyright, patent, trademark, and attribution notices that are present in the software. (d) If you distribute any portion of the software in source code form, you may do so only under this license by including a complete copy of this license with your distribution. If you distribute any portion of the software in compiled or object code form, you may only do so under a license that complies with this license. (e) The software is licensed \"as-is.\" You bear the risk of using it. The contributors give no express warranties, guarantees or conditions. You may have additional consumer rights under your local laws which this license cannot change. To the extent permitted under your local laws, the contributors exclude the implied warranties of merchantability, fitness for a particular purpose and non-infringement. (f) Source code usage under this License is limited to review, compilation and contributions. Contributions to Konnexions software products under this License may only be made in consultation with Konnexions GmbH and through the appropriate Konnexions software repositories.","title":"DCR-CORE - License"},{"location":"release_history/","text":"DCR-CORE - Release History Version 0.9.7 Release Date: 07.09.2022 1 New Features Creation and use of reference files for pytest 2 Modified Features Delimitation of the documentation to the DCR application Delimitation of the tests to the DCR application Updating the third party software used 3 Applied Software Software Version Remark Status Git 2.34.1 base version upgrade Pandoc 2.19.2 upgrade PFlib TET 5.3 Poppler 22.02.0 upgrade Python3 3.10.7 upgrade Tesseract OCR 5.2.0-22-g0daf1 base version upgrade TeX Live 2022 base version upgrade Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.9.6 Release Date: 07.08.2022 1 New Features Initial version. 2 Applied Software Software Version Remark Status Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.6 Python3 - pip 22.1.2 Tesseract OCR 5.1.0 base version TeX Live 2022 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version","title":"Release History"},{"location":"release_history/#dcr-core-release-history","text":"","title":"DCR-CORE - Release History"},{"location":"release_history/#version-097","text":"Release Date: 07.09.2022","title":"Version 0.9.7"},{"location":"release_history/#1-new-features","text":"Creation and use of reference files for pytest","title":"1 New Features"},{"location":"release_history/#2-modified-features","text":"Delimitation of the documentation to the DCR application Delimitation of the tests to the DCR application Updating the third party software used","title":"2 Modified Features"},{"location":"release_history/#3-applied-software","text":"Software Version Remark Status Git 2.34.1 base version upgrade Pandoc 2.19.2 upgrade PFlib TET 5.3 Poppler 22.02.0 upgrade Python3 3.10.7 upgrade Tesseract OCR 5.2.0-22-g0daf1 base version upgrade TeX Live 2022 base version upgrade","title":"3 Applied Software"},{"location":"release_history/#version-096","text":"Release Date: 07.08.2022","title":"Version 0.9.6"},{"location":"release_history/#1-new-features_1","text":"Initial version.","title":"1 New Features"},{"location":"release_history/#2-applied-software","text":"Software Version Remark Status Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.6 Python3 - pip 22.1.2 Tesseract OCR 5.1.0 base version TeX Live 2022 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"2 Applied Software"},{"location":"release_notes/","text":"DCR-CORE - Release Notes 1. Version 0.9.8 Release Date: dd.mm.2022 1.1 New Features TODO 1.2 Modified Features TODO 1.3 Applied Software Software Version Remark Status Git 2.34.1 base version Pandoc 2.19.2 PFlib TET 5.3 Poppler 22.02.0 Python3 3.10.7 Tesseract OCR 5.2.0-22-g0daf1 base version TeX Live 2022 base version Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version","title":"Release Notes"},{"location":"release_notes/#dcr-core-release-notes","text":"","title":"DCR-CORE - Release Notes"},{"location":"release_notes/#1-version-098","text":"Release Date: dd.mm.2022","title":"1. Version 0.9.8"},{"location":"release_notes/#11-new-features","text":"TODO","title":"1.1 New Features"},{"location":"release_notes/#12-modified-features","text":"TODO","title":"1.2 Modified Features"},{"location":"release_notes/#13-applied-software","text":"Software Version Remark Status Git 2.34.1 base version Pandoc 2.19.2 PFlib TET 5.3 Poppler 22.02.0 Python3 3.10.7 Tesseract OCR 5.2.0-22-g0daf1 base version TeX Live 2022 base version","title":"1.3 Applied Software"},{"location":"api-docs/","text":"API Overview Modules No modules Classes No classes Functions No functions This file was automatically generated via lazydocs .","title":"Index"},{"location":"api-docs/#api-overview","text":"","title":"API Overview"},{"location":"api-docs/#modules","text":"No modules","title":"Modules"},{"location":"api-docs/#classes","text":"No classes","title":"Classes"},{"location":"api-docs/#functions","text":"No functions This file was automatically generated via lazydocs .","title":"Functions"}]}